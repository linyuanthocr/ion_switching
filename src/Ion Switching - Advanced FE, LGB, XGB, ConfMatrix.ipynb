{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# [University of Liverpool - Ion Switching](https://www.kaggle.com/c/liverpool-ion-switching)"},{"metadata":{},"cell_type":"markdown","source":"## The kernel initially consist of a thorough overview of the parameters of all successful commits, the result (LB) obtained and their analysis (see section \"My upgrade\").\n\n## I developed a shell to build different models, check the result of their work, using Confusion matrices and weighted average of their predictions.\n\n## I am experimenting with different good kernels for FE, models and their tuning. Already used more 10 different basic kernels (in each commit their list is in the \"Acknowledgements\" section and - in the kernel's code when the code was taken from the basic kernel unchanged or almost unchanged).\n\nIn the section \"Acknowledgements\" I make a link to the basic kernel where I got the code from, and to where the code came from in the this basic kernel (the original source), after that I upvoted all these good kernels."},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgements\n\n* [Physically Possible](https://www.kaggle.com/jazivxt/physically-possible)\n* [Simple EDA-Model](https://www.kaggle.com/siavrez/simple-eda-model)\n* [MM 2020 NCAAM: LGB, XGB, LogReg - Tuning&Merging](https://www.kaggle.com/vbmokin/mm-2020-ncaam-lgb-xgb-logreg-tuning-merging)\n* [Merging FE & Prediction - xgb, lgb, logr, linr](https://www.kaggle.com/vbmokin/merging-fe-prediction-xgb-lgb-logr-linr)\n* [BOD prediction in river - 15 regression models](https://www.kaggle.com/vbmokin/bod-prediction-in-river-15-regression-models)\n* [Automatic selection from 20 classifier models](https://www.kaggle.com/vbmokin/automatic-selection-from-20-classifier-models)\n* [ðŸ’¥3 Simple Ideas [Ensemble]](https://www.kaggle.com/teejmahal20/3-simple-ideas-ensemble)"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n## Table of Contents\n\n1. [My upgrade](#1)\n    -  [Commit now](#1.1)\n    -  [Previous commits: LGB-1, FE-1,2](#1.2)\n    -  [Previous commits: LGB-2, FE-3](#1.3)\n    -  [Previous commits: LGB-2, MLP, LogReg, FE-3](#1.4)\n    -  [Previous commits: Advanced FE & LGB-2](#1.5)\n    -  [Previous commits: Advanced FE & LGB-2 & XGB-1](#1.6)\n1. [Import libraries](#2)\n1. [Download data & FE](#3)\n1. [Models tuning](#4)\n    -  [Ridge Regression](#4.1)\n    -  [SGDRegressor](#4.2)    \n    -  [Logistic Regression](#4.3)\n    -  [MLP](#4.4)\n    -  [LGB](#4.5)\n    -  [XGB](#4.6)\n1. [Showing Confusion Matrices](#5)\n1. [Comparison and merging solutions](#6)\n1. [Submission](#7)"},{"metadata":{},"cell_type":"markdown","source":"## 1. My upgrade<a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"The using and tuning different regression models and tuning LGB models, MLP model, building Confusion matrices, selection of seed_random."},{"metadata":{},"cell_type":"markdown","source":"## 1.1. Commit now <a class=\"anchor\" id=\"1.1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# FE-2 & FE-3\n\n# Seed for random_state\nseed_random = 42\n\n# FE-2 : window_sizes = [10, 25, 50, 100, 500, 1000, 5000, 10000, 25000, 50000, 100000, 500000]\n# About selection of these values see my post: https://www.kaggle.com/c/liverpool-ion-switching/discussion/135073\nwindow_sizes = [10, 50]\n# with new my features: signal_shift_+2, signal_shift_-2\n# without ratio-, diff-features\n# without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch' with smallest feature importance\n\n# LGB-2 model tuning\nlr_lgb = 0.03\nnum_leaves = 200\nnum_iterations = 2000\n# 'metric'is 'logloss'\n\n# XGB-1 model tuning\nlr_xgb = 0.03\nmax_depth_xgb = 10\nnum_boost_round_xgb = 1000\n# 'eval_metric'is 'logloss'\n\n# Set weight of models\nw_lgb = 0.5\nw_xgb = 1 - w_lgb\nprint(w_xgb)\n# without LogReg, MLP, Ridge Regression and SGDRegressor models","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Variant 1 of FE (FE-1) from the kernels: https://www.kaggle.com/suicaokhoailang/an-embarrassingly-simple-baseline-0-960-lb and https://www.kaggle.com/tunguz/simple-ion-ridge-regression-starter\n* Variant 2 of FE (FE-2) from the kernel: https://www.kaggle.com/teejmahal20/regression-with-optimized-rounder and https://www.kaggle.com/pestipeti/eda-ion-switching\n* Variant 3 of FE (FE-3) from the kernels: https://www.kaggle.com/jazivxt/physically-possible and https://www.kaggle.com/siavrez/simple-eda-model"},{"metadata":{},"cell_type":"markdown","source":"* LGB model 1 (LGB-1) from the kernel https://www.kaggle.com/teejmahal20/regression-with-optimized-rounder\n* LGB model 2 (LGB-2) from the kernel https://www.kaggle.com/jazivxt/physically-possible"},{"metadata":{},"cell_type":"markdown","source":"* XGB model 1 (XGB-1) from the kernel https://www.kaggle.com/teejmahal20/3-simple-ideas-ensemble"},{"metadata":{},"cell_type":"markdown","source":"## Commits 1-12 (FE-1) are created under the first metric variant and are currently unsuccessful"},{"metadata":{},"cell_type":"markdown","source":"## 1.2. Previous commits: LGB-1, FE-1,2 <a class=\"anchor\" id=\"1.2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### Commit 13\n\n* seed_random = 42\n* lr = 0.05\n* num_iterations = 2000\n* w_ridge = 0.05\n* w_sgd = 0.05\n* w_lgb = 0.85 <- basic LGB-1\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb\n* FE-1\n* FE-2 (window_sizes = [20] replace of [10, 25, 50, 100, 500, 1000, 5000, 10000, 25000])\n\n**LB = 0.179**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 14\n\n* seed_random = 42\n* lr = 0.05\n* num_iterations = 2000\n* w_ridge = 0.05\n* w_sgd = 0.05\n* w_lgb = 0.85 <- basic LGB-1\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb\n* FE-1\n\n**LB = 0.325**"},{"metadata":{},"cell_type":"markdown","source":"## 1.3. Previous commits: LGB-2, FE-3 <a class=\"anchor\" id=\"1.3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### Commit 21\n\n* seed_random = 42\n* num_leaves = 2**7+1\n* lr = 0.05\n* num_iterations = 2000\n* w_ridge = 0.001\n* w_sgd = 0.001\n* w_lgb = 0.996\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.936**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 22\n\n* seed_random = 42\n* num_leaves = 2**7+1\n* lr = 0.05\n* num_iterations = 2000\n* w_ridge = 0.001\n* w_sgd = 0.001\n* w_lgb = 0.499\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.860**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 23\n\n* seed_random = 42\n* num_leaves = 2**7+1\n* lr = 0.05\n* num_iterations = 10000\n* w_ridge = 0\n* w_sgd = 0\n* w_lgb = 0.998\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.936**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 24\n\n* seed_random = 42\n* num_leaves = 2**7+1\n* lr = 0.01\n* num_iterations = 2000\n* w_ridge = 0\n* w_sgd = 0\n* w_lgb = 0.998\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.936**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 24\n\n* seed_random = 42\n* num_leaves = 200\n* lr = 0.01\n* num_iterations = 2000\n* w_ridge = 0\n* w_sgd = 0\n* w_lgb = 0.998\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.936**"},{"metadata":{},"cell_type":"markdown","source":"## 1.4. Previous commits: LGB-2, MLP, LogReg, FE-3 <a class=\"anchor\" id=\"1.4\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### Commit 27\n\n* seed_random = 42\n* num_leaves = 200\n* cv_mlp = 2\n* lr_mlp = 0.01\n* n_layer_0 = 2\n* n_layer_max = 10\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_ridge = 0\n* w_sgd = 0\n* w_lgb = 0.7\n* w_mlp = 0.2\n* w_logreg = 1 - w_ridge - w_sgd - w_lgb \n\n**LB = 0.935**"},{"metadata":{},"cell_type":"markdown","source":"## 1.5. Previous commits: Advanced FE & LGB-2 <a class=\"anchor\" id=\"1.5\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### Commit 30\n\n* FE-3 = 51 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 4000\n* w_lgb = 1\n\n**LB = 0.936**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 32\n\n* FE-1 & FE-3 with 'group', without 'signal' = 52 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 4000\n* w_lgb = 1\n\n**LB = 0.882**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 33\n\n* FE-1 & FE-3 without 'group', with 'signal' = 52 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 4000\n* w_lgb = 1\n\n**LB = 0.898**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 35\n\n* FE-2 (window_sizes = [10, 50]) & FE-3 = 79 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 36\n\n* FE-2 (window_sizes = [25, 100, 5000]) & FE-3 = 79 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.936**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 37\n\n* FE-2 (window_sizes = [500, 1000, 10000]) & FE-3 = 79 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.935**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 39\n\n* FE-2 (window_sizes = [10, 25, 50, 5000, 10000], without some features) & FE-3 = 88 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.936**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 40\n\n* FE-2 (window_sizes = window_sizes = [10, 50, 5000]) & FE-3 = 96 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 41\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 42\n\n* FE-2 (window_sizes = window_sizes = [10, 50, 5000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 44\n\n* FE-2 (window_sizes = window_sizes = [20, 30, 25000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.936**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 45\n\n* FE-2 (window_sizes = window_sizes = [75, 2500, 15000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.936**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 46\n\n* FE-2 (window_sizes = window_sizes = [25, 5000, 10000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.936**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 47\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 48\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.938**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 49\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 10000\n* w_lgb = 1\n\n**LB = 0.937**\n\nCalculation Time - 5.2 hours"},{"metadata":{},"cell_type":"markdown","source":"### Commit 50\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 511\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 51\n\n* FE-2 (window_sizes = window_sizes = [10, 50, 5000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 53\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* 'metrics': f1_score and 'logloss' (replace of f1_score and 'rmse' in previous commits)\n\n**LB = 0.938**\n\nThe kernel calculated faster by 300 seconds, but the LB was the same (see commit 48)"},{"metadata":{},"cell_type":"markdown","source":"### Commit 54\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 250\n* num_iterations = 2000\n* w_lgb = 1\n* 'metrics': f1_score and 'logloss' (replace of f1_score and 'rmse' in previous commits)\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 55\n\n* FE-2 (window_sizes = window_sizes = [50000, 100000, 500000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* metrics are 'logloss' & f1_score\n\n**LB = 0.919**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 56\n\n* FE-2 (window_sizes = window_sizes = [10, 50, 50000] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 99 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* metrics are 'logloss' & f1_score\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 57\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 0\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* metrics are 'logloss' & f1_score\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 58\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 7\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* metrics are 'logloss' & f1_score\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 59\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2) & FE-3 = 84 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.015\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 1\n* metrics are 'logloss' & f1_score\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"## 1.6. Previous commits: Advanced FE & LGB-2 & XGB-1<a class=\"anchor\" id=\"1.6\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### Commit 63\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without ratio-, diff-, norm-features) & FE-3 = 72 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.1\n* num_leaves = 200\n* num_iterations = 200\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.1\n* max_depth_xgb = 10\n* num_boost_round_xgb = 22\n* w_xgb = 0.5\n* metrics is 'rmse'\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 64\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without ratio-, diff-, norm-features) & FE-3 = 72 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.1\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.1\n* max_depth_xgb = 10\n* num_boost_round_xgb = 2000\n* w_xgb = 0.5\n* metrics is 'rmse'\n\n**LB = 0.937**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 70\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-, norm-features) & FE-3 = 69 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.1\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.1\n* max_depth_xgb = 10\n* num_boost_round_xgb = 800\n* w_xgb = 0.5\n* metrics is 'rmse'\n\n**LB = 0.938**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 71\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-, norm-features) & FE-3 = 69 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.01\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.1\n* max_depth_xgb = 10\n* num_boost_round_xgb = 800\n* w_xgb = 0.5\n* metrics is 'rmse'\n\n**LB = 0.938**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 72\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-, norm-features) & FE-3 = 69 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.1\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.1\n* max_depth_xgb = 10\n* num_boost_round_xgb = 800\n* w_xgb = 0.5\n* metrics is 'logloss'\n\n**LB = 0.938**"},{"metadata":{},"cell_type":"markdown","source":"### Commit 74\n\n* FE-2 (window_sizes = window_sizes = [10, 50] with new features: signal_shift_+2, signal_shift_-2, without 'medianbatch', 'abs_avgbatch', 'abs_maxbatch', ratio-, diff-features) & FE-3 = 76 features\n* seed_random = 42\n* LGB-2\n* lr_lgb = 0.05\n* num_leaves = 200\n* num_iterations = 2000\n* w_lgb = 0.5\n* metris are 'logloss' & f1_score\n* XGB-1\n* lr_xgb = 0.05\n* max_depth_xgb = 10\n* num_boost_round_xgb = 1000\n* w_xgb = 0.5\n* metrics is 'logloss'\n\n**LB = 0.939 (the best)**"},{"metadata":{},"cell_type":"markdown","source":"## 2. Import libraries <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression, Ridge, SGDRegressor\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV, KFold, train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, f1_score, mean_absolute_error, make_scorer\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom functools import partial\nimport scipy as sp\n\nimport time\nimport datetime\n\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 3. Download data & FE <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        if col != 'time':\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv')\ntest = pd.read_csv('/kaggle/input/liverpool-ion-switching/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE-2 - thanks to the kernels:\n* https://www.kaggle.com/teejmahal20/regression-with-optimized-rounder\n* https://www.kaggle.com/pestipeti/eda-ion-switching"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor window in window_sizes:\n    train[\"rolling_mean_\" + str(window)] = train['signal'].rolling(window=window).mean()\n    train[\"rolling_std_\" + str(window)] = train['signal'].rolling(window=window).std()\n    train[\"rolling_var_\" + str(window)] = train['signal'].rolling(window=window).var()\n    train[\"rolling_min_\" + str(window)] = train['signal'].rolling(window=window).min()\n    train[\"rolling_max_\" + str(window)] = train['signal'].rolling(window=window).max()\n    \n    #train[\"rolling_min_max_ratio_\" + str(window)] = train[\"rolling_min_\" + str(window)] / train[\"rolling_max_\" + str(window)]\n    #train[\"rolling_min_max_diff_\" + str(window)] = train[\"rolling_max_\" + str(window)] - train[\"rolling_min_\" + str(window)]\n    \n    a = (train['signal'] - train['rolling_min_' + str(window)]) / (train['rolling_max_' + str(window)] - train['rolling_min_' + str(window)])\n    train[\"norm_\" + str(window)] = a * (np.floor(train['rolling_max_' + str(window)]) - np.ceil(train['rolling_min_' + str(window)]))\n    \ntrain = train.replace([np.inf, -np.inf], np.nan)    \ntrain.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor window in window_sizes:\n    test[\"rolling_mean_\" + str(window)] = test['signal'].rolling(window=window).mean()\n    test[\"rolling_std_\" + str(window)] = test['signal'].rolling(window=window).std()\n    test[\"rolling_var_\" + str(window)] = test['signal'].rolling(window=window).var()\n    test[\"rolling_min_\" + str(window)] = test['signal'].rolling(window=window).min()\n    test[\"rolling_max_\" + str(window)] = test['signal'].rolling(window=window).max()\n    \n    #test[\"rolling_min_max_ratio_\" + str(window)] = test[\"rolling_min_\" + str(window)] / test[\"rolling_max_\" + str(window)]\n    #test[\"rolling_min_max_diff_\" + str(window)] = test[\"rolling_max_\" + str(window)] - test[\"rolling_min_\" + str(window)]\n\n    \n    a = (test['signal'] - test['rolling_min_' + str(window)]) / (test['rolling_max_' + str(window)] - test['rolling_min_' + str(window)])\n    test[\"norm_\" + str(window)] = a * (np.floor(test['rolling_max_' + str(window)]) - np.ceil(test['rolling_min_' + str(window)]))\n\ntest = test.replace([np.inf, -np.inf], np.nan)    \ntest.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FE-3 - thanks to \n* https://www.kaggle.com/jazivxt/physically-possible\n* https://www.kaggle.com/siavrez/simple-eda-model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index // 25_000\n    df['batch_index'] = df.index  - (df.batch * 25_000)\n    df['batch_slices'] = df['batch_index']  // 2500\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean()\n        d['median'+c] = df.groupby([c])['signal'].median()\n        d['max'+c] = df.groupby([c])['signal'].max()\n        d['min'+c] = df.groupby([c])['signal'].min()\n        d['std'+c] = df.groupby([c])['signal'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        d['range'+c] = d['max'+c] - d['min'+c]\n        d['maxtomin'+c] = d['max'+c] / d['min'+c]\n        d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) / 2\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n\n    \n    # add shifts_1\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n    \n    # add shifts_2 - my upgrade\n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==1].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-2'][i] = np.nan\n    for i in df[df['batch_index']==49998].index:\n        df['signal_shift_-2'][i] = np.nan\n    \n    df = df.drop(columns=['batch', 'batch_index', 'batch_slices', 'batch_slices2'])\n\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels']]:\n        df[c+'_msignal'] = df[c] - df['signal']\n        \n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    gc.collect()\n    return df\n\ntrain = features(train)\ntest = features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['open_channels']\ncol = [c for c in train.columns if c not in ['time', 'open_channels', 'group', 'medianbatch', 'abs_avgbatch', 'abs_maxbatch']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Model tuning <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### Standardization for regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# scaler = StandardScaler()\n# train_reg = pd.DataFrame(\n#     scaler.fit_transform(train[col]),\n#     columns=train[col].columns,\n#     index=train[col].index\n# )\n# test_reg = pd.DataFrame(\n#     scaler.transform(test[col]),\n#     columns=test[col].columns,\n#     index=test[col].index\n# )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Ridge Regression<a class=\"anchor\" id=\"4.1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# ridge_reg = Ridge(tol=5e-5, fit_intercept=False, random_state=seed_random)\n# ridge_reg.fit(train_reg, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_train_ridge = ridge_reg.predict(train_reg)\n# y_preds_ridge = ridge_reg.predict(test_reg)\n# y_train_ridge = np.clip(y_train_ridge, 0, 10).astype(int)\n# y_preds_ridge = np.clip(y_preds_ridge, 0, 10).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 SGDRegressor <a class=\"anchor\" id=\"4.2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# sgd = SGDRegressor(learning_rate = 'adaptive', fit_intercept=False, random_state=seed_random)\n# sgd.fit(train_reg, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_train_sgd = sgd.predict(train_reg)\n# y_preds_sgd = sgd.predict(test_reg)\n# y_train_sgd = np.clip(y_train_sgd, 0, 10).astype(int)\n# y_preds_sgd = np.clip(y_preds_sgd, 0, 10).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Logistic Regression <a class=\"anchor\" id=\"4.3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# logreg = LogisticRegression(random_state=seed_random)\n# logreg.fit(train_reg, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_logreg_train = logreg.predict(train_reg)\n# y_logreg_pred = logreg.predict(test_reg)\n# y_logreg_train = np.clip(y_logreg_train, 0, 10).astype(int)\n# y_logreg_pred = np.clip(y_logreg_pred, 0, 10).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 MLP <a class=\"anchor\" id=\"4.4\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # https://www.kaggle.com/vbmokin/automatic-selection-from-20-classifier-models\n# # https://www.kaggle.com/vbmokin/bod-prediction-in-river-15-regression-models\n# mlp = MLPClassifier()\n# f1scoring = make_scorer(f1_score, average = 'macro')\n# param_grid = {'hidden_layer_sizes': [i for i in range(n_layer_0, n_layer_max)],\n#               'activation': ['relu'],\n#               'solver': ['adam'],\n#               'learning_rate': ['constant'],\n#               'learning_rate_init': [lr_mlp],\n#               'power_t': [0.5],\n#               'alpha': [0.0001],\n#               'max_iter': [1000],\n#               'early_stopping': [True],\n#               'warm_start': [False],\n#               'random_state': [seed_random]}\n# mlp_GS = GridSearchCV(mlp, param_grid=param_grid, scoring=f1scoring,\n#                    cv=cv_mlp, verbose=True, pre_dispatch='2*n_jobs')\n# mlp_GS.fit(train_reg, y)\n# print(\"Best parameters set:\", mlp_GS.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_mlp_pred = mlp_GS.predict(test_reg)\n# y_pred_train_mlp = mlp_GS.predict(train_reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del train_reg, test_reg\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 LGB <a class=\"anchor\" id=\"4.5\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/siavrez/simple-eda-model\ndef MacroF1Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average = 'macro')\n    return ('MacroF1Metric', score, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Thanks to https://www.kaggle.com/jazivxt/physically-possible with tuning from https://www.kaggle.com/siavrez/simple-eda-model and my tuning\nX_train, X_valid, y_train, y_valid = train_test_split(train[col], y, test_size=0.3, random_state=seed_random)\nparams = {'learning_rate': lr_lgb, \n          'max_depth': -1, \n          'num_leaves': num_leaves,\n          'metric': 'logloss', \n          'random_state': seed_random, \n          'n_jobs':-1, \n          'sample_fraction':0.33}\nmodel = lgb.train(params, lgb.Dataset(X_train, y_train), num_iterations, lgb.Dataset(X_valid, y_valid), verbose_eval=25, early_stopping_rounds=200, feval=MacroF1Metric)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ny_lgb_pred = model.predict(test[col], num_iteration=model.best_iteration)\ny_pred_train_lgb = model.predict(train[col], num_iteration=model.best_iteration)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('LGB score {0:.4f}'.format(np.mean(f1_score(y, np.round(np.clip(y_pred_train_lgb,0,10)).astype(int), average=\"macro\"))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(model,ax = axes,height = 0.5)\nplt.show();plt.close()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6 XGB <a class=\"anchor\" id=\"4.6\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/teejmahal20/3-simple-ideas-ensemble\ntrain_set = xgb.DMatrix(X_train, y_train)\nval_set = xgb.DMatrix(X_valid, y_valid)\ndel X_train, X_valid, y_train, y_valid\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams_xgb = {'colsample_bytree': 0.375,\n              'learning_rate': lr_xgb,\n              'max_depth': max_depth_xgb, \n              'subsample': 1, \n              'objective':'reg:squarederror',\n              'eval_metric':'logloss'}\n\nmodelx = xgb.train(params_xgb, train_set, num_boost_round=num_boost_round_xgb, evals=[(train_set, 'train'), (val_set, 'val')], \n                                     verbose_eval=25, early_stopping_rounds=200)\ndel train_set, val_set\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ny_xgb_pred = modelx.predict(xgb.DMatrix(test[col]))\ny_pred_train_xgb = modelx.predict(xgb.DMatrix(train[col]))\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('XGB score {0:.4f}'.format(np.mean(f1_score(y, np.round(np.clip(y_pred_train_xgb,0,10)).astype(int), average=\"macro\"))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Showing Confusion Matrices <a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix\n# Thanks to https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(14,14)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for Ridge model\n#plot_cm(y, y_train_ridge, 'Confusion matrix for Ridge model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for SGDRegressor model\n#plot_cm(y, y_train_sgd, 'Confusion matrix for SGDRegressor model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Showing Confusion Matrix for Logistic Regression\n# plot_cm(y, y_logreg_train, 'Confusion matrix for Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for LGB model\nplot_cm(y, y_pred_train_lgb, 'Confusion matrix for LGB model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for LGB model\nplot_cm(y, y_pred_train_xgb, 'Confusion matrix for XGB model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Showing Confusion Matrix for MLP model\n# plot_cm(y, y_pred_train_mlp, 'Confusion matrix for MLP model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Comparison and merging solutions <a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### Merging solution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for the main solution before rounded\ny_train_preds = w_lgb*y_pred_train_lgb + w_xgb*y_pred_train_xgb\nplot_cm(y, y_train_preds, 'Confusion matrix for the main solution before rounded')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_pred_train_lgb, y_pred_train_xgb\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_preds = w_ridge*y_preds_ridge + w_sgd*y_preds_sgd + w_logreg*y_logreg_pred + w_lgb*y_lgb_pred\n#y_preds = w_logreg*y_logreg_pred + w_lgb*y_lgb_pred + w_mlp*y_mlp_pred\ny_preds = w_lgb*y_lgb_pred + w_xgb*y_xgb_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_lgb_pred, y_xgb_pred\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Processing prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_proc(pred):\n    pred = np.round(np.clip(pred, 0, 10))\n    return pred.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction processing for the main solution\ny_preds = pred_proc(y_preds)\ny_train_preds = pred_proc(y_train_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building Confusion matrices for processing solutions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for processing solution of Ridge model\n#plot_cm(y, pred_proc(y_train_ridge), 'Confusion matrix for processing solution of Ridge model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for processing solution of SGDRegression model\n#plot_cm(y, pred_proc(y_train_sgd), 'Confusion matrix for processing solution of SGDRegression model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Showing Confusion Matrix for processing solution of Logistic Regression model\n# plot_cm(y, pred_proc(y_logreg_train), 'Confusion matrix for processing solution of Logistic Regression model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Showing Confusion Matrix for processing solution of LGB model\n# plot_cm(y, pred_proc(y_pred_train_lgb), 'Confusion matrix for processing solution of LGB model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Showing Confusion Matrix for processing solution of MLP model\n# plot_cm(y, pred_proc(y_pred_train_mlp), 'Confusion matrix for processing solution of MLP model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for the processing main solution\nplot_cm(y, y_train_preds, 'Confusion matrix for the processing main solution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The main solution score {0:.4f}'.format(np.mean(f1_score(y, y_train_preds, average=\"macro\"))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_train_preds\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Submission <a class=\"anchor\" id=\"7\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"test['open_channels'] = y_preds\ntest[['time','open_channels']].to_csv('submission.csv', index=False, float_format='%.4f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.time.values[:10])\nprint(test['open_channels'].mean())\nprint(test['open_channels'].hist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"I hope you find this kernel useful and enjoyable."},{"metadata":{},"cell_type":"markdown","source":"Your comments and feedback are most welcome."},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}