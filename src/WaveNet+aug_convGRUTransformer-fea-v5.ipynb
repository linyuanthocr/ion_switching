{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WaveNet+aug_convGRUTransformer-fea-v5.ipynb","provenance":[{"file_id":"1ifXs8aFubfj8NI2KCVNdzPm9UbBKws57","timestamp":1589498786513},{"file_id":"1j5uOsr_vIOinvEsNvGuxpRA7aATgfPHD","timestamp":1589052056626},{"file_id":"1AI_WQ0xVmMSrBNRfCEAz08SnkeWyVMm_","timestamp":1588563626234}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"huixWaL42uZi","colab_type":"text"},"source":["The validation scheme is based on [seq2seq-rnn-with-gru](https://www.kaggle.com/brandenkmurray/seq2seq-rnn-with-gru/output), and cleaned data is from [data-without-drift](https://www.kaggle.com/cdeotte/data-without-drift) and Kalman filter is from [https://www.kaggle.com/teejmahal20/single-model-lgbm-kalman-filter](single-model-lgbm-kalman-filter) and the added feature is from [wavenet-with-1-more-feature](wavenet-with-1-more-feature). I also used ragnar's data in this version [clean-kalman](https://www.kaggle.com/ragnar123/clean-kalman). The Wavenet is based on [https://github.com/philipperemy/keras-tcn](https://github.com/philipperemy/keras-tcn), [https://github.com/peustr/wavenet](https://github.com/peustr/wavenet) and [https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet) and also [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm). If any refrence is not mentioned it was not intentional, please add them in comments.\n","\n","Previous versions were mainly based on [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm)  "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.121605Z","start_time":"2020-04-11T02:14:22.792317Z"},"_kg_hide-input":true,"id":"LqmWjeYJ2uZn","trusted":true,"colab_type":"code","colab":{}},"source":["!pip install --no-warn-conflicts -q tensorflow-addons"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DbSxqaBWiqQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.152214Z","start_time":"2020-04-11T02:14:24.125295Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"y1qOuodBfSxN","trusted":true,"colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import (TimeDistributed, Dropout, BatchNormalization, Flatten, Convolution1D, Activation, Input, Dense, LSTM, Lambda, Bidirectional,\n","                                     Add, AveragePooling1D, Multiply, GRU, GRUCell, LSTMCell, SimpleRNNCell, SimpleRNN, TimeDistributed, RNN,SpatialDropout1D,\n","                                     RepeatVector, Conv1D, MaxPooling1D, GlobalMaxPooling1D,Concatenate, GlobalAveragePooling1D, UpSampling1D)\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler, CSVLogger\n","from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy, mean_squared_error\n","# from tensorflow.keras.experimental import export_saved_model, load_from_saved_model\n","from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n","from tensorflow.keras.utils import Sequence, to_categorical\n","from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n","from tensorflow.keras import losses, models, optimizers\n","from tensorflow.keras import backend as K\n","from tensorflow.python.ops import array_ops\n","import tensorflow as tf\n","from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n","from sklearn.metrics import f1_score, cohen_kappa_score, mean_squared_error\n","from logging import getLogger, Formatter, StreamHandler, FileHandler, INFO, DEBUG, NOTSET\n","from sklearn.model_selection import KFold, GroupKFold\n","from tqdm import tqdm_notebook as tqdm\n","from contextlib import contextmanager\n","from joblib import Parallel, delayed\n","from IPython.display import display\n","from sklearn import preprocessing\n","from sklearn.utils import class_weight\n","import tensorflow_addons as tfa\n","import scipy.stats as stats\n","import random as rn\n","import pandas as pd\n","import numpy as np\n","import scipy as sp\n","import itertools\n","import warnings\n","import time\n","import pywt\n","import os\n","import gc\n","\n","from tensorflow.keras.metrics import Precision, Recall\n","from scipy import signal\n","\n","# from tensorflow_addons.metrics import F1Score\n","\n","warnings.simplefilter('ignore')\n","warnings.filterwarnings('ignore')\n","pd.set_option('display.max_columns', 1000)\n","pd.set_option('display.max_rows', 500)\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"AVvcAEP0iEyp","colab_type":"code","colab":{}},"source":["Kaggle = False\n","Colab = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WqtT_-ofmSz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"35082ea3-448e-4c0b-8311-7fa3e47f4811","executionInfo":{"status":"ok","timestamp":1589946385346,"user_tz":420,"elapsed":10021,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","print(gpu_info)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Wed May 20 03:46:23 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LC8XqE14cSRm","trusted":true,"colab_type":"code","outputId":"4c40f935-f741-4d11-b791-77f7a0d424b0","executionInfo":{"status":"ok","timestamp":1589946385347,"user_tz":420,"elapsed":9963,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os, sys\n","from pathlib import Path\n","\n","if Colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    path = \"/content/drive/My Drive\"\n","\n","    os.chdir(path)\n","    os.listdir(path)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z0fJcaGLoPY3","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-sYfdXdWyTI","trusted":true,"colab_type":"code","colab":{}},"source":["# sys.path.append('ion_switch/keras-one-cycle')\n","# # os.listdir(patholr)\n","# from clr import OneCycleLR"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.159766Z","start_time":"2020-04-11T02:14:24.155918Z"},"trusted":true,"id":"zdosugVWcOf0","colab_type":"code","colab":{}},"source":["if Kaggle:\n","    PATH = '/kaggle/input/'\n","    outdir = '.'\n","# PATH = '/Users/helen/Desktop/Data/'\n","else:\n","    PATH = 'ion_switch/'\n","    outdir = Path(PATH+'res')\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)\n","    outdir = Path(PATH+'res/wavenet-dlr-res')\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.168634Z","start_time":"2020-04-11T02:14:24.163612Z"},"id":"UafJMtyefSxU","trusted":true,"colab_type":"code","colab":{}},"source":["EPOCHS=180\n","NNBATCHSIZE= 16\n","BATCHSIZE = 4000\n","SEED = 321\n","SELECT = True\n","SPLITS = 5\n","# LR = 4e-5 // for folder 1\n","# WD = 5e-6\n","LR = 0.001\n","WD = 1e-5\n","Gamma = 1.0 #0.99994\n","BETA = 0.99996\n","fe_config = [\n","    (True, BATCHSIZE),\n","]\n","TREES = 100\n","DEPTH = 12\n","COMPETITION = 'ION-Switching'\n","logger = getLogger(COMPETITION)\n","LOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\n","MODELNAME = 'WaveNetAug-ConvGRUtfProb11-NESCAL-MFEA'\n","weights = []\n","TRAINEDMODEL = os.path.join(outdir, 'wavenet-es-v1/wavenet_es_f0_checkpoint.h5')\n","MODEL_PATH = os.path.join(outdir, 'WaveNet-DLR-FOCALLOSS-ClassifierCBRW-Prob11-NESCAL_len4000_bs16_lr0.001_tn100_dn12','all')\n","\n","VERSION = '{}'.format(MODELNAME)\n","outdir = os.path.join(outdir, VERSION)\n","if not os.path.exists(outdir):\n","    os.mkdir(outdir)\n","\n","from datetime import datetime\n","dateTimeObj = datetime.now()\n","# timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H\")\n","timestampStr = 'all-al'\n","outdir = os.path.join(outdir, timestampStr)\n","if not os.path.exists(outdir):\n","    os.mkdir(outdir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fouKVpNaFNKQ","trusted":true,"colab_type":"code","colab":{}},"source":["\n","@contextmanager\n","def timer(name : Text):\n","    t0 = time.time()\n","    yield\n","    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.177661Z","start_time":"2020-04-11T02:14:24.171732Z"},"id":"EE4v8h1tfSxb","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def init_logger():\n","\n","    handler = StreamHandler()\n","    handler.setLevel(INFO)\n","    handler.setFormatter(Formatter(LOGFORMAT))\n","    fh_handler = FileHandler(os.path.join(outdir,'{}-len{}-lr{}-{}.log'.format(MODELNAME,BATCHSIZE,LR,timestampStr)))\n","    fh_handler.setFormatter(Formatter(LOGFORMAT))\n","    logger.setLevel(INFO)\n","    logger.addHandler(handler)\n","    logger.addHandler(fh_handler)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.358957Z","start_time":"2020-04-11T02:14:24.187096Z"},"id":"OC5DOcDifSxx","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def seed_everything(seed : int) -> NoReturn :\n","    \n","    rn.seed(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    tf.random.set_seed(seed)\n","    # os.environ['TF_CUDNN_DETERMINISTIC'] = str(seed) \n","\n","seed_everything(SEED)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5tInVDxBn-lQ","trusted":true,"colab_type":"code","colab":{}},"source":["class CyclicLR(tf.keras.callbacks.Callback):\n","\n","    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n","                 gamma=1., scale_fn=None, scale_mode='cycle'):\n","        super(CyclicLR, self).__init__()\n","\n","        self.base_lr = base_lr\n","        self.max_lr = max_lr\n","        self.step_size = step_size\n","        self.mode = mode\n","        self.gamma = gamma\n","        if scale_fn == None:\n","            if self.mode == 'triangular':\n","                self.scale_fn = lambda x: 1.\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'triangular2':\n","                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'exp_range':\n","                self.scale_fn = lambda x: gamma ** (x)\n","                self.scale_mode = 'iterations'\n","        else:\n","            self.scale_fn = scale_fn\n","            self.scale_mode = scale_mode\n","        self.clr_iterations = 0.\n","        self.trn_iterations = 0.\n","        self.history = {}\n","\n","        self._reset()\n","\n","    def _reset(self, new_base_lr=None, new_max_lr=None,\n","               new_step_size=None):\n","        \"\"\"Resets cycle iterations.\n","        Optional boundary/step size adjustment.\n","        \"\"\"\n","        if new_base_lr != None:\n","            self.base_lr = new_base_lr\n","        if new_max_lr != None:\n","            self.max_lr = new_max_lr\n","        if new_step_size != None:\n","            self.step_size = new_step_size\n","        self.clr_iterations = 0.\n","\n","    def clr(self):\n","        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n","        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n","        if self.scale_mode == 'cycle':\n","            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n","        else:\n","            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n","                self.clr_iterations)\n","\n","    def on_train_begin(self, logs={}):\n","        logs = logs or {}\n","\n","        if self.clr_iterations == 0:\n","            K.set_value(self.model.optimizer.lr, self.base_lr)\n","        else:\n","            K.set_value(self.model.optimizer.lr, self.clr())\n","\n","    def on_batch_end(self, epoch, logs=None):\n","\n","        logs = logs or {}\n","        self.trn_iterations += 1\n","        self.clr_iterations += 1\n","\n","        K.set_value(self.model.optimizer.lr, self.clr())\n","        # print(\"learning rate- self.model.optimizer.lr: \", self.model.optimizer.lr)\n","\n","    # def on_epoch_end(self, epoch, logs=None):\n","\n","    #     logs = logs or {}\n","    #     self.trn_iterations += 1\n","    #     self.clr_iterations += 1\n","\n","    #     K.set_value(self.model.optimizer.lr, self.clr())\n","    #     logger.info(f'epoch:{epoch:03d},'+str(K.eval(self.model.optimizer.lr)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmKgNg1Mmzrk","trusted":true,"colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","# 写一个LossHistory类，保存训练集的loss和acc\n","# 当然我也可以完全不这么做，可以直接使用model.fit()方法返回的 history对象去做\n","'''Callback有6个常用的方法，这里实现其中的四个\n","    def on_epoch_begin(self, epoch, logs=None):\n","    def on_epoch_end(self, epoch, logs=None):\n","    def on_batch_begin(self, batch, logs=None):\n","    def on_batch_end(self, batch, logs=None):\n","    def on_train_begin(self, logs=None):\n","    def on_train_end(self, logs=None):\n","'''\n","class LossHistory(Callback):  # 继承自Callback类\n"," \n","    '''\n","    在模型开始的时候定义四个属性，每一个属性都是字典类型，存储相对应的值和epoch\n","    '''\n","    def on_train_begin(self, logs={}):\n","        self.losses = {'batch':[], 'epoch':[]}\n","        self.accuracy = {'batch':[], 'epoch':[]}\n","        self.val_loss = {'batch':[], 'epoch':[]}\n","        self.val_acc = {'batch':[], 'epoch':[]}\n"," \n","    # 在每一个batch结束后记录相应的值\n","    def on_batch_end(self, batch, logs={}):\n","        self.losses['batch'].append(logs.get('loss'))\n","        self.accuracy['batch'].append(logs.get('accuracy'))\n","        self.val_loss['batch'].append(logs.get('val_loss'))\n","        self.val_acc['batch'].append(logs.get('val_accuracy'))\n","    \n","    # 在每一个epoch之后记录相应的值\n","    def on_epoch_end(self, epoch, logs={}):\n","        trloss, tracc, vloss, vacc = logs.get('loss'), logs.get('accuracy'), logs.get('val_loss'), logs.get('val_accuracy')\n","        self.losses['epoch'].append(trloss)\n","        self.accuracy['epoch'].append(tracc)\n","        self.val_loss['epoch'].append(vloss)\n","        self.val_acc['epoch'].append(vacc)\n","        logger.info(\"epoch:{:03d}, train_loss:{:1.5f}, train_acc:{:1.5f}, val_loss:{:1.5f}, val_acc:{:1.5f}\".format(epoch, \n","                                                                                                                trloss, tracc, vloss, vacc))\n"," \n","    def loss_plot(self, loss_type, pngname):\n","        '''\n","        loss_type：指的是 'epoch'或者是'batch'，分别表示是一个batch之后记录还是一个epoch之后记录\n","        '''\n","        iters = range(len(self.losses[loss_type]))\n","        plt.figure()\n","        # acc\n","        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n","        # loss\n","        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n","        if loss_type == 'epoch':\n","            # val_acc\n","            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n","            # val_loss\n","            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n","        plt.grid(True)\n","        plt.xlabel(loss_type)\n","        plt.ylabel('acc-loss')\n","        plt.legend(loc=\"upper right\")\n","        plt.savefig(pngname)\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.368249Z","start_time":"2020-04-11T02:14:24.362616Z"},"id":"adUHGQUTfSyA","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def read_data(base : os.path.abspath) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n","    \n","    train = pd.read_csv(PATH+'clean-kalman/train_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n","    test  = pd.read_csv(PATH+'clean-kalman/test_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32})\n","    sub  = pd.read_csv(PATH+'liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n","    train_prob = pd.read_csv(PATH+'CnnGRUTransformer-Prob11-NESCAL_tranformerprobv2.csv', index_col=False, dtype=np.float32)\n","    test_prob = pd.read_csv(PATH+'CnnGRUTransformer-Prob11-NESCAL_test_tranformerprobv2.csv', index_col=False, dtype=np.float32) \n","    return train, test, sub, train_prob, test_prob\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssU24ATw_hd4","colab_type":"code","colab":{}},"source":["def read_datanew(base : os.path.abspath) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n","    \n","    train = pd.read_csv('train_kalman_clean_multifeas.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n","    test  = pd.read_csv('test_kalman_clean_multifeas.csv', dtype={'time': np.float32, 'signal': np.float32})\n","    sub  = pd.read_csv(PATH+'liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n","    return train, test, sub"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.376185Z","start_time":"2020-04-11T02:14:24.371687Z"},"id":"HpDaJQ5yfSyI","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def batching(df : pd.DataFrame,\n","             batch_size : int) -> pd.DataFrame :\n","    \n","    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n","    df['group'] = df['group'].astype(np.uint16)\n","        \n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.391087Z","start_time":"2020-04-11T02:14:24.378989Z"},"id":"iQxOYF3tfSyj","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def reduce_mem_usage(df: pd.DataFrame,\n","                     verbose: bool = True) -> pd.DataFrame:\n","    \n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2\n","\n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","\n","            if str(col_type)[:3] == 'int':\n","\n","                if (c_min > np.iinfo(np.int32).min\n","                      and c_max < np.iinfo(np.int32).max):\n","                    df[col] = df[col].astype(np.int32)\n","                elif (c_min > np.iinfo(np.int64).min\n","                      and c_max < np.iinfo(np.int64).max):\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                if (c_min > np.finfo(np.float16).min\n","                        and c_max < np.finfo(np.float16).max):\n","                    df[col] = df[col].astype(np.float16)\n","                elif (c_min > np.finfo(np.float32).min\n","                      and c_max < np.finfo(np.float32).max):\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    reduction = (start_mem - end_mem) / start_mem\n","\n","    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n","    if verbose:\n","        print(msg)\n","\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.402186Z","start_time":"2020-04-11T02:14:24.393787Z"},"id":"dytmNmL_fSzj","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def lag_with_pct_change(df : pd.DataFrame,\n","                        shift_sizes : Optional[List]=[1, 2],\n","                        add_pct_change : Optional[bool]=False,\n","                        add_pct_change_lag : Optional[bool]=False,\n","                        add_diff : Optional[bool]=False) -> pd.DataFrame:\n","    \n","    for shift_size in shift_sizes:    \n","        df['signal_shift_pos_'+str(shift_size)] = df.groupby('group')['signal'].shift(shift_size).fillna(0)\n","        df['signal_shift_neg_'+str(shift_size)] = df.groupby('group')['signal'].shift(-1*shift_size).fillna(0)\n","\n","    if add_pct_change:\n","        df['pct_change'] = df['signal'].pct_change()\n","        if add_pct_change_lag:\n","            for shift_size in shift_sizes:    \n","                df['pct_change_shift_pos_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(shift_size).fillna(0)\n","                df['pct_change_shift_neg_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(-1*shift_size).fillna(0)\n","    if add_diff:\n","        for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'group', 'index']]:\n","            df[c+'_msignal'] = df[c] - df['signal']\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFFIxs0eowVn","colab_type":"code","colab":{}},"source":["def calc_gradients(s, n_grads=4):\n","    '''\n","    Calculate gradients for a pandas series. Returns the same number of samples\n","    '''\n","    grads = pd.DataFrame()\n","    \n","    g = s.values\n","    for i in range(n_grads):\n","        g = np.gradient(g)\n","        grads['grad_' + str(i+1)] = g\n","        \n","    return grads"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXTybY2Mo-A7","colab_type":"code","colab":{}},"source":["def calc_low_pass(s, n_filts=10):\n","    '''\n","    Applies low pass filters to the signal. Left delayed and no delayed\n","    '''\n","    wns = np.logspace(-2, -0.3, n_filts)\n","    \n","    low_pass = pd.DataFrame()\n","    x = s.values\n","    for wn in wns:\n","        b, a = signal.butter(1, Wn=wn, btype='low')\n","        zi = signal.lfilter_zi(b, a)\n","        low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n","        low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n","        \n","    return low_pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9Kkm6fjpDna","colab_type":"code","colab":{}},"source":["def calc_high_pass(s, n_filts=10):\n","    '''\n","    Applies high pass filters to the signal. Left delayed and no delayed\n","    '''\n","    wns = np.logspace(-2, -0.1, n_filts)\n","    \n","    high_pass = pd.DataFrame()\n","    x = s.values\n","    for wn in wns:\n","        b, a = signal.butter(1, Wn=wn, btype='high')\n","        zi = signal.lfilter_zi(b, a)\n","        high_pass['highpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n","        high_pass['highpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n","        \n","    return high_pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qp-rat_vpGnR","colab_type":"code","colab":{}},"source":["def calc_roll_stats(s, windows=[10, 50, 100, 500, 1000]):\n","    '''\n","    Calculates rolling stats like mean, std, min, max...\n","    '''\n","    roll_stats = pd.DataFrame()\n","    for w in windows:\n","        roll_stats['roll_mean_' + str(w)] = s.rolling(window=w, min_periods=1).mean()\n","        roll_stats['roll_std_' + str(w)] = s.rolling(window=w, min_periods=1).std()\n","        roll_stats['roll_min_' + str(w)] = s.rolling(window=w, min_periods=1).min()\n","        roll_stats['roll_max_' + str(w)] = s.rolling(window=w, min_periods=1).max()\n","        roll_stats['roll_range_' + str(w)] = roll_stats['roll_max_' + str(w)] - roll_stats['roll_min_' + str(w)]\n","        roll_stats['roll_q10_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.10)\n","        roll_stats['roll_q25_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.25)\n","        roll_stats['roll_q50_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.50)\n","        roll_stats['roll_q75_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.75)\n","        roll_stats['roll_q90_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.90)\n","    \n","    # add zeros when na values (std)\n","    # roll_stats = roll_stats.fillna(value=0)\n","             \n","    return roll_stats"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"peEFdgg6pLS3","colab_type":"code","colab":{}},"source":["def calc_ewm(s, windows=[10, 50, 100, 500, 1000]):\n","    '''\n","    Calculates exponential weighted functions\n","    '''\n","    ewm = pd.DataFrame()\n","    for w in windows:\n","        ewm['ewm_mean_' + str(w)] = s.ewm(span=w, min_periods=1).mean()\n","        ewm['ewm_std_' + str(w)] = s.ewm(span=w, min_periods=1).std()\n","        \n","    # add zeros when na values (std)\n","    # ewm = ewm.fillna(value=0)\n","        \n","    return ewm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFOtNM86xnhU","colab_type":"code","colab":{}},"source":["def add_features(s):\n","    '''\n","    All calculations together\n","    '''\n","    gradients = calc_gradients(s)\n","    # print(\"gradients.shape:\", gradients.shape)\n","    low_pass = calc_low_pass(s)\n","    # print(\"low_pass.shape:\", low_pass.shape)\n","    high_pass = calc_high_pass(s)\n","    # print(\"high_pass.shape:\", high_pass.shape)\n","    roll_stats = calc_roll_stats(s)\n","    # print(\"roll_stats.shape:\", roll_stats.shape)\n","    ewm = calc_ewm(s)\n","    # print(\"ewm.shape:\", ewm.shape)\n","    return pd.concat([gradients, low_pass, high_pass, roll_stats, ewm], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.409381Z","start_time":"2020-04-11T02:14:24.404800Z"},"id":"m-ULWLF_fS0B","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def run_feat_enginnering(df : pd.DataFrame,\n","                         create_all_data_feats : bool,\n","                         batch_size : int) -> pd.DataFrame:\n","    # print(\"df.shape 1 :\", df.shape)\n","    df = batching(df, batch_size=batch_size)\n","    if create_all_data_feats:\n","        df = lag_with_pct_change(df, [1, 2, 3],  add_pct_change=False, add_pct_change_lag=False, add_diff=False)\n","    df['signal_2'] = df['signal'] ** 2\n","    s = df['signal']\n","    # print(\"s.shape:\", s.shape)\n","    # print(\"batch_size:\", batch_size)\n","    ls = []\n","    for i in tqdm(range(int(s.shape[0]/batch_size))):\n","        sig = s[i*batch_size:(i+1)*batch_size].copy().reset_index(drop=True)\n","        sig_featured = add_features(sig)\n","        ls.append(sig_featured)\n","    # print(np.shape(ls))\n","    pdls =  pd.concat(ls, axis=0)\n","    # print(\"pdls.shape\", pdls.shape)\n","    # print(\"df.shape:\", df.shape)\n","    # print(df.columns)\n","    # print(pdls.columns)\n","    return pd.concat([df.reset_index(drop=True), pdls.reset_index(drop=True)], axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.419991Z","start_time":"2020-04-11T02:14:24.412368Z"},"id":"6E87jcu2fS0O","trusted":true,"colab_type":"code","colab":{}},"source":["def feature_selection(df : pd.DataFrame,\n","                      df_test : pd.DataFrame) -> Tuple[pd.DataFrame , pd.DataFrame, List]:\n","    use_cols = [col for col in df.columns if col not in ['index','group', 'open_channels', 'time']]\n","    print(use_cols)\n","    df = df.replace([np.inf, -np.inf], np.nan)\n","    df_test = df_test.replace([np.inf, -np.inf], np.nan)\n","    for col in use_cols:\n","        col_mean = pd.concat([df[col], df_test[col]], axis=0).mean()\n","        df[col] = df[col].fillna(col_mean)\n","        df_test[col] = df_test[col].fillna(col_mean)\n","   \n","    gc.collect()\n","    return df, df_test, use_cols\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"41LbkJYqxJlV","trusted":true,"colab_type":"code","colab":{}},"source":["def jitter(x, sigma=0.03):\n","    # print(\"jitter x shape:\", x.shape)\n","    # https://arxiv.org/pdf/1706.00527.pdf\n","    return x + np.random.normal(loc=0., scale=sigma, size=x.shape)\n","\n","def scaling(x, sigma=0.1):\n","    # https://arxiv.org/pdf/1706.00527.pdf\n","    factor = np.random.normal(loc=1., scale=sigma, size=(x.shape[0],x.shape[2]))\n","    return np.multiply(x, factor[:,np.newaxis,:])\n","\n","def rotation(x):\n","    flip = np.random.choice([-1, 1], size=(x.shape[0],x.shape[2]))\n","    rotate_axis = np.arange(x.shape[2])\n","    np.random.shuffle(rotate_axis)    \n","    return flip[:,np.newaxis,:] * x[:,:,rotate_axis]\n","\n","def rotation2d(x, sigma=0.2):\n","    thetas = np.random.normal(loc=0, scale=sigma, size=(x.shape[0]))\n","    c = np.cos(thetas)\n","    s = np.sin(thetas)\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        rot = np.array(((c[i], -s[i]), (s[i], c[i])))\n","        ret[i] = np.dot(pat, rot)\n","    return ret\n","\n","def permutation(x, max_segments=5, seg_mode=\"equal\"):\n","    orig_steps = np.arange(x.shape[1])\n","    \n","    num_segs = np.random.randint(1, max_segments, size=(x.shape[0]))\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        if num_segs[i] > 1:\n","            if seg_mode == \"random\":\n","                split_points = np.random.choice(x.shape[1]-2, num_segs[i]-1, replace=False)\n","                split_points.sort()\n","                splits = np.split(orig_steps, split_points)\n","            else:\n","                splits = np.array_split(orig_steps, num_segs[i])\n","            warp = np.concatenate(np.random.permutation(splits)).ravel()\n","            ret[i] = pat[warp]\n","        else:\n","            ret[i] = pat\n","    return ret\n","\n","def magnitude_warp(x, sigma=0.2, knot=4):\n","    from scipy.interpolate import CubicSpline\n","    orig_steps = np.arange(x.shape[1])\n","    \n","    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n","    warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        warper = np.array([CubicSpline(warp_steps[:,dim], random_warps[i,:,dim])(orig_steps) for dim in range(x.shape[2])]).T\n","        ret[i] = pat * warper\n","\n","    return ret\n","\n","def time_warp(x, sigma=0.2, knot=4):\n","    from scipy.interpolate import CubicSpline\n","    orig_steps = np.arange(x.shape[1])\n","    \n","    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n","    warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        for dim in range(x.shape[2]):\n","            time_warp = CubicSpline(warp_steps[:,dim], warp_steps[:,dim] * random_warps[i,:,dim])(orig_steps)\n","            scale = (x.shape[1]-1)/time_warp[-1]\n","            ret[i,:,dim] = np.interp(orig_steps, np.clip(scale*time_warp, 0, x.shape[1]-1), pat[:,dim]).T\n","    return ret\n","\n","def window_slice(x, reduce_ratio=0.9):\n","    # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n","    target_len = np.ceil(reduce_ratio*x.shape[1]).astype(int)\n","    if target_len >= x.shape[1]:\n","        return x\n","    starts = np.random.randint(low=0, high=x.shape[1]-target_len, size=(x.shape[0])).astype(int)\n","    ends = (target_len + starts).astype(int)\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        for dim in range(x.shape[2]):\n","            ret[i,:,dim] = np.interp(np.linspace(0, target_len, num=x.shape[1]), np.arange(target_len), pat[starts[i]:ends[i],dim]).T\n","    return ret\n","\n","def window_warp(x, window_ratio=0.1, scales=[0.5, 2.]):\n","    # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n","    warp_scales = np.random.choice(scales, x.shape[0])\n","    warp_size = np.ceil(window_ratio*x.shape[1]).astype(int)\n","    window_steps = np.arange(warp_size)\n","        \n","    window_starts = np.random.randint(low=1, high=x.shape[1]-warp_size-1, size=(x.shape[0])).astype(int)\n","    window_ends = (window_starts + warp_size).astype(int)\n","            \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        for dim in range(x.shape[2]):\n","            start_seg = pat[:window_starts[i],dim]\n","            window_seg = np.interp(np.linspace(0, warp_size-1, num=int(warp_size*warp_scales[i])), window_steps, pat[window_starts[i]:window_ends[i],dim])\n","            end_seg = pat[window_ends[i]:,dim]\n","            warped = np.concatenate((start_seg, window_seg, end_seg))                \n","            ret[i,:,dim] = np.interp(np.arange(x.shape[1]), np.linspace(0, x.shape[1]-1., num=warped.size), warped).T\n","    return ret\n","\n","def spawner(x, labels, sigma=0.05, verbose=0):\n","    # https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6983028/\n","    \n","    import utils.dtw as dtw\n","    random_points = np.random.randint(low=1, high=x.shape[1]-1, size=x.shape[0])\n","    window = np.ceil(x.shape[1] / 10.).astype(int)\n","    orig_steps = np.arange(x.shape[1])\n","    l = np.argmax(labels, axis=1) if labels.ndim > 1 else labels\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(tqdm(x)):\n","        # guarentees that same one isnt selected\n","        choices = np.delete(np.arange(x.shape[0]), i)\n","        # remove ones of different classes\n","        choices = np.where(l[choices] == l[i])[0]\n","        if choices.size > 0:     \n","            random_sample = x[np.random.choice(choices)]\n","            # SPAWNER splits the path into two randomly\n","            path1 = dtw.dtw(pat[:random_points[i]], random_sample[:random_points[i]], dtw.RETURN_PATH, slope_constraint=\"symmetric\", window=window)\n","            path2 = dtw.dtw(pat[random_points[i]:], random_sample[random_points[i]:], dtw.RETURN_PATH, slope_constraint=\"symmetric\", window=window)\n","            combined = np.concatenate((np.vstack(path1), np.vstack(path2+random_points[i])), axis=1)\n","            if verbose:\n","                print(random_points[i])\n","                dtw_value, cost, DTW_map, path = dtw.dtw(pat, random_sample, return_flag = dtw.RETURN_ALL, slope_constraint=slope_constraint, window=window)\n","                dtw.draw_graph1d(cost, DTW_map, path, pat, random_sample)\n","                dtw.draw_graph1d(cost, DTW_map, combined, pat, random_sample)\n","            mean = np.mean([pat[combined[0]], random_sample[combined[1]]], axis=0)\n","            for dim in range(x.shape[2]):\n","                ret[i,:,dim] = np.interp(orig_steps, np.linspace(0, x.shape[1]-1., num=mean.shape[0]), mean[:,dim]).T\n","        else:\n","            print(\"There is only one pattern of class %d, skipping pattern average\"%l[i])\n","            ret[i,:] = pat\n","    return jitter(ret, sigma=sigma)\n","\n","def wdba(x, labels, batch_size=6, slope_constraint=\"symmetric\", use_window=True):\n","    # https://ieeexplore.ieee.org/document/8215569\n","    \n","    import utils.dtw as dtw\n","    \n","    if use_window:\n","        window = np.ceil(x.shape[1] / 10.).astype(int)\n","    else:\n","        window = None\n","    orig_steps = np.arange(x.shape[1])\n","    l = np.argmax(labels, axis=1) if labels.ndim > 1 else labels\n","        \n","    ret = np.zeros_like(x)\n","    for i in tqdm(range(ret.shape[0])):\n","        # get the same class as i\n","        choices = np.where(l == l[i])[0]\n","        if choices.size > 0:        \n","            # pick random intra-class pattern\n","            k = min(choices.size, batch_size)\n","            random_prototypes = x[np.random.choice(choices, k, replace=False)]\n","            \n","            # calculate dtw between all\n","            dtw_matrix = np.zeros((k, k))\n","            for p, prototype in enumerate(random_prototypes):\n","                for s, sample in enumerate(random_prototypes):\n","                    if p == s:\n","                        dtw_matrix[p, s] = 0.\n","                    else:\n","                        dtw_matrix[p, s] = dtw.dtw(prototype, sample, dtw.RETURN_VALUE, slope_constraint=slope_constraint, window=window)\n","                        \n","            # get medoid\n","            medoid_id = np.argsort(np.sum(dtw_matrix, axis=1))[0]\n","            nearest_order = np.argsort(dtw_matrix[medoid_id])\n","            medoid_pattern = random_prototypes[medoid_id]\n","            \n","            # start weighted DBA\n","            average_pattern = np.zeros_like(medoid_pattern)\n","            weighted_sums = np.zeros((medoid_pattern.shape[0]))\n","            for nid in nearest_order:\n","                if nid == medoid_id or dtw_matrix[medoid_id, nearest_order[1]] == 0.:\n","                    average_pattern += medoid_pattern \n","                    weighted_sums += np.ones_like(weighted_sums) \n","                else:\n","                    path = dtw.dtw(medoid_pattern, random_prototypes[nid], dtw.RETURN_PATH, slope_constraint=slope_constraint, window=window)\n","                    dtw_value = dtw_matrix[medoid_id, nid]\n","                    warped = random_prototypes[nid, path[1]]\n","                    weight = np.exp(np.log(0.5)*dtw_value/dtw_matrix[medoid_id, nearest_order[1]])\n","                    average_pattern[path[0]] += weight * warped\n","                    weighted_sums[path[0]] += weight \n","            \n","            ret[i,:] = average_pattern / weighted_sums[:,np.newaxis]\n","        else:\n","            print(\"There is only one pattern of class %d, skipping pattern average\"%l[i])\n","            ret[i,:] = x[i]\n","    return ret\n","\n","# Proposed\n","\n","def random_guided_warp(x, labels, slope_constraint=\"symmetric\", use_window=True, dtw_type=\"normal\"):\n","    import utils.dtw as dtw\n","    \n","    if use_window:\n","        window = np.ceil(x.shape[1] / 10.).astype(int)\n","    else:\n","        window = None\n","    orig_steps = np.arange(x.shape[1])\n","    l = np.argmax(labels, axis=1) if labels.ndim > 1 else labels\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(tqdm(x)):\n","        # guarentees that same one isnt selected\n","        choices = np.delete(np.arange(x.shape[0]), i)\n","        # remove ones of different classes\n","        choices = np.where(l[choices] == l[i])[0]\n","        if choices.size > 0:        \n","            # pick random intra-class pattern\n","            random_prototype = x[np.random.choice(choices)]\n","            \n","            if dtw_type == \"shape\":\n","                path = dtw.shape_dtw(random_prototype, pat, dtw.RETURN_PATH, slope_constraint=slope_constraint, window=window)\n","            else:\n","                path = dtw.dtw(random_prototype, pat, dtw.RETURN_PATH, slope_constraint=slope_constraint, window=window)\n","                            \n","            # Time warp\n","            warped = pat[path[1]]\n","            for dim in range(x.shape[2]):\n","                ret[i,:,dim] = np.interp(orig_steps, np.linspace(0, x.shape[1]-1., num=warped.shape[0]), warped[:,dim]).T\n","        else:\n","            print(\"There is only one pattern of class %d, skipping timewarping\"%l[i])\n","            ret[i,:] = pat\n","    return ret\n","\n","def discriminative_guided_warp(x, labels, batch_size=6, slope_constraint=\"symmetric\", use_window=True, dtw_type=\"normal\", use_variable_slice=True):\n","    import utils.dtw as dtw\n","    \n","    if use_window:\n","        window = np.ceil(x.shape[1] / 10.).astype(int)\n","    else:\n","        window = None\n","    orig_steps = np.arange(x.shape[1])\n","    l = np.argmax(labels, axis=1) if labels.ndim > 1 else labels\n","    \n","    positive_batch = np.ceil(batch_size / 2).astype(int)\n","    negative_batch = np.floor(batch_size / 2).astype(int)\n","        \n","    ret = np.zeros_like(x)\n","    warp_amount = np.zeros(x.shape[0])\n","    for i, pat in enumerate(tqdm(x)):\n","        # guarentees that same one isnt selected\n","        choices = np.delete(np.arange(x.shape[0]), i)\n","        \n","        # remove ones of different classes\n","        positive = np.where(l[choices] == l[i])[0]\n","        negative = np.where(l[choices] != l[i])[0]\n","        \n","        if positive.size > 0 and negative.size > 0:\n","            pos_k = min(positive.size, positive_batch)\n","            neg_k = min(negative.size, negative_batch)\n","            positive_prototypes = x[np.random.choice(positive, pos_k, replace=False)]\n","            negative_prototypes = x[np.random.choice(negative, neg_k, replace=False)]\n","                        \n","            # vector embedding and nearest prototype in one\n","            pos_aves = np.zeros((pos_k))\n","            neg_aves = np.zeros((pos_k))\n","            if dtw_type == \"shape\":\n","                for p, pos_prot in enumerate(positive_prototypes):\n","                    for ps, pos_samp in enumerate(positive_prototypes):\n","                        if p != ps:\n","                            pos_aves[p] += (1./(pos_k-1.))*dtw.shape_dtw(pos_prot, pos_samp, dtw.RETURN_VALUE, slope_constraint=slope_constraint, window=window)\n","                    for ns, neg_samp in enumerate(negative_prototypes):\n","                        neg_aves[p] += (1./neg_k)*dtw.shape_dtw(pos_prot, neg_samp, dtw.RETURN_VALUE, slope_constraint=slope_constraint, window=window)\n","                selected_id = np.argmax(neg_aves - pos_aves)\n","                path = dtw.shape_dtw(positive_prototypes[selected_id], pat, dtw.RETURN_PATH, slope_constraint=slope_constraint, window=window)\n","            else:\n","                for p, pos_prot in enumerate(positive_prototypes):\n","                    for ps, pos_samp in enumerate(positive_prototypes):\n","                        if p != ps:\n","                            pos_aves[p] += (1./(pos_k-1.))*dtw.dtw(pos_prot, pos_samp, dtw.RETURN_VALUE, slope_constraint=slope_constraint, window=window)\n","                    for ns, neg_samp in enumerate(negative_prototypes):\n","                        neg_aves[p] += (1./neg_k)*dtw.dtw(pos_prot, neg_samp, dtw.RETURN_VALUE, slope_constraint=slope_constraint, window=window)\n","                selected_id = np.argmax(neg_aves - pos_aves)\n","                path = dtw.dtw(positive_prototypes[selected_id], pat, dtw.RETURN_PATH, slope_constraint=slope_constraint, window=window)\n","                   \n","            # Time warp\n","            warped = pat[path[1]]\n","            warp_path_interp = np.interp(orig_steps, np.linspace(0, x.shape[1]-1., num=warped.shape[0]), path[1])\n","            warp_amount[i] = np.sum(np.abs(orig_steps-warp_path_interp))\n","            for dim in range(x.shape[2]):\n","                ret[i,:,dim] = np.interp(orig_steps, np.linspace(0, x.shape[1]-1., num=warped.shape[0]), warped[:,dim]).T\n","        else:\n","            print(\"There is only one pattern of class %d\"%l[i])\n","            ret[i,:] = pat\n","            warp_amount[i] = 0.\n","    if use_variable_slice:\n","        max_warp = np.max(warp_amount)\n","        if max_warp == 0:\n","            # unchanged\n","            ret = window_slice(ret, reduce_ratio=0.95)\n","        else:\n","            for i, pat in enumerate(ret):\n","                # Variable Sllicing\n","                ret[i] = window_slice(pat[np.newaxis,:,:], reduce_ratio=0.95+0.05*warp_amount[i]/max_warp)[0]\n","    return ret"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9yEnd2lT4RQb","trusted":true,"colab_type":"code","colab":{}},"source":["def feature_roll_diff(x):\n","    for i in range(3):\n","      pos_x = np.roll(x[:,:,0][:,:,None], i)\n","      neg_x = np.roll(x[:,:,0][:,:,None], -i)\n","      # print(\"pos_x.shape\", pos_x.shape)\n","      x =  np.concatenate((x, pos_x),axis = 2)\n","      x =  np.concatenate((x, neg_x),axis = 2)\n","      # print(\"x.shape 1 \", x.shape)\n","      x =  np.concatenate((x, pos_x - x[:,:,0][:,:,None]),axis = 2) \n","      x =  np.concatenate((x, neg_x - x[:,:,0][:,:,None]),axis = 2)\n","      # print(\"x.shape 2 \", x.shape)\n","    x =  np.concatenate((x, x[:,:,0][:,:,None]**2),axis = 2)\n","      # print(\"x.shape 3 \", x.shape) \n","    return x       "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AdlC7eijQ1TI","trusted":true,"colab_type":"code","colab":{}},"source":["# @tf.function(input_signature=[tf.TensorSpec(None, tf.float32)]) \n","# def tf_feature_roll_diff(input): \n","#   y = tf.numpy_function(feature_roll_diff, [input], tf.float32) \n","#   return y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.427483Z","start_time":"2020-04-11T02:14:24.422535Z"},"trusted":true,"id":"Ft7_2jVZcOgQ","colab_type":"code","colab":{}},"source":["\n","def augment(X: np.array, y:np.array) -> Tuple[np.array, np.array]:\n","    X_aug = np.flip(X, axis=1)\n","    # print(\"X_aug shape:\", X_aug.shape)\n","    X = np.vstack((X, X_aug))\n","\n","    y = np.vstack((y, np.flip(y, axis=1)))\n","    \n","    return X, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVJepJOdUGqK","colab_type":"code","colab":{}},"source":["def augment_label(X: np.array, y:np.array, Xtest : np.array, Xval : np.array) -> Tuple[np.array, np.array, np.array, np.array]:\n","    X_aug = np.flip(X, axis=1)\n","    print(\"X_aug shape 1:\", X_aug.shape)\n","    X = np.concatenate((X, np.zeros_like(X[:,:,0][:,:,np.newaxis],dtype=np.uint8)),axis = 2)\n","    Xtest = np.concatenate((Xtest, np.zeros_like(Xtest[:,:,0][:,:,np.newaxis],dtype=np.uint8)),axis = 2)\n","    Xval = np.concatenate((Xval, np.zeros_like(Xval[:,:,0][:,:,np.newaxis],dtype=np.uint8)),axis = 2)   \n","    \n","    X_aug = np.concatenate((X_aug, np.ones_like(X_aug[:,:,0][:,:,np.newaxis],dtype=np.uint8)),axis = 2)\n","\n","    print(\"X_aug shape 2:\", X_aug.shape)\n","    X = np.vstack((X, X_aug))\n","    y = np.vstack((y, np.flip(y, axis=1)))\n","    \n","    return X, y, Xtest, Xval"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.432964Z","start_time":"2020-04-11T02:14:24.430264Z"},"trusted":true,"id":"Y-nODxu7cOgS","colab_type":"code","colab":{}},"source":["# Add ops to save and restore all the variables.\n","# saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.438940Z","start_time":"2020-04-11T02:14:24.436211Z"},"trusted":true,"id":"VRzoz-eacOgU","colab_type":"code","colab":{}},"source":["# # %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:24:41.652529Z\",\"start_time\":\"2020-04-03T23:24:41.645025Z\"}}\n","# class EarlyStopping:\n","#     def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n","#         self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n","#         self.counter, self.best_score = 0, None\n","#         self.is_maximize = is_maximize\n","\n","#     def load_best_weights(self, sess):\n","#         saver.restore(sess, self.checkpoint_path)\n","\n","#     def __call__(self, score, sess):\n","#         if self.best_score is None or \\\n","#         (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n","#             saver.save(sess, self.checkpoint_path)\n","#             self.best_score, self.counter = score, 0\n","#         else:\n","#             self.counter += 1\n","#             if self.counter >= self.patience:\n","#                 return True\n","#         return False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yzq5iOKjuWUY","trusted":true,"colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.pipeline import make_pipeline\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","\n","class ShiftedFeatureMaker(BaseEstimator, TransformerMixin):\n","    \n","    def __init__(self, periods=[1], column=\"signal\", add_minus=False, fill_value=None, copy=True):\n","        self.periods = periods\n","        self.column = column\n","        self.add_minus = add_minus\n","        self.fill_value = fill_value\n","        self.copy = copy\n","        \n","    def fit(self, X, y):\n","        \"\"\"Mock method\"\"\"\n","        return self\n","    \n","    def transform(self, X: pd.DataFrame, y=None):\n","        periods = np.asarray(self.periods, dtype=np.int32)\n","        \n","        if self.add_minus:\n","            periods = np.append(periods, -periods)\n","        \n","        X_transformed = X.copy() if self.copy else X\n","        \n","        for p in periods:\n","            X_transformed[f\"{self.column}_shifted_{p}\"] = X_transformed[self.column].shift(\n","                periods=p, fill_value=self.fill_value\n","            )\n","            \n","        return X_transformed\n","\n","\n","class ColumnDropper(BaseEstimator, TransformerMixin):\n","    \n","    def __init__(self, columns=None):\n","        self.columns = columns\n","    \n","    def fit(self, X, y):\n","        \"\"\"Mock method\"\"\"\n","        return self\n","    \n","    def transform(self, X: pd.DataFrame, y=None):\n","        return X[[c for c in X.columns if c not in self.columns]]\n","\n","\n","def add_category(train, test):\n","    train[\"category\"] = 0\n","    test[\"category\"] = 0\n","    \n","    # train segments with more then 9 open channels classes\n","    train.loc[2_000_000:2_500_000-1, 'category'] = 1\n","    train.loc[4_500_000:5_000_000-1, 'category'] = 1\n","    \n","    # # delete very noized part \n","    # train.loc[3_650_000:3_820_000, \"category\"] = -1\n","    # train = train[train.category != -1].reset_index(drop=True)\n","    \n","    # test segments with more then 9 open channels classes (potentially)\n","    test.loc[500_000:600_000-1, \"category\"] = 1\n","    test.loc[700_000:800_000-1, \"category\"] = 1\n","    \n","    return train, test\n","\n","def add_5_category(train, test):\n","\n","    train[\"category\"] = 0\n","    test[\"category\"] = 0\n","\n","    #slow open channel\n","    batch = 1; a = 500000*(batch-1); b = 500000*batch\n","    batch = 2; c = 500000*(batch-1); d = 500000*batch\n","    train.loc[a:b, \"category\"] = 1\n","    train.loc[c:d, \"category\"] = 1 \n","\n","    # fast open channel\n","    batch = 3; a = 500000*(batch-1); b = 500000*batch\n","    batch = 7; c = 500000*(batch-1); d = 500000*batch\n","    train.loc[a:b, \"category\"] = 2\n","    train.loc[c:d, \"category\"] = 2\n","\n","    # 3 channel\n","    batch = 4; a = 500000*(batch-1); b = 500000*batch\n","    batch = 8; c = 500000*(batch-1); d = 500000*batch\n","    train.loc[a:b, \"category\"] = 3\n","    train.loc[c:d, \"category\"] = 3\n","\n","    # 5 open channel\n","    batch = 6; a = 500000*(batch-1); b = 500000*batch\n","    batch = 9; c = 500000*(batch-1); d = 500000*batch\n","    train.loc[a:b, \"category\"] = 4\n","    train.loc[c:d, \"category\"] = 4\n","\n","    # 10 open channel\n","    batch = 5; a = 500000*(batch-1); b = 500000*batch\n","    batch = 10; c = 500000*(batch-1); d = 500000*batch\n","    train.loc[a:b, \"category\"] = 5\n","    train.loc[c:d, \"category\"] = 5\n","\n","    a = 0 # SUBSAMPLE A, Model 1s\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 1 \n","\n","    a = 1 # SUBSAMPLE B, Model 3\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 3\n","\n","    a = 2 # SUBSAMPLE C, Model 5\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 4\n","\n","    a = 3 # SUBSAMPLE D, Model 1s\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 1\n","\n","    a = 4 # SUBSAMPLE E, Model 1f\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 2\n","\n","    a = 5 # SUBSAMPLE F, Model 10\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 5\n","\n","    a = 6 # SUBSAMPLE G, Model 5\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 4\n","\n","    a = 7 # SUBSAMPLE H, Model 10\n","    test[\"category\"].iloc[100000*a:100000*(a+1)]= 5\n","\n","    a = 8 # SUBSAMPLE I, Model 1s\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 1\n","\n","    a = 9 # SUBSAMPLE J, Model 3\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 3\n","\n","    # BATCHES 3 AND 4, Model 1s\n","    test[\"category\"].iloc[1000000:2000000] = 1\n","\n","    return train, test   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6l0nc-llBdy","trusted":true,"colab_type":"code","colab":{}},"source":["import pickle\n","\n","def prob_with_RF(train_rfc, val_rfc, test_rfc, pkl_model_filename, force_build=False):\n","    shifted_rfc = make_pipeline(\n","        ShiftedFeatureMaker(\n","            periods=range(1, 20),\n","            add_minus=True,\n","            fill_value=0\n","        ),\n","        ColumnDropper(\n","            columns=[\"open_channels\", \"time\", \"group\"]\n","        ),\n","        # RandomForestClassifier(\n","        #     n_estimators=120,\n","        #     max_depth=19,\n","        #     max_features=10,\n","        #     random_state=42,\n","        #     n_jobs=20,\n","        #     verbose=2\n","        # )\n","        RandomForestClassifier(\n","            n_estimators=TREES,\n","            max_depth=DEPTH,\n","            max_features=10,\n","            random_state=42,\n","            n_jobs=-1,\n","            verbose=2\n","        )\n","    )\n","    if force_build or (not os.path.exists(pkl_model_filename)):\n","        print(\"Train RF model:\", pkl_model_filename)\n","        shifted_rfc.fit(train_rfc, train_rfc.open_channels)\n","        # Save to file in the current working directory\n","        # pkl_filename = \"pickle_model.pkl\"\n","        with open(pkl_model_filename, 'wb') as file:\n","          pickle.dump(shifted_rfc, file)\n","          print(\"Save RF model: \", pkl_model_filename)\n","\n","    # Load from file\n","    with open(pkl_model_filename, 'rb') as file:\n","        pickle_model = pickle.load(file)\n","        print(\"Loading RF model: \", pkl_model_filename)\n","    train_predictions = pickle_model.predict_proba(train_rfc)\n","    # print(\"train predic\")\n","    # print(train_predictions[:10])\n","    test_predictions = pickle_model.predict_proba(test_rfc)\n","    val_predictions = pickle_model.predict_proba(val_rfc)\n","    for i in range(11):\n","        train_rfc['Prob_{}'.format(i)] = train_predictions[:,i]\n","        test_rfc['Prob_{}'.format(i)] = test_predictions[:,i]\n","        val_rfc['Prob_{}'.format(i)] = val_predictions[:,i]\n","    \n","    train_rfc = reduce_mem_usage(train_rfc)\n","    test_rfc = reduce_mem_usage(test_rfc)\n","    val_rfc = reduce_mem_usage(val_rfc)\n","    return train_rfc, val_rfc, test_rfc\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xuj2XnjrBJmg","trusted":true,"colab_type":"code","colab":{}},"source":["# a = np.random.randn(3,4,2)\n","# print(a)\n","# print(a.shape)\n","# b = np.random.randn(3,4)\n","# print(b)\n","# print(b.shape)\n","# print(b[:,:,None].shape)\n","# c = np.concatenate((a,b[:,:,None]), axis=2)\n","# print(c)\n","# print(c.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MY_Z_SDD1kkJ","trusted":true,"colab_type":"code","colab":{}},"source":["# class CustomGen(TimeseriesGenerator):\n","#     def __getitem__(self, idx):\n","#         x, y = super().__getitem__(idx)\n","#         print(\"x.shape 1:\", x.shape)\n","#         x = np.squeeze(x)\n","#         # y = np.squeeze(y)\n","#         # print(y[0])\n","#         # timelen = x.shape[1]\n","#         # jitterscale = 0.1\n","#         # mulscale = 0.1\n","#         ## do processing here\n","#         # if (np.random.rand()<0):\n","#         #     if(np.random.rand()<0.5):\n","#         #         # augid = np.random.randint(low = 0, high = timelen, size = np.ceil(timelen*jitterscale).astype(int))\n","#         #         # augid = np.unique(augid)\n","#         #         x[:,:,0] = np.squeeze(jitter(x[:,:,0][:,:,None], sigma = 0.01))\n","#         #     else:\n","#         #         # augid = np.random.randint(low = 0, high = timelen, size = np.ceil(timelen*mulscale).astype(int))\n","#         #         # augid = np.unique(augid)\n","#         #         x[:,:,0] = np.squeeze(scaling(x[:,:,0][:,:,None], sigma = 0.02))\n","#         #     for i in range(3):\n","#         #         pos_x = np.roll(x[:,:,0], i)\n","#         #         neg_x = np.roll(x[:,:,0], -i)\n","#         #         x[:,:,12+i*4] = pos_x\n","#         #         x[:,:,12+i*4+1] = neg_x\n","#         #         x[:,:,12+i*4+3] = pos_x - x[:,:,0]\n","#         #         x[:,:,12+i*4+4] = neg_x - x[:,:,0]\n","#         #     x[:,:,24] = x[:,:,0]**2\n","#         # print(\"x.shape 2:\", x.shape)\n","#         return x, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RXy4eGuFM4Gq","trusted":true,"colab_type":"code","colab":{}},"source":["from tensorflow.python.keras.utils.data_utils import Sequence\n","\n","class CustomSequence(Sequence):\n","    def __init__(self, x_set, y_set, batch_size, augmentations=None):\n","        self.x, self.y = x_set, y_set\n","        self.batch_size = batch_size\n","        self.augment = augmentations\n","        self.indices = np.arange(self.x.shape[0])\n","        self.aug_rate = 1.0\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.x) / float(self.batch_size)))\n","\n","    def my_aug(self, x, y):\n","        if(np.random.rand()<0.3):\n","            shift = (np.random.randint(2000, size=1))[0]\n","            x = tf.roll(x, shift=shift, axis=1)\n","            y = tf.roll(y, shift=shift, axis=1)\n","        # if(np.random.rand()<0.5):\n","        #     x[:,:,0] = np.squeeze(jitter(x[:,:,0][:,:,None], sigma = 0.01))\n","        # else:\n","        #     x[:,:,0] = np.squeeze(scaling(x[:,:,0][:,:,None], sigma = 0.02))\n","        # for i in range(3):\n","        #     pos_x = np.roll(x[:,:,0], i)\n","        #     neg_x = np.roll(x[:,:,0], -i)\n","        #     x[:,:,12+i*4] = pos_x\n","        #     x[:,:,12+i*4+1] = neg_x\n","        #     x[:,:,12+i*4+3] = pos_x - x[:,:,0]\n","        #     x[:,:,12+i*4+4] = neg_x - x[:,:,0]\n","        # x[:,:,24] = x[:,:,0]**2\n","        return x, y\n","\n","    def __getitem__(self, idx):\n","        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        batch_x = self.x[inds]\n","        batch_y = self.y[inds]\n","        if (np.random.rand()<0.1):\n","            batch_x, batch_y = self.my_aug(batch_x, batch_y)\n","\n","        return np.array(batch_x), np.array(batch_y)\n","        # return np.stack([\n","        #     self.augment(image=x)[\"image\"] for x in batch_x\n","        # ], axis=0), np.array(batch_y)\n","    def on_epoch_end(self):\n","          np.random.shuffle(self.indices)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cZcailO_uCR","trusted":true,"colab_type":"code","colab":{}},"source":["# a = np.random.randn(8,3)\n","# print(a)\n","# b = np.random.randn(8,5)\n","# print(b)\n","# data_gen = CustomSequence(a,b, batch_size=2)\n","# print(\"get custom data\")\n","# tx, ty = data_gen[0]\n","# print(tx.shape)\n","# print(ty.shape)\n","# print(tx)\n","# print(ty)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"amY5F_7BqFHc","trusted":true,"colab_type":"code","colab":{}},"source":["def generate_sample_weights(training_data): \n","    sample_weights = np.where(training_data[:,:,8] == 0, 1, 3.0)\n","    print(\"unique sample:\", np.unique(sample_weights))\n","    print(\"3 count:\", np.count_nonzero(np.asarray(sample_weights)-1))\n","    print(\"1 count:\", np.count_nonzero(np.asarray(sample_weights)-3))\n","    return np.asarray(sample_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qYlXoLL86sYW","colab_type":"code","colab":{}},"source":["def combine_data(train : pd.DataFrame,\n","                 train_tfprob : pd.DataFrame) -> NoReturn:\n","    for col in train_tfprob.columns:\n","        if 'Prob' not in col:\n","            continue\n","        train[col] = 0\n","        train[col] = train_tfprob[col]\n","    return train"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3U92DpUQYxgE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.477074Z","start_time":"2020-04-11T02:14:24.441269Z"},"code_folding":[],"id":"e4QulGxHfS0n","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def run_cv_model_by_batch(train : pd.DataFrame,\n","                          test : pd.DataFrame,\n","                          splits : int,\n","                          batch_col : Text,\n","                          feats : List,\n","                          sample_submission: pd.DataFrame,\n","                          nn_epochs : int,\n","                          nn_batch_size : int) -> NoReturn:\n","    seed_everything(SEED)\n","    K.clear_session()\n","    if not os.path.exists(outdir):\n","      os.mkdir(outdir)\n","    print(outdir)\n","    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n","    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n","    tf.compat.v1.keras.backend.set_session(sess)\n","\n","    # train_rfc = pd.DataFrame(train, columns=['time', 'signal', 'open_channels', 'group'])\n","\n","    # test_rfc = pd.DataFrame(test, columns=['time', 'signal', 'group'])\n","    # train_rfc, test_rfc = add_category(train_rfc, test_rfc)\n","    # print(train_rfc.shape[0],train_rfc.shape[1])\n","    # print(train_rfc.head())\n","    print(\"train.shape:\", train.shape)\n","    print(\"test.shape:\", test.shape)\n","    oof_ = np.zeros((len(train), 11))\n","    preds_ = np.zeros((len(test), 11))\n","    target = ['open_channels']\n","    group = train['group']\n","    kf = GroupKFold(n_splits=5)\n","    splits = [x for x in kf.split(train, train[target], group)]\n","\n","    # pkl_path = os.path.join(PATH,'res/RFC/pickle_model_rfc_20_10.pkl')\n","    # pkl_gen_path = os.path.join(PATH,'res/RFC/pickle_model_rfc_20_10_run3.pkl')\n","\n","    new_splits = []\n","    for sp in splits:\n","        new_split = []\n","        new_split.append(np.unique(group[sp[0]]))\n","        new_split.append(np.unique(group[sp[1]]))\n","        new_split.append(sp[0])  \n","        new_split.append(sp[1])    \n","        new_splits.append(new_split)\n","\n","    \n","\n","    # Calculate the weights for each class so that we can balance the data\n","    weights_ = class_weight.compute_class_weight('balanced',\n","                                                np.unique(train.open_channels),\n","                                                train.open_channels)\n","    print(\"weights_:\", weights_)\n","\n","    nums_ = train.open_channels.value_counts(sort=False)\n","    print(\"nums_:\", nums_)\n","    beta = BETA\n","    effective_num = 1.0 - np.power(beta, nums_)\n","    weights = (1.0 - beta) / np.array(effective_num)\n","    cb_weights_ = weights / np.sum(weights) * 11\n","    print(\"cb_weights_:\", cb_weights_)   \n","        \n","    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n","\n","    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n","    # print(tr.head())\n","    target_cols = ['target_'+str(i) for i in range(11)]\n","    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n","    print(np.shape(train_tr))\n","    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n","    print(np.shape(train))\n","    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n","    del tr\n","    # print(\"test shape:\", np.shape(test))\n","    for n_fold, (tr_idx, val_idx, tr_orig_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n","        sub_dir = os.path.join(outdir,\"{}_fold\".format(n_fold))\n","        # if n_fold<4:\n","        #     continue\n","        # if n_fold== 4:\n","        #     break\n","\n","        if not os.path.exists(sub_dir):\n","            os.mkdir(sub_dir)\n","        # pkl_model_filename = os.path.join(sub_dir,'rf-{}-tn{}-dn{}.pkl'.format(n_fold, TREES, DEPTH))\n","        # pkl_model_filename = os.path.join(PATH,'res/RFC/RF100-D12/rf-f{}-tn{}-dn{}.pkl'.format(n_fold, TREES, DEPTH))\n","        # pkl_model_filename = os.path.join(PATH,'myrfres/rf-f{}-tn{}-dn{}.pkl'.format(n_fold, TREES, DEPTH))\n","        # print(\"train index\")\n","        # print(\"tr_orig_idx max and min:\", max(tr_orig_idx), min(tr_orig_idx))\n","        # print(\"tr_orig_idx shape:\", np.shape(tr_orig_idx))\n","        # train_rfc_ = train_rfc.iloc[tr_orig_idx].copy()\n","        # valid_rfc_ = train_rfc.iloc[val_orig_idx].copy()\n","        # test_rfc_ = test_rfc.copy()\n","        # train_rfc_, valid_rfc_, test_rfc_ = prob_with_RF(train_rfc_, valid_rfc_, test_rfc_, pkl_model_filename)\n","        # # print(train_rfc_.head())\n","        # # print(valid_rfc_.head())\n","        # # print(test_rfc.head())\n","        # print(\"train_rfc_.shape: \",train_rfc_.shape)\n","        # print(\"tr_idx shape: \", np.shape(tr_idx))\n","        # print(\"train shape:\", np.shape(train))\n","\n","        # if n_fold < 2:\n","        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n","        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n","        test_ = test.copy()\n","\n","        # print(\"train_x shape 1: \", np.shape(train_x))\n","        # for i in range(11):\n","        #   trainprob = np.array(list(train_rfc_.groupby('group').apply(lambda x: x['Prob_{}'.format(i)].values)))\n","        #   train_x = np.concatenate((train_x, trainprob[:,:,None]),axis = 2)\n","        #   vprob = np.array(list(valid_rfc_.groupby('group').apply(lambda x: x['Prob_{}'.format(i)].values)))\n","        #   valid_x = np.concatenate((valid_x, vprob[:,:,None]),axis = 2)\n","        #   testprob = np.array(list(test_rfc_.groupby('group').apply(lambda x: x['Prob_{}'.format(i)].values)))\n","        #   test_ = np.concatenate((test_, testprob[:,:,None]),axis = 2)\n","        # print(\"train_x shape 2: \", np.shape(train_x))\n","\n","\n","        # train_x, train_y = augment(train_x, train_y)\n","        train_x, train_y, test_, valid_x = augment_label(train_x, train_y, test_, valid_x)\n","\n","        # print(\"unique train_x[8]:\", np.unique(train_x[:,:,8]))\n","        # train_x = feature_roll_diff(train_x)\n","        # test_ = feature_roll_diff(test_)\n","        # valid_x = feature_roll_diff(valid_x)\n","        print(\"test shape after roll diff:\", np.shape(test_))\n","\n","        print(\"train_x shape 3: \", np.shape(train_x))\n","        print(\"train_y shape 3: \", np.shape(train_y))\n","        gc.collect()\n","        shape_ = (None, train_x.shape[2])\n","        model = ClassifierCBRW(shape_)\n","        print(\"model initilization done!\")\n","        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n","        cb_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n","                              patience=6, min_lr=1e-6)\n","        cb_clr = CyclicLR(base_lr=1e-7, max_lr = LR, step_size= int(1.0*(train.shape[0])/(nn_batch_size*4)) , \n","                          mode='exp_range', gamma=Gamma, scale_fn=None, scale_mode='cycle')\n","        cb_prg = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,leave_overall_progress=False, \n","                                               show_epoch_progress=False,show_overall_progress=True)\n","        # cb_csv_logger= CSVLogger(os.path.join(sub_dir,'res.csv'))\n","        # cb_history = LossHistory()  # 这里是使用自定义的Callback回调函数，当然本身fit函数也会返回一个history可供使用\n","        \n","        save_checkpoint_path = os.path.join(sub_dir,'checkpoint-modelonly-{}.h5'.format(n_fold))\n","        save_finalmodel_path = os.path.join(sub_dir,'fmodel-modelonly-{}.h5'.format(n_fold))\n","        save_bestf1macro_path = os.path.join(sub_dir,'checkpoint-{}.h5'.format(n_fold)) \n","        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_checkpoint_path,\n","                                              monitor='val_accuracy',\n","                                              mode = 'max',            \n","                                              save_weights_only=True,\n","                                              save_best_only=True,           \n","                                              verbose=1)\n","        # print(train_y[0])\n","#         data_gen = CustomSequence(train_x,train_y, batch_size = nn_batch_size)\n","#         # history = model.fit_generator(generator = data_gen,\n","#         #     epochs=nn_epochs,\n","#         #     callbacks=[cb_prg, cb_clr, cp_callback,\n","#         #                 MacroF1ES(model, valid_x, valid_y, patience=70, delta=0, \n","#         #                           checkpoint_path=save_bestf1macro_path)],\n","#         #     verbose=1,\n","#         #     validation_data=(valid_x,valid_y))\n","        # model.load_weights(os.path.join(MODEL_PATH, \"{}_fold\".format(n_fold),'checkpoint-{}.h5'.format(n_fold)))\n","# #         # history = model.fit(x = data_gen,\n","# #         #     epochs=nn_epochs,\n","# #         #     callbacks=[cb_prg, cb_lr_schedule, cp_callback,\n","# #         #                 MacroF1ES(model, valid_x, valid_y, patience=70, delta=0, \n","# #         #                           checkpoint_path=save_bestf1macro_path)],\n","# #         #     batch_size=nn_batch_size,\n","# #         #     verbose=1,\n","# #         #     validation_data=(valid_x,valid_y))\n","#         history = model.fit(train_x, train_y,\n","#             epochs=nn_epochs,\n","#             callbacks=[cb_prg, cb_lr_schedule, cp_callback,\n","#                         MacroF1ES(model, valid_x, valid_y, patience=70, delta=-0.000005, \n","#                                   checkpoint_path=save_bestf1macro_path)],\n","#             batch_size=nn_batch_size,\n","# #             sample_weights = generate_sample_weights(train_x),\n","#             verbose=1,\n","#             validation_data=(valid_x,valid_y))\n","#         pd.DataFrame(history.history).to_csv(os.path.join(sub_dir,'{}-len{}-lr{}-{}-log-{}.csv'.format(MODELNAME,BATCHSIZE,LR,timestampStr, n_fold)), float_format='%.4f')\n","#         # print('\\nhistory dict:', history.history)\n","#         del history\n","#         model.save_weights(save_finalmodel_path)\n","        model.load_weights(save_bestf1macro_path)\n","        preds_f = model.predict(valid_x)\n","        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  \n","                             np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n","        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n","        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n","        oof_[val_orig_idx,:] += preds_f\n","        te_preds = model.predict(test_)\n","        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n","        preds_ += te_preds / SPLITS\n","        del train_x, train_y, test_, valid_x, valid_y, preds_f, te_preds\n","        del model\n","        \n","\n","    f1_score_ =f1_score(np.argmax(train_tr, axis=2).reshape(-1),  np.argmax(oof_, axis=1), average = 'macro')\n","    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n","    sample_submission['open_channels'] = np.argmax(preds_, axis=1).astype(int)\n","    sample_submission.to_csv(os.path.join(outdir,'{}_bf1_pred.csv'.format(VERSION)), index=False, float_format='%.4f')\n","    display(sample_submission.head())\n","    # np.save(os.path.join(outdir,'oof.npy'), oof_)\n","    # np.save(os.path.join(outdir,'preds.npy'), preds_)\n","\n","    return \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.485692Z","start_time":"2020-04-11T02:14:24.479620Z"},"id":"dqt0VJAS2ucB","trusted":true,"colab_type":"code","colab":{}},"source":["# def lr_schedule(epoch):\n","#     if epoch < 40:\n","#         lr = LR\n","#     elif epoch < 50:\n","#         lr = LR / 3\n","#     elif epoch < 60:\n","#         lr = LR / 6\n","#     elif epoch < 75:\n","#         lr = LR / 9\n","#     elif epoch < 85:\n","#         lr = LR / 12\n","#     elif epoch < 100:\n","#         lr = LR / 15\n","#     else:\n","#         lr = LR / 50\n","#     return lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aaClcd8zJIyC","trusted":true,"colab_type":"code","colab":{}},"source":["# function that decrease the learning as epochs increase (i also change this part of the code)\n","def lr_schedule(epoch):\n","    if epoch < 30:\n","        lr = LR\n","    elif epoch < 40:\n","        lr = LR / 3\n","    elif epoch < 50:\n","        lr = LR / 5\n","    elif epoch < 60:\n","        lr = LR / 7\n","    elif epoch < 70:\n","        lr = LR / 9\n","    elif epoch < 80:\n","        lr = LR / 11\n","    elif epoch < 90:\n","        lr = LR / 13\n","    elif epoch < 150:\n","        lr = LR / 100\n","    else:\n","        lr = LR / 1000\n","    return lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.499088Z","start_time":"2020-04-11T02:14:24.488223Z"},"id":"SjbrEQSa2ucN","trusted":true,"colab_type":"code","colab":{}},"source":["class Mish(tf.keras.layers.Layer):\n","\n","    def __init__(self, **kwargs):\n","        super(Mish, self).__init__(**kwargs)\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        return inputs * K.tanh(K.softplus(inputs))\n","\n","    def get_config(self):\n","        base_config = super(Mish, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","def mish(x):\n","\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n"," \n","from tensorflow.keras.utils import get_custom_objects\n","# from tensorflow.keras.layers import Activation\n","get_custom_objects().update({'mish': Activation(mish)})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.521416Z","start_time":"2020-04-11T02:14:24.502541Z"},"id":"dM8RiNfL2ucY","trusted":true,"colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras import initializers\n","from tensorflow.keras import regularizers\n","from tensorflow.keras import constraints\n","\n","class Attention(Layer):\n","    \"\"\"Multi-headed attention layer.\"\"\"\n","    \n","    def __init__(self, hidden_size, \n","                 num_heads = 8, \n","                 attention_dropout=.1,\n","                 trainable=True,\n","                 name='Attention'):\n","        \n","        if hidden_size % num_heads != 0:\n","            raise ValueError(\"Hidden size must be evenly divisible by the number of heads.\")\n","            \n","        self.hidden_size = hidden_size\n","        self.num_heads = num_heads\n","        self.trainable = trainable\n","        self.attention_dropout = attention_dropout\n","        self.dense = tf.keras.layers.Dense(self.hidden_size, use_bias=False)\n","        self.time_dense = TimeDistributed(Dense(self.hidden_size, use_bias=False))\n","        super(Attention, self).__init__(name=name)\n","\n","    def split_heads(self, x):\n","        \"\"\"Split x into different heads, and transpose the resulting value.\n","        The tensor is transposed to insure the inner dimensions hold the correct\n","        values during the matrix multiplication.\n","        Args:\n","          x: A tensor with shape [batch_size, length, hidden_size]\n","        Returns:\n","          A tensor with shape [batch_size, num_heads, length, hidden_size/num_heads]\n","        \"\"\"\n","        with tf.name_scope(\"split_heads\"):\n","            batch_size = tf.shape(x)[0]\n","            length = tf.shape(x)[1]\n","\n","            # Calculate depth of last dimension after it has been split.\n","            depth = (self.hidden_size // self.num_heads)\n","\n","            # Split the last dimension\n","            x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n","\n","            # Transpose the result\n","            return tf.transpose(x, [0, 2, 1, 3])\n","    \n","    def combine_heads(self, x):\n","        \"\"\"Combine tensor that has been split.\n","        Args:\n","          x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n","        Returns:\n","          A tensor with shape [batch_size, length, hidden_size]\n","        \"\"\"\n","        with tf.name_scope(\"combine_heads\"):\n","            batch_size = tf.shape(x)[0]\n","            length = tf.shape(x)[2]\n","            x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n","            return tf.reshape(x, [batch_size, length, self.hidden_size])        \n","\n","    def call(self, inputs):\n","        \"\"\"Apply attention mechanism to inputs.\n","        Args:\n","          inputs: a tensor with shape [batch_size, length_x, hidden_size]\n","        Returns:\n","          Attention layer output with shape [batch_size, length_x, hidden_size]\n","        \"\"\"\n","        # Google developper use tf.layer.Dense to linearly project the queries, keys, and values.\n","        q = self.dense(inputs)\n","        k = self.dense(inputs)\n","        v = self.dense(inputs)\n","\n","        q = self.split_heads(q)\n","        k = self.split_heads(k)\n","        v = self.split_heads(v)\n","        \n","        # Scale q to prevent the dot product between q and k from growing too large.\n","        depth = (self.hidden_size // self.num_heads)\n","        q *= depth ** -0.5\n","        \n","        logits = tf.matmul(q, k, transpose_b=True)\n","        # logits += self.bias\n","        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n","        \n","        if self.trainable:\n","            weights = tf.nn.dropout(weights, 1.0 - self.attention_dropout)\n","        \n","        attention_output = tf.matmul(weights, v)\n","        attention_output = self.combine_heads(attention_output)\n","        attention_output = self.time_dense(attention_output)\n","        attention_output = Dropout(self.attention_dropout)(attention_output)\n","        return attention_output\n","        \n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape(input_shape)\n","\n","    def get_config(self):\n","        base_config = super(Attention, self).get_config()\n","        config = {'hidden_size' : self.hidden_size,\n","                    'num_heads' : self.num_heads,\n","                    'trainable' : self.trainable,\n","                    'attention_dropout' : self.attention_dropout,\n","                    'name':'Attention'}\n","        # config = {'name':'Attention'}\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vrzikcyx8ptJ","trusted":true,"colab_type":"code","colab":{}},"source":["# focal loss with one-hot labels, multiclass\n","def cb_focal_loss(classes_weights, gamma=2., alpha=.25, w=3e-4):\n","    # classes_weights contains weights of each classes\n","    def cb_focal_loss_fixed(target_tensor, prediction_tensor):\n","        '''\n","        prediction_tensor is the output tensor with shape [None, 100], where 100 is the number of classes\n","        target_tensor is the label tensor, same shape as predcition_tensor\n","        '''\n","\n","        #1# get focal loss with no balanced weight which presented in paper function (4)\n","        zeros = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n","        one_minus_p = array_ops.where(tf.greater(target_tensor,zeros), target_tensor - prediction_tensor, zeros)\n","        FT = -1 * (one_minus_p ** gamma) * tf.math.log(tf.clip_by_value(prediction_tensor, 1e-8, 1.0))\n","\n","        #2# get balanced weight alpha\n","        classes_weight = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n","        classes_w_tensor = tf.convert_to_tensor(classes_weights, dtype=prediction_tensor.dtype)\n","        classes_weight += classes_w_tensor\n","\n","        alpha = array_ops.where(tf.greater(target_tensor, zeros), classes_weight, zeros)\n","\n","        #3# get balanced focal loss\n","        balanced_fl = alpha * FT\n","        balanced_fl = tf.reduce_mean(balanced_fl)\n","\n","        #4# add other op to prevent overfit\n","        # reference : https://spaces.ac.cn/archives/4493\n","        nb_classes = len(classes_weights)\n","        fianal_loss = (1-w) * balanced_fl + w * K.categorical_crossentropy(K.ones_like(prediction_tensor)/nb_classes, prediction_tensor)\n","\n","        return fianal_loss\n","    return cb_focal_loss_fixed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JM2iC9UwFd-c","trusted":true,"colab_type":"code","colab":{}},"source":["def multi_category_focal_loss2(gamma=2., alpha=.25):\n","    \"\"\"\n","    focal loss for multi category of multi label problem\n","    适用于多分类或多标签问题的focal loss\n","    alpha控制真值y_true为1/0时的权重\n","        1的权重为alpha, 0的权重为1-alpha\n","    当你的模型欠拟合，学习存在困难时，可以尝试适用本函数作为loss\n","    当模型过于激进(无论何时总是倾向于预测出1),尝试将alpha调小\n","    当模型过于惰性(无论何时总是倾向于预测出0,或是某一个固定的常数,说明没有学到有效特征)\n","        尝试将alpha调大,鼓励模型进行预测出1。\n","    Usage:\n","     model.compile(loss=[multi_category_focal_loss2(alpha=0.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n","    \"\"\"\n","    epsilon = 1.e-7\n","    gamma = float(gamma)\n","    alpha = tf.constant(alpha, dtype=tf.float32)\n","\n","    def multi_category_focal_loss2_fixed(y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n","    \n","        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n","        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n","        ce = -tf.math.log(y_t)\n","        weight = tf.pow(tf.subtract(1., y_t), gamma)\n","        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n","        loss = tf.reduce_mean(fl)\n","        return loss\n","    return multi_category_focal_loss2_fixed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6d4mH_0Omj2","trusted":true,"colab_type":"code","colab":{}},"source":["def get_f1(y_true, y_pred): #taken from old keras source code\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.549703Z","start_time":"2020-04-11T02:14:24.533479Z"},"id":"YSESlpaNwkkx","trusted":true,"colab_type":"code","colab":{}},"source":["def WaveNetResidualConv1D(num_filters, kernel_size, stacked_layer):\n","\n","    def build_residual_block(l_input):\n","        resid_input = l_input\n","        for dilation_rate in [2**i for i in range(stacked_layer)]:\n","            l_sigmoid_conv1d = Conv1D(\n","              num_filters, kernel_size, dilation_rate=dilation_rate,\n","              padding='same', activation='sigmoid')(l_input)\n","            l_tanh_conv1d = Conv1D(\n","             num_filters, kernel_size, dilation_rate=dilation_rate,\n","             padding='same', activation='mish')(l_input)\n","            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n","            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n","            resid_input = Add()([resid_input ,l_input])\n","        return resid_input\n","    return build_residual_block\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEn4iIRTaslB","trusted":true,"colab_type":"code","colab":{}},"source":["def Classifier(shape_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    l_output = Dense(11, activation='softmax')(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZymKtoVo8F06","trusted":true,"colab_type":"code","colab":{}},"source":["def ClassifierW(shape_, weights_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    l_output = Dense(11, activation='softmax')(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","\n","    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n","    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n","\n","#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n","#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0lPAcchdy7-","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLrqe5Fc7qpl","trusted":true,"colab_type":"code","colab":{}},"source":["def ClassifierCBRW(shape_):\n","    \n","    def cbr(x, out_layer, kernel, stride, dilation):\n","        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n","        x = BatchNormalization()(x)\n","        x = Activation(\"relu\")(x)\n","        return x\n","    \n","    def wave_block(x, filters, kernel_size, n):\n","        dilation_rates = [2**i for i in range(n)]\n","        x = Conv1D(filters = filters,\n","                   kernel_size = 1,\n","                   padding = 'same')(x)\n","        res_x = x\n","        for dilation_rate in dilation_rates:\n","            tanh_out = Conv1D(filters = filters,\n","                              kernel_size = kernel_size,\n","                              padding = 'same', \n","                              activation = 'tanh', \n","                              dilation_rate = dilation_rate)(x)\n","            sigm_out = Conv1D(filters = filters,\n","                              kernel_size = kernel_size,\n","                              padding = 'same',\n","                              activation = 'sigmoid', \n","                              dilation_rate = dilation_rate)(x)\n","            x = Multiply()([tanh_out, sigm_out])\n","            x = Conv1D(filters = filters,\n","                       kernel_size = 1,\n","                       padding = 'same')(x)\n","            res_x = Add()([res_x, x])\n","        return res_x\n","    \n","    inp = Input(shape = (shape_))\n","    x = cbr(inp, 64, 7, 1, 1)\n","    x = BatchNormalization()(x)\n","    x = wave_block(x, 16, 3, 12)\n","    x = BatchNormalization()(x)\n","    x = wave_block(x, 32, 3, 8)\n","    x = BatchNormalization()(x)\n","    x = wave_block(x, 64, 3, 4)\n","    x = BatchNormalization()(x)\n","    x = wave_block(x, 128, 3, 1)\n","    x = cbr(x, 32, 7, 1, 1)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.2)(x)\n","    out = Dense(11, activation = 'softmax', name = 'out')(x)\n","    \n","    model = models.Model(inputs = inp, outputs = out)\n","    \n","    opt = Adam(lr = LR)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(loss=losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gwDhfmmRSf_3","colab_type":"code","colab":{}},"source":["# a = np.random.rand(3,2,4)\n","# print(a)\n","# b = np.argmax(a, axis=-1)\n","# print(b)\n","\n","# c = K.one_hot(\n","#     b, 4\n","# )\n","# print(c)\n","# index = tf.where(c)\n","# print(index)\n","# d = K.reshape(K.argmax(a, axis=-1), shape =(-1))\n","# print(d)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9ezeQ8ueb03","colab_type":"code","colab":{}},"source":["# a =np.array([[1,0],[0,0],[1,1],[0,1]])\n","# idx = a.sum(axis=1)==1\n","# print(idx)\n","# if idx.any():\n","#     print(idx)\n","#     print(a[idx,:])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xv3qyj-yLGWf","colab_type":"code","colab":{}},"source":["# def f1(y_true, y_pred):\n","#     y_pred = K.round(y_pred)\n","#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n","#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n","#     fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n","#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n","\n","#     p = tp / (tp + fp + K.epsilon())\n","#     r = tp / (tp + fn + K.epsilon())\n","\n","#     f1 = 2*p*r / (p+r+K.epsilon())\n","#     f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n","#     return K.mean(f1)\n","\n","def f1macro(y_true, y_pred):\n","    b,l,c = np.shape(y_true)\n","    y_pred = K.argmax(y_pred, axis=-1)\n","    y_pred = K.one_hot(y_pred, 11)\n","    tp = K.sum(K.sum(K.cast(y_true*y_pred, 'float'), axis=1),axis=0)\n","    tn = K.sum(K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1),axis=0)\n","    fp = K.sum(K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1),axis=0)\n","    fn = K.sum(K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1),axis=0)\n","    p = tp / (tp + fp + K.epsilon())\n","    r = tp / (tp + fn + K.epsilon())\n","    f1 = 2*p*r / (p+r+K.epsilon())\n","    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n","    return K.mean(f1)\n","\n","def f1_loss(y_true, y_pred):\n","    \n","    tp = K.sum(K.sum(K.cast(y_true*y_pred, 'float'), axis=1),axis=0)\n","    tn = K.sum(K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1),axis=0)\n","    fp = K.sum(K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1),axis=0)\n","    fn = K.sum(K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1),axis=0)\n","    p = tp / (tp + fp + K.epsilon())\n","    r = tp / (tp + fn + K.epsilon())\n","\n","    f1 = 2*p*r / (p+r+K.epsilon())\n","    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n","    return 1 - K.mean(f1)\n","\n","def f1_lossv2(y_true, y_pred):\n","    loss = 0\n","    lack_cls = (K.sum(K.sum(y_true, axis=1),axis=0) ==0)\n","    index = K.cast(tf.where(lack_cls), dtype='int32')\n","    if  index.shape[0] is not None:\n","        loss = K.binary_crossentropy(tf.gather(y_true, index, axis=-1), \n","                                  tf.gather(y_pred, index, axis=-1))\n","        loss = K.mean(loss)\n","    # y_pred = K.sigmoid(y_pred)\n","    tp = K.sum(K.sum(K.cast(y_true*y_pred, 'float'), axis=1),axis=0)\n","    tn = K.sum(K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1),axis=0)\n","    fp = K.sum(K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1),axis=0)\n","    fn = K.sum(K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1),axis=0)\n","    p = tp / (tp + fp + K.epsilon())\n","    r = tp / (tp + fn + K.epsilon())\n","\n","    f1 = 2*p*r / (p+r+K.epsilon())\n","    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n","    return 1 - K.mean(f1) + loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jlwsTKyp4Uks","colab_type":"code","colab":{}},"source":["# y_true = tf.convert_to_tensor([[[1,0, 1,1],[1,0,0,0],[0,0,1,0],[0,0,1,0]], [[1,0, 1,1],[1,0,0,0],[0,0,1,0],[0,0,1,0]]], dtype='float32')\n","# print(y_true.shape)\n","# y_pred = tf.convert_to_tensor([[[0.8,0.1, 0,0.1],[0.7,0.3,0, 0],[0,0.8,0.1,0.1],[0,0.2,0.8,0.0]], [[0.8,0.1, 0,0.1],[0.7,0.3,0, 0],[0,0.8,0.1,0.1],[0,0.2,0.8,0.0]]], dtype='float32')\n","# print(y_pred.shape)\n","# loss = tf.keras.backend.categorical_crossentropy(y_true, y_pred)\n","# print(loss)\n","# l1 = f1_lossv2(y_true, y_pred)\n","# print(l1)\n","# # print(\"===================\")\n","# # a = tf.convert_to_tensor([[[0., 0.],[0., 0.],[0., 0.],[0., 0.]]], dtype='float32')\n","# # b = tf.convert_to_tensor([[[0.1, 0.1],[0.3, 0. ],[0.8, 0.1],[0.2, 0. ]]],dtype='float32')\n","# # lossab = tf.keras.backend.categorical_crossentropy(a, b)\n","# # print(lossab)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpv6b4yQ3ngG","colab_type":"code","colab":{}},"source":["# def f1_lossv2(y_true, y_pred):\n","#     loss = 0\n","#     lack_cls = y_true.sum(dim=0) == 0\n","#     if lack_cls.any():\n","#         loss += F.binary_cross_entropy_with_logits(\n","#             y_pred[:, lack_cls], y_true[:, lack_cls])\n","#     y_pred = torch.sigmoid(y_pred)\n","#     y_pred = torch.clamp(y_pred * (1-y_true), min=0.01) + y_pred * y_true\n","#     tp = y_pred * y_true\n","#     tp = tp.sum(dim=0)\n","#     precision = tp / (y_pred.sum(dim=0) + 1e-8)\n","#     recall = tp / (y_true.sum(dim=0) + 1e-8)\n","#     f1 = 2 * (precision * recall / (precision + recall + 1e-8))\n","#     return 1 - f1.mean() + loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKt1D_NZKYrJ","colab_type":"code","colab":{}},"source":["# def ClassifierCBRWF1(shape_):\n","    \n","#     def cbr(x, out_layer, kernel, stride, dilation):\n","#         x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n","#         x = BatchNormalization()(x)\n","#         x = Activation(\"relu\")(x)\n","#         return x\n","    \n","#     def wave_block(x, filters, kernel_size, n):\n","#         dilation_rates = [2**i for i in range(n)]\n","#         x = Conv1D(filters = filters,\n","#                    kernel_size = 1,\n","#                    padding = 'same')(x)\n","#         res_x = x\n","#         for dilation_rate in dilation_rates:\n","#             tanh_out = Conv1D(filters = filters,\n","#                               kernel_size = kernel_size,\n","#                               padding = 'same', \n","#                               activation = 'tanh', \n","#                               dilation_rate = dilation_rate)(x)\n","#             sigm_out = Conv1D(filters = filters,\n","#                               kernel_size = kernel_size,\n","#                               padding = 'same',\n","#                               activation = 'sigmoid', \n","#                               dilation_rate = dilation_rate)(x)\n","#             x = Multiply()([tanh_out, sigm_out])\n","#             x = Conv1D(filters = filters,\n","#                        kernel_size = 1,\n","#                        padding = 'same')(x)\n","#             res_x = Add()([res_x, x])\n","#         return res_x\n","    \n","#     inp = Input(shape = (shape_))\n","#     x = cbr(inp, 64, 7, 1, 1)\n","#     x = BatchNormalization()(x)\n","#     x = wave_block(x, 16, 3, 12)\n","#     x = BatchNormalization()(x)\n","#     x = Dropout(0.2)(x)\n","#     x = wave_block(x, 32, 3, 8)\n","#     x = BatchNormalization()(x)\n","#     x = Dropout(0.2)(x)\n","#     x = wave_block(x, 64, 3, 4)\n","#     x = BatchNormalization()(x)\n","#     x = Dropout(0.2)(x)\n","#     x = wave_block(x, 128, 3, 1)\n","#     x = Dropout(0.2)(x)\n","#     x = cbr(x, 32, 7, 1, 1)\n","#     x = BatchNormalization()(x)\n","#     x = Dropout(0.2)(x)\n","#     out = Dense(11, activation = 'softmax', name = 'out')(x)\n","    \n","#     model = models.Model(inputs = inp, outputs = out)\n","    \n","#     opt = tfa.optimizers.AdamW(lr = LR, weight_decay=WD)\n","#     opt = tfa.optimizers.SWA(opt)\n","#     model.compile(loss=f1_lossv2, optimizer = opt, metrics = ['accuracy', f1macro])\n","#     return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HrEY7J_kN7h","colab_type":"code","colab":{}},"source":["# def ClassifierCBRWF1v2(shape_):\n","    \n","#     def cbr(x, out_layer, kernel, stride, dilation):\n","#         x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n","#         x = BatchNormalization()(x)\n","#         x = Activation(\"relu\")(x)\n","#         return x\n","    \n","#     def wave_block(x, filters, kernel_size, n):\n","#         dilation_rates = [2**i for i in range(n)]\n","#         x = Conv1D(filters = filters,\n","#                    kernel_size = 1,\n","#                    padding = 'same')(x)\n","#         res_x = x\n","#         for dilation_rate in dilation_rates:\n","#             tanh_out = Conv1D(filters = filters,\n","#                               kernel_size = kernel_size,\n","#                               padding = 'same', \n","#                               activation = 'tanh', \n","#                               dilation_rate = dilation_rate)(x)\n","#             sigm_out = Conv1D(filters = filters,\n","#                               kernel_size = kernel_size,\n","#                               padding = 'same',\n","#                               activation = 'sigmoid', \n","#                               dilation_rate = dilation_rate)(x)\n","#             x = Multiply()([tanh_out, sigm_out])\n","#             x = Conv1D(filters = filters,\n","#                        kernel_size = 1,\n","#                        padding = 'same')(x)\n","#             res_x = Add()([res_x, x])\n","#         return res_x\n","    \n","#     inp = Input(shape = (shape_))\n","#     x = cbr(inp, 64, 7, 1, 1)\n","#     x = BatchNormalization()(x)\n","#     x = wave_block(x, 16, 3, 12)\n","#     x = BatchNormalization()(x)\n","#     # x = Dropout(0.1)(x)\n","#     x = wave_block(x, 32, 3, 8)\n","#     x = BatchNormalization()(x)\n","#     # x = Dropout(0.1)(x)\n","#     x = wave_block(x, 64, 3, 4)\n","#     x = BatchNormalization()(x)\n","#     # x = Dropout(0.2)(x)\n","#     x = wave_block(x, 128, 3, 1)\n","#     # x = Dropout(0.2)(x)\n","#     x = cbr(x, 32, 7, 1, 1)\n","#     x = BatchNormalization()(x)\n","#     x = Dropout(0.3)(x)\n","#     out = Dense(11, activation = 'softmax', name = 'out')(x)\n","    \n","#     model = models.Model(inputs = inp, outputs = out)\n","    \n","#     opt = tfa.optimizers.AdamW(lr = LR, weight_decay=WD)\n","#     opt = tfa.optimizers.SWA(opt)\n","#     model.compile(loss=losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n","#     return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.579491Z","start_time":"2020-04-11T02:14:24.567764Z"},"trusted":true,"id":"L4cQ0L8ucOgn","colab_type":"code","colab":{}},"source":["class MacroF1ES(Callback):\n","    def __init__(self, model, inputs, targets, \n","                 patience=5, delta=0, checkpoint_path='checkpoint.h5', is_maximize=True):\n","        \n","        self.model = model\n","        self.inputs = inputs\n","        self.targets = np.argmax(targets, axis=2).reshape(-1)\n","        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n","        self.counter, self.best_score = 0, None\n","        self.is_maximize = is_maximize\n","        self.stopped_epoch = 0\n","        \n","    def on_epoch_end(self, epoch, logs):\n","        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n","        score = f1_score(self.targets, pred, average=\"macro\")\n","        logger.info(f'\\n epoch:{epoch:03d}, F1Macro: {score:.5f}')   \n","        \n","        if self.best_score is None or \\\n","        (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n","            self.model.save_weights(self.checkpoint_path)\n","#             torch.save(model.state_dict(), self.checkpoint_path)\n","            self.best_score, self.counter = score, 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience: ##stop training\n","                self.stopped_epoch = epoch\n","                self.model.stop_training = True\n","                \n","    def on_train_end(self, logs=None):\n","        if self.stopped_epoch > 0:\n","              logger.info('Epoch %05d: early stopping' % (self.stopped_epoch + 1))   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.589759Z","start_time":"2020-04-11T02:14:24.582640Z"},"id":"1yfpldH_2ucw","trusted":true,"colab_type":"code","colab":{}},"source":["class MacroF1(Callback):\n","    def __init__(self, model, inputs, targets):\n","        self.model = model\n","        self.inputs = inputs\n","        self.targets = np.argmax(targets, axis=2).reshape(-1)\n","\n","    def on_epoch_end(self, epoch, logs):\n","        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n","        score = f1_score(self.targets, pred, average=\"macro\")\n","        print(f' F1Macro: {score:.5f}')    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.596693Z","start_time":"2020-04-11T02:14:24.591931Z"},"id":"skru4lPt2uc6","trusted":true,"colab_type":"code","colab":{}},"source":["def normalize(train, test):\n","    \n","    train_input_mean = train.signal.mean()\n","    train_input_sigma = train.signal.std()\n","    train['signal'] = (train.signal-train_input_mean)/train_input_sigma\n","    test['signal'] = (test.signal-train_input_mean)/train_input_sigma\n","\n","    return train, test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.609602Z","start_time":"2020-04-11T02:14:24.600166Z"},"id":"02TDJtejfS0_","trusted":true,"colab_type":"code","colab":{}},"source":["\n","# def run_everything(fe_config : List) -> NoReturn:\n","#     not_feats_cols = ['time']\n","#     target_col = ['open_channels']\n","#     init_logger()\n","#     with timer(f'Reading Data'):\n","#         logger.info('Reading Darta Started ...')\n","#         base = os.path.abspath(PATH+'liverpool-ion-switching/')\n","#         train, test, sample_submission, train_tfprob, test_tfprob = read_data(base)\n","#         train, test = normalize(train, test)    \n","#         logger.info('Reading and Normalizing Data Completed ...')\n","#     with timer(f'Creating Features'):\n","#         logger.info('Feature Enginnering Started ...')\n","#         for config in fe_config:\n","#             train = run_feat_enginnering(train, create_all_data_feats=config[0], batch_size=config[1])\n","#             test  = run_feat_enginnering(test,  create_all_data_feats=config[0], batch_size=config[1])\n","#             display(train.head())\n","#             display(test.head())\n","#         train, test = add_category(train, test)\n","#         train = combine_data(train, train_tfprob)\n","#         test = combine_data(test, test_tfprob)\n","#         train, test, feats = feature_selection(train, test)\n","#         print(\"after feature processing\")\n","#         display(train.head())\n","#         display(test.head())\n","#         train = reduce_mem_usage(train)\n","#         # train.to_csv('train_kalman_clean_multifea.csv', float_format='%.4f')\n","#         ixs = np.array_split(train.index, 100)\n","#         for ix, subset in tqdm(enumerate(ixs)):\n","#             if ix == 0:\n","#                 train.loc[subset].to_csv('train_kalman_clean_multifea.csv', mode='w')\n","#             else:\n","#                 train.loc[subset].to_csv('train_kalman_clean_multifea.csv', header=None, mode='a')\n","\n","#         print(\"saving train done!!!\")\n","#         test = reduce_mem_usage(test)\n","#         print(\"saving test done!!!\")\n","#         # test.to_csv('test_kalman_clean_multifea.csv', float_format='%.4f')\n","#         ixs = np.array_split(test.index, 100)\n","#         for ix, subset in tqdm(enumerate(ixs)):\n","#             if ix == 0:\n","#                 test.loc[subset].to_csv('test_kalman_clean_multifea.csv', mode='w')\n","#             else:\n","#                 test.loc[subset].to_csv('test_kalman_clean_multifea.csv', header=None, mode='a')\n","#         del train_tfprob\n","#         del test_tfprob      \n","#         logger.info('Feature Enginnering Completed ...')\n","\n","#     with timer(f'Running Wavenet model'):\n","#         logger.info(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started ...')\n","#         run_cv_model_by_batch(train, test, splits=SPLITS, batch_col='group', feats=feats, \n","#                               sample_submission=sample_submission, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n","#         logger.info(f'Training completed ...')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4RhSZ_2-Ph5h","colab_type":"code","colab":{}},"source":["\n","def run_everything(fe_config : List) -> NoReturn:\n","    not_feats_cols = ['time']\n","    target_col = ['open_channels']\n","    init_logger()\n","    with timer(f'Reading Data'):\n","        logger.info('Reading Darta Started ...')\n","        base = os.path.abspath(PATH+'liverpool-ion-switching/')\n","        train, test, sample_submission = read_datanew(base)\n","        # train, test = normalize(train, test)    \n","        logger.info('Reading and Normalizing Data Completed ...')\n","    with timer(f'Creating Features'):\n","        logger.info('Feature Enginnering Started ...')\n","        feats = [col for col in train.columns if col not in ['Unnamed: 0','index','group', 'open_channels', 'time']]\n","        print(feats)\n","        train = reduce_mem_usage(train)\n","        test = reduce_mem_usage(test)\n","        logger.info('Feature Enginnering Completed ...')\n","\n","    with timer(f'Running Wavenet model'):\n","        logger.info(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started ...')\n","        run_cv_model_by_batch(train, test, splits=SPLITS, batch_col='group', feats=feats, \n","                              sample_submission=sample_submission, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n","        logger.info(f'Training completed ...')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:15:53.901793Z","start_time":"2020-04-11T02:14:24.612467Z"},"id":"_w4_OJJtfS1K","trusted":true,"colab_type":"code","outputId":"3d2e1740-a5b9-4b04-8557-4077f12085bc","executionInfo":{"status":"ok","timestamp":1589946543495,"user_tz":420,"elapsed":167459,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["run_everything(fe_config)"],"execution_count":66,"outputs":[{"output_type":"stream","text":["2020-05-20 03:46:27,761 INFO Reading Darta Started ...\n","2020-05-20 03:47:00,669 INFO Reading and Normalizing Data Completed ...\n","2020-05-20 03:47:00,671 INFO [Reading Data] done in 33 s\n","2020-05-20 03:47:00,672 INFO Feature Enginnering Started ...\n"],"name":"stderr"},{"output_type":"stream","text":["['signal', 'signal_shift_pos_1', 'signal_shift_neg_1', 'signal_shift_pos_2', 'signal_shift_neg_2', 'signal_shift_pos_3', 'signal_shift_neg_3', 'signal_2', 'grad_1', 'grad_2', 'lowpass_lf_0.0100', 'lowpass_ff_0.0100', 'lowpass_lf_0.0369', 'lowpass_ff_0.0369', 'lowpass_lf_0.1359', 'lowpass_ff_0.1359', 'lowpass_lf_0.5012', 'lowpass_ff_0.5012', 'highpass_lf_0.0100', 'highpass_ff_0.0100', 'highpass_lf_0.0430', 'highpass_ff_0.0430', 'highpass_lf_0.1848', 'highpass_ff_0.1848', 'highpass_lf_0.7943', 'highpass_ff_0.7943', 'roll_mean_10', 'roll_std_10', 'roll_mean_50', 'roll_std_50', 'ewm_mean_10', 'ewm_std_10', 'ewm_mean_50', 'ewm_std_50', 'category', 'tProb0', 'tProb1', 'tProb2', 'tProb3', 'tProb4', 'tProb5', 'tProb6', 'tProb7', 'tProb8', 'tProb9', 'tProb10']\n","Mem. usage decreased to 514.98 MB (72.2 % reduction)\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-20 03:47:15,234 INFO Feature Enginnering Completed ...\n","2020-05-20 03:47:15,235 INFO [Creating Features] done in 15 s\n","2020-05-20 03:47:15,237 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n"],"name":"stderr"},{"output_type":"stream","text":["Mem. usage decreased to 198.36 MB (72.9 % reduction)\n","ion_switch/res/wavenet-dlr-res/WaveNetAug-ConvGRUtfProb11-NESCAL-MFEA/all-al\n","train.shape: (5000000, 50)\n","test.shape: (2000000, 49)\n","weights_: [ 0.36652399  0.46106257  0.82059173  0.67983748  1.12675802  1.63577934\n","  2.41635544  1.71516878  1.85390282  3.33929955 12.72060713]\n","nums_: 0     1240152\n","1      985865\n","2      553924\n","3      668609\n","4      403410\n","5      277877\n","6      188112\n","7      265015\n","8      245183\n","9      136120\n","10      35733\n","Name: open_channels, dtype: int64\n","cb_weights_: [0.97174558 0.97174558 0.97174558 0.97174558 0.97174567 0.97176004\n"," 0.97227024 0.97176977 0.97179906 0.97596    1.27771292]\n","(1250, 4000, 11)\n","(1250, 4000, 46)\n","X_aug shape 1: (1000, 4000, 46)\n","X_aug shape 2: (1000, 4000, 47)\n","test shape after roll diff: (500, 4000, 47)\n","train_x shape 3:  (2000, 4000, 47)\n","train_y shape 3:  (2000, 4000, 11)\n","model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-20 03:47:40,358 INFO Training fold 1 completed. macro f1 score : 0.94318\n"],"name":"stderr"},{"output_type":"stream","text":["X_aug shape 1: (1000, 4000, 46)\n","X_aug shape 2: (1000, 4000, 47)\n","test shape after roll diff: (500, 4000, 47)\n","train_x shape 3:  (2000, 4000, 47)\n","train_y shape 3:  (2000, 4000, 11)\n","model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-20 03:47:53,332 INFO Training fold 2 completed. macro f1 score : 0.94344\n"],"name":"stderr"},{"output_type":"stream","text":["X_aug shape 1: (1000, 4000, 46)\n","X_aug shape 2: (1000, 4000, 47)\n","test shape after roll diff: (500, 4000, 47)\n","train_x shape 3:  (2000, 4000, 47)\n","train_y shape 3:  (2000, 4000, 11)\n","model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-20 03:48:34,424 INFO Training fold 3 completed. macro f1 score : 0.94182\n"],"name":"stderr"},{"output_type":"stream","text":["X_aug shape 1: (1000, 4000, 46)\n","X_aug shape 2: (1000, 4000, 47)\n","test shape after roll diff: (500, 4000, 47)\n","train_x shape 3:  (2000, 4000, 47)\n","train_y shape 3:  (2000, 4000, 11)\n","model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-20 03:48:44,530 INFO Training fold 4 completed. macro f1 score : 0.94281\n"],"name":"stderr"},{"output_type":"stream","text":["X_aug shape 1: (1000, 4000, 46)\n","X_aug shape 2: (1000, 4000, 47)\n","test shape after roll diff: (500, 4000, 47)\n","train_x shape 3:  (2000, 4000, 47)\n","train_y shape 3:  (2000, 4000, 11)\n","model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-20 03:48:54,650 INFO Training fold 5 completed. macro f1 score : 0.94290\n","2020-05-20 03:48:58,077 INFO Training completed. oof macro f1 score : 0.94286\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>time</th>\n","      <th>open_channels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>500.000092</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>500.000214</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>500.000305</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>500.000397</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>500.000488</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         time  open_channels\n","0  500.000092              0\n","1  500.000214              0\n","2  500.000305              0\n","3  500.000397              0\n","4  500.000488              0"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2020-05-20 03:49:02,367 INFO Training completed ...\n","2020-05-20 03:49:02,368 INFO [Running Wavenet model] done in 107 s\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"cdcRgik41Ugw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PqHiwFl-Vxff","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2CMgZCYC5Yh6","trusted":true,"colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"240b8738-4ae3-48c2-b953-7d7040201cc0","executionInfo":{"status":"ok","timestamp":1589946543496,"user_tz":420,"elapsed":167411,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}}},"source":[" print(\"test\")\n"," "],"execution_count":67,"outputs":[{"output_type":"stream","text":["test\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MjHu5CkUrorQ","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-u3pTgK3mNFt","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EbDSqah3O6Q4","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIkAcnCRRN-O","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}