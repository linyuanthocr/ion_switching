{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WaveNet+Attention+RF.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huixWaL42uZi",
        "colab_type": "text"
      },
      "source": [
        "The validation scheme is based on [seq2seq-rnn-with-gru](https://www.kaggle.com/brandenkmurray/seq2seq-rnn-with-gru/output), and cleaned data is from [data-without-drift](https://www.kaggle.com/cdeotte/data-without-drift) and Kalman filter is from [https://www.kaggle.com/teejmahal20/single-model-lgbm-kalman-filter](single-model-lgbm-kalman-filter) and the added feature is from [wavenet-with-1-more-feature](wavenet-with-1-more-feature). I also used ragnar's data in this version [clean-kalman](https://www.kaggle.com/ragnar123/clean-kalman). The Wavenet is based on [https://github.com/philipperemy/keras-tcn](https://github.com/philipperemy/keras-tcn), [https://github.com/peustr/wavenet](https://github.com/peustr/wavenet) and [https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet) and also [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm). If any refrence is not mentioned it was not intentional, please add them in comments.\n",
        "\n",
        "Previous versions were mainly based on [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.121605Z",
          "start_time": "2020-04-11T02:14:22.792317Z"
        },
        "_kg_hide-input": true,
        "id": "LqmWjeYJ2uZn",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --no-warn-conflicts -q tensorflow-addons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.152214Z",
          "start_time": "2020-04-11T02:14:24.125295Z"
        },
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "y1qOuodBfSxN",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import (TimeDistributed, Dropout, BatchNormalization, Flatten, Convolution1D, Activation, Input, Dense, LSTM, Lambda, Bidirectional,\n",
        "                                     Add, AveragePooling1D, Multiply, GRU, GRUCell, LSTMCell, SimpleRNNCell, SimpleRNN, TimeDistributed, RNN,SpatialDropout1D,\n",
        "                                     RepeatVector, Conv1D, MaxPooling1D, GlobalMaxPooling1D,Concatenate, GlobalAveragePooling1D, UpSampling1D)\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler, CSVLogger\n",
        "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy, mean_squared_error\n",
        "# from tensorflow.keras.experimental import export_saved_model, load_from_saved_model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras import losses, models, optimizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.python.ops import array_ops\n",
        "import tensorflow as tf\n",
        "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
        "from sklearn.metrics import f1_score, cohen_kappa_score, mean_squared_error\n",
        "from logging import getLogger, Formatter, StreamHandler, FileHandler, INFO, DEBUG, NOTSET\n",
        "from sklearn.model_selection import KFold, GroupKFold\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from contextlib import contextmanager\n",
        "from joblib import Parallel, delayed\n",
        "from IPython.display import display\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow_addons as tfa\n",
        "import scipy.stats as stats\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import itertools\n",
        "import warnings\n",
        "import time\n",
        "import pywt\n",
        "import os\n",
        "import gc\n",
        "\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "# from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 1000)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AVvcAEP0iEyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Kaggle = False\n",
        "Colab = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC8XqE14cSRm",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "from pathlib import Path\n",
        "\n",
        "if Colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    path = \"/content/drive/My Drive\"\n",
        "\n",
        "    os.chdir(path)\n",
        "    os.listdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0fJcaGLoPY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-sYfdXdWyTI",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sys.path.append('ion_switch/keras-one-cycle')\n",
        "# # os.listdir(patholr)\n",
        "# from clr import OneCycleLR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.159766Z",
          "start_time": "2020-04-11T02:14:24.155918Z"
        },
        "trusted": true,
        "id": "zdosugVWcOf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if Kaggle:\n",
        "    PATH = '/kaggle/input/'\n",
        "    outdir = '.'\n",
        "# PATH = '/Users/helen/Desktop/Data/'\n",
        "else:\n",
        "    PATH = 'ion_switch/'\n",
        "    outdir = Path(PATH+'res')\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)\n",
        "    outdir = Path(PATH+'res/wavenet-clr-res')\n",
        "    if not os.path.exists(outdir):\n",
        "        os.mkdir(outdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.168634Z",
          "start_time": "2020-04-11T02:14:24.163612Z"
        },
        "id": "UafJMtyefSxU",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=150\n",
        "NNBATCHSIZE= 32\n",
        "BATCHSIZE = 1000\n",
        "SEED = 321\n",
        "SELECT = True\n",
        "SPLITS = 5\n",
        "LR = 0.001\n",
        "WD = 1e-5\n",
        "Gamma = 1.0 #0.99994\n",
        "BETA = 0.99996\n",
        "fe_config = [\n",
        "    (True, BATCHSIZE),\n",
        "]\n",
        "COMPETITION = 'ION-Switching'\n",
        "logger = getLogger(COMPETITION)\n",
        "LOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\n",
        "MODELNAME = 'WaveNet-CLR-FOCALLOSS-Attention-v5'\n",
        "\n",
        "TRAINEDMODEL = os.path.join(outdir, 'wavenet-es-v1/wavenet_es_f0_checkpoint.h5')\n",
        "\n",
        "VERSION = '{}_len{}_bs{}_lr{}'.format(MODELNAME, BATCHSIZE, NNBATCHSIZE, LR)\n",
        "outdir = os.path.join(outdir, VERSION)\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)\n",
        "\n",
        "from datetime import datetime\n",
        "dateTimeObj = datetime.now()\n",
        "timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H\")\n",
        "\n",
        "outdir = os.path.join(outdir, timestampStr)\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fouKVpNaFNKQ",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "@contextmanager\n",
        "def timer(name : Text):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.177661Z",
          "start_time": "2020-04-11T02:14:24.171732Z"
        },
        "id": "EE4v8h1tfSxb",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def init_logger():\n",
        "\n",
        "    handler = StreamHandler()\n",
        "    handler.setLevel(INFO)\n",
        "    handler.setFormatter(Formatter(LOGFORMAT))\n",
        "    fh_handler = FileHandler(os.path.join(outdir,'{}-len{}-lr{}-{}.log'.format(MODELNAME,BATCHSIZE,LR,timestampStr)))\n",
        "    fh_handler.setFormatter(Formatter(LOGFORMAT))\n",
        "    logger.setLevel(INFO)\n",
        "    logger.addHandler(handler)\n",
        "    logger.addHandler(fh_handler)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.358957Z",
          "start_time": "2020-04-11T02:14:24.187096Z"
        },
        "id": "OC5DOcDifSxx",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def seed_everything(seed : int) -> NoReturn :\n",
        "    \n",
        "    rn.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    # os.environ['TF_CUDNN_DETERMINISTIC'] = str(seed) \n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tInVDxBn-lQ",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CyclicLR(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma ** (x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "\n",
        "    def clr(self):\n",
        "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
        "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n",
        "                self.clr_iterations)\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "\n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "        # print(\"learning rate- self.model.optimizer.lr: \", self.model.optimizer.lr)\n",
        "\n",
        "    # def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "    #     logs = logs or {}\n",
        "    #     self.trn_iterations += 1\n",
        "    #     self.clr_iterations += 1\n",
        "\n",
        "    #     K.set_value(self.model.optimizer.lr, self.clr())\n",
        "    #     logger.info(f'epoch:{epoch:03d},'+str(K.eval(self.model.optimizer.lr)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmKgNg1Mmzrk",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 写一个LossHistory类，保存训练集的loss和acc\n",
        "# 当然我也可以完全不这么做，可以直接使用model.fit()方法返回的 history对象去做\n",
        "'''Callback有6个常用的方法，这里实现其中的四个\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "    def on_train_begin(self, logs=None):\n",
        "    def on_train_end(self, logs=None):\n",
        "'''\n",
        "class LossHistory(Callback):  # 继承自Callback类\n",
        " \n",
        "    '''\n",
        "    在模型开始的时候定义四个属性，每一个属性都是字典类型，存储相对应的值和epoch\n",
        "    '''\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = {'batch':[], 'epoch':[]}\n",
        "        self.accuracy = {'batch':[], 'epoch':[]}\n",
        "        self.val_loss = {'batch':[], 'epoch':[]}\n",
        "        self.val_acc = {'batch':[], 'epoch':[]}\n",
        " \n",
        "    # 在每一个batch结束后记录相应的值\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses['batch'].append(logs.get('loss'))\n",
        "        self.accuracy['batch'].append(logs.get('accuracy'))\n",
        "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
        "        self.val_acc['batch'].append(logs.get('val_accuracy'))\n",
        "    \n",
        "    # 在每一个epoch之后记录相应的值\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        trloss, tracc, vloss, vacc = logs.get('loss'), logs.get('accuracy'), logs.get('val_loss'), logs.get('val_accuracy')\n",
        "        self.losses['epoch'].append(trloss)\n",
        "        self.accuracy['epoch'].append(tracc)\n",
        "        self.val_loss['epoch'].append(vloss)\n",
        "        self.val_acc['epoch'].append(vacc)\n",
        "        logger.info(\"epoch:{:03d}, train_loss:{:1.5f}, train_acc:{:1.5f}, val_loss:{:1.5f}, val_acc:{:1.5f}\".format(epoch, \n",
        "                                                                                                                trloss, tracc, vloss, vacc))\n",
        " \n",
        "    def loss_plot(self, loss_type, pngname):\n",
        "        '''\n",
        "        loss_type：指的是 'epoch'或者是'batch'，分别表示是一个batch之后记录还是一个epoch之后记录\n",
        "        '''\n",
        "        iters = range(len(self.losses[loss_type]))\n",
        "        plt.figure()\n",
        "        # acc\n",
        "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
        "        # loss\n",
        "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
        "        if loss_type == 'epoch':\n",
        "            # val_acc\n",
        "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
        "            # val_loss\n",
        "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
        "        plt.grid(True)\n",
        "        plt.xlabel(loss_type)\n",
        "        plt.ylabel('acc-loss')\n",
        "        plt.legend(loc=\"upper right\")\n",
        "        plt.savefig(pngname)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.368249Z",
          "start_time": "2020-04-11T02:14:24.362616Z"
        },
        "id": "adUHGQUTfSyA",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def read_data(base : os.path.abspath) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \n",
        "    train = pd.read_csv(PATH+'clean-kalman/train_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
        "    test  = pd.read_csv(PATH+'clean-kalman/test_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32})\n",
        "    sub  = pd.read_csv(PATH+'liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n",
        "    \n",
        "    return train, test, sub\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.376185Z",
          "start_time": "2020-04-11T02:14:24.371687Z"
        },
        "id": "HpDaJQ5yfSyI",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def batching(df : pd.DataFrame,\n",
        "             batch_size : int) -> pd.DataFrame :\n",
        "    \n",
        "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
        "    df['group'] = df['group'].astype(np.uint16)\n",
        "        \n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.391087Z",
          "start_time": "2020-04-11T02:14:24.378989Z"
        },
        "id": "iQxOYF3tfSyj",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def reduce_mem_usage(df: pd.DataFrame,\n",
        "                     verbose: bool = True) -> pd.DataFrame:\n",
        "    \n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "\n",
        "            if str(col_type)[:3] == 'int':\n",
        "\n",
        "                if (c_min > np.iinfo(np.int32).min\n",
        "                      and c_max < np.iinfo(np.int32).max):\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif (c_min > np.iinfo(np.int64).min\n",
        "                      and c_max < np.iinfo(np.int64).max):\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if (c_min > np.finfo(np.float16).min\n",
        "                        and c_max < np.finfo(np.float16).max):\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif (c_min > np.finfo(np.float32).min\n",
        "                      and c_max < np.finfo(np.float32).max):\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    reduction = (start_mem - end_mem) / start_mem\n",
        "\n",
        "    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n",
        "    if verbose:\n",
        "        print(msg)\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.402186Z",
          "start_time": "2020-04-11T02:14:24.393787Z"
        },
        "id": "dytmNmL_fSzj",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def lag_with_pct_change(df : pd.DataFrame,\n",
        "                        shift_sizes : Optional[List]=[1, 2],\n",
        "                        add_pct_change : Optional[bool]=False,\n",
        "                        add_pct_change_lag : Optional[bool]=False,\n",
        "                        add_diff : Optional[bool]=False) -> pd.DataFrame:\n",
        "    \n",
        "    for shift_size in shift_sizes:    \n",
        "        df['signal_shift_pos_'+str(shift_size)] = df.groupby('group')['signal'].shift(shift_size).fillna(0)\n",
        "        df['signal_shift_neg_'+str(shift_size)] = df.groupby('group')['signal'].shift(-1*shift_size).fillna(0)\n",
        "\n",
        "    if add_pct_change:\n",
        "        df['pct_change'] = df['signal'].pct_change()\n",
        "        if add_pct_change_lag:\n",
        "            for shift_size in shift_sizes:    \n",
        "                df['pct_change_shift_pos_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(shift_size).fillna(0)\n",
        "                df['pct_change_shift_neg_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(-1*shift_size).fillna(0)\n",
        "    if add_diff:\n",
        "        for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'group', 'index']]:\n",
        "            df[c+'_msignal'] = df[c] - df['signal']\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.409381Z",
          "start_time": "2020-04-11T02:14:24.404800Z"
        },
        "id": "m-ULWLF_fS0B",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def run_feat_enginnering(df : pd.DataFrame,\n",
        "                         create_all_data_feats : bool,\n",
        "                         batch_size : int) -> pd.DataFrame:\n",
        "    \n",
        "    df = batching(df, batch_size=batch_size)\n",
        "    if create_all_data_feats:\n",
        "        df = lag_with_pct_change(df, [1, 2, 3],  add_pct_change=False, add_pct_change_lag=False, add_diff=True)\n",
        "    df['signal_2'] = df['signal'] ** 2\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.419991Z",
          "start_time": "2020-04-11T02:14:24.412368Z"
        },
        "id": "6E87jcu2fS0O",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_selection(df : pd.DataFrame,\n",
        "                      df_test : pd.DataFrame) -> Tuple[pd.DataFrame , pd.DataFrame, List]:\n",
        "    use_cols = [col for col in df.columns if col not in ['index','group', 'open_channels', 'time']]\n",
        "    print(use_cols)\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "    df_test = df_test.replace([np.inf, -np.inf], np.nan)\n",
        "    for col in use_cols:\n",
        "        col_mean = pd.concat([df[col], df_test[col]], axis=0).mean()\n",
        "        df[col] = df[col].fillna(col_mean)\n",
        "        df_test[col] = df_test[col].fillna(col_mean)\n",
        "   \n",
        "    gc.collect()\n",
        "    return df, df_test, use_cols\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.427483Z",
          "start_time": "2020-04-11T02:14:24.422535Z"
        },
        "trusted": true,
        "id": "Ft7_2jVZcOgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def augment(X: np.array, y:np.array) -> Tuple[np.array, np.array]:\n",
        "    \n",
        "    X = np.vstack((X, np.flip(X, axis=1)))\n",
        "    y = np.vstack((y, np.flip(y, axis=1)))\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.432964Z",
          "start_time": "2020-04-11T02:14:24.430264Z"
        },
        "trusted": true,
        "id": "Y-nODxu7cOgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add ops to save and restore all the variables.\n",
        "# saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.438940Z",
          "start_time": "2020-04-11T02:14:24.436211Z"
        },
        "trusted": true,
        "id": "VRzoz-eacOgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:24:41.652529Z\",\"start_time\":\"2020-04-03T23:24:41.645025Z\"}}\n",
        "# class EarlyStopping:\n",
        "#     def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n",
        "#         self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n",
        "#         self.counter, self.best_score = 0, None\n",
        "#         self.is_maximize = is_maximize\n",
        "\n",
        "#     def load_best_weights(self, sess):\n",
        "#         saver.restore(sess, self.checkpoint_path)\n",
        "\n",
        "#     def __call__(self, score, sess):\n",
        "#         if self.best_score is None or \\\n",
        "#         (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n",
        "#             saver.save(sess, self.checkpoint_path)\n",
        "#             self.best_score, self.counter = score, 0\n",
        "#         else:\n",
        "#             self.counter += 1\n",
        "#             if self.counter >= self.patience:\n",
        "#                 return True\n",
        "#         return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzq5iOKjuWUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "class ShiftedFeatureMaker(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    def __init__(self, periods=[1], column=\"signal\", add_minus=False, fill_value=None, copy=True):\n",
        "        self.periods = periods\n",
        "        self.column = column\n",
        "        self.add_minus = add_minus\n",
        "        self.fill_value = fill_value\n",
        "        self.copy = copy\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Mock method\"\"\"\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X: pd.DataFrame, y=None):\n",
        "        periods = np.asarray(self.periods, dtype=np.int32)\n",
        "        \n",
        "        if self.add_minus:\n",
        "            periods = np.append(periods, -periods)\n",
        "        \n",
        "        X_transformed = X.copy() if self.copy else X\n",
        "        \n",
        "        for p in periods:\n",
        "            X_transformed[f\"{self.column}_shifted_{p}\"] = X_transformed[self.column].shift(\n",
        "                periods=p, fill_value=self.fill_value\n",
        "            )\n",
        "            \n",
        "        return X_transformed\n",
        "\n",
        "\n",
        "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
        "    \n",
        "    def __init__(self, columns=None):\n",
        "        self.columns = columns\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Mock method\"\"\"\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X: pd.DataFrame, y=None):\n",
        "        return X[[c for c in X.columns if c not in self.columns]]\n",
        "\n",
        "\n",
        "def add_category(train, test):\n",
        "    train[\"category\"] = 0\n",
        "    test[\"category\"] = 0\n",
        "    \n",
        "    # train segments with more then 9 open channels classes\n",
        "    train.loc[2_000_000:2_500_000-1, 'category'] = 1\n",
        "    train.loc[4_500_000:5_000_000-1, 'category'] = 1\n",
        "    \n",
        "    # # delete very noized part \n",
        "    # train.loc[3_650_000:3_820_000, \"category\"] = -1\n",
        "    # train = train[train.category != -1].reset_index(drop=True)\n",
        "    \n",
        "    # test segments with more then 9 open channels classes (potentially)\n",
        "    test.loc[500_000:600_000-1, \"category\"] = 1\n",
        "    test.loc[700_000:800_000-1, \"category\"] = 1\n",
        "    \n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6l0nc-llBdy",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "def prob_with_RF(train_rfc, val_rfc, test_rfc, pkl_model_filename, force_build=False):\n",
        "    shifted_rfc = make_pipeline(\n",
        "        ShiftedFeatureMaker(\n",
        "            periods=range(1, 20),\n",
        "            add_minus=True,\n",
        "            fill_value=0\n",
        "        ),\n",
        "        ColumnDropper(\n",
        "            columns=[\"open_channels\", \"time\", \"group\"]\n",
        "        ),\n",
        "        # RandomForestClassifier(\n",
        "        #     n_estimators=120,\n",
        "        #     max_depth=19,\n",
        "        #     max_features=10,\n",
        "        #     random_state=42,\n",
        "        #     n_jobs=20,\n",
        "        #     verbose=2\n",
        "        # )\n",
        "        RandomForestClassifier(\n",
        "            n_estimators=20,\n",
        "            max_depth=10,\n",
        "            max_features=10,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbose=2\n",
        "        )\n",
        "    )\n",
        "    if force_build or (not os.path.exists(pkl_model_filename)):\n",
        "        print(\"Train RF model:\", pkl_model_filename)\n",
        "        shifted_rfc.fit(train_rfc, train_rfc.open_channels)\n",
        "        # Save to file in the current working directory\n",
        "        # pkl_filename = \"pickle_model.pkl\"\n",
        "        with open(pkl_model_filename, 'wb') as file:\n",
        "          pickle.dump(shifted_rfc, file)\n",
        "          print(\"Save RF model: \", pkl_model_filename)\n",
        "\n",
        "    # Load from file\n",
        "    with open(pkl_model_filename, 'rb') as file:\n",
        "        pickle_model = pickle.load(file)\n",
        "        print(\"Loading RF model: \", pkl_model_filename)\n",
        "    train_predictions = pickle_model.predict_proba(train_rfc)\n",
        "    print(\"train predic\")\n",
        "    print(train_predictions[:10])\n",
        "    test_predictions = pickle_model.predict_proba(test_rfc)\n",
        "    val_predictions = pickle_model.predict_proba(val_rfc)\n",
        "    for i in range(11):\n",
        "        train_rfc['Prob_{}'.format(i)] = train_predictions[:,i]\n",
        "        test_rfc['Prob_{}'.format(i)] = test_predictions[:,i]\n",
        "        val_rfc['Prob_{}'.format(i)] = val_predictions[:,i]\n",
        "    return train_rfc, val_rfc, test_rfc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xuj2XnjrBJmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a = np.random.randn(3,4,2)\n",
        "# print(a)\n",
        "# print(a.shape)\n",
        "# b = np.random.randn(3,4)\n",
        "# print(b)\n",
        "# print(b.shape)\n",
        "# print(b[:,:,None].shape)\n",
        "# c = np.concatenate((a,b[:,:,None]), axis=2)\n",
        "# print(c)\n",
        "# print(c.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.477074Z",
          "start_time": "2020-04-11T02:14:24.441269Z"
        },
        "code_folding": [],
        "id": "e4QulGxHfS0n",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def run_cv_model_by_batch(train : pd.DataFrame,\n",
        "                          test : pd.DataFrame,\n",
        "                          splits : int,\n",
        "                          batch_col : Text,\n",
        "                          feats : List,\n",
        "                          sample_submission: pd.DataFrame,\n",
        "                          nn_epochs : int,\n",
        "                          nn_batch_size : int) -> NoReturn:\n",
        "    seed_everything(SEED)\n",
        "    K.clear_session()\n",
        "    if not os.path.exists(outdir):\n",
        "      os.mkdir(outdir)\n",
        "    print(outdir)\n",
        "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
        "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
        "    tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "    train_rfc = pd.DataFrame(train, columns=['time', 'signal', 'open_channels', 'group'])\n",
        "\n",
        "    test_rfc = pd.DataFrame(test, columns=['time', 'signal', 'group'])\n",
        "    train_rfc, test_rfc = add_category(train_rfc, test_rfc)\n",
        "    print(train_rfc.shape[0],train_rfc.shape[1])\n",
        "    print(train_rfc.head())\n",
        "    oof_ = np.zeros((len(train), 11))\n",
        "    preds_ = np.zeros((len(test), 11))\n",
        "    target = ['open_channels']\n",
        "    group = train['group']\n",
        "    kf = GroupKFold(n_splits=5)\n",
        "    splits = [x for x in kf.split(train, train[target], group)]\n",
        "\n",
        "    pkl_path = os.path.join(PATH,'res/RFC/pickle_model_rfc_20_10.pkl')\n",
        "    pkl_gen_path = os.path.join(PATH,'res/RFC/pickle_model_rfc_20_10_run3.pkl')\n",
        "\n",
        "    new_splits = []\n",
        "    for sp in splits:\n",
        "        new_split = []\n",
        "        new_split.append(np.unique(group[sp[0]]))\n",
        "        new_split.append(np.unique(group[sp[1]]))\n",
        "        new_split.append(sp[0])  \n",
        "        new_split.append(sp[1])    \n",
        "        new_splits.append(new_split)\n",
        "\n",
        "    # Calculate the weights for each class so that we can balance the data\n",
        "    weights_ = class_weight.compute_class_weight('balanced',\n",
        "                                                np.unique(train.open_channels),\n",
        "                                                train.open_channels)\n",
        "    print(\"weights_:\", weights_)\n",
        "\n",
        "    nums_ = train.open_channels.value_counts(sort=False)\n",
        "    print(\"nums_:\", nums_)\n",
        "    beta = BETA\n",
        "    effective_num = 1.0 - np.power(beta, nums_)\n",
        "    weights = (1.0 - beta) / np.array(effective_num)\n",
        "    cb_weights_ = weights / np.sum(weights) * 11\n",
        "    print(\"cb_weights_:\", cb_weights_)   \n",
        "        \n",
        "    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n",
        "\n",
        "    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n",
        "    # print(tr.head())\n",
        "    target_cols = ['target_'+str(i) for i in range(11)]\n",
        "    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n",
        "    print(np.shape(train_tr))\n",
        "    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n",
        "    print(np.shape(train))\n",
        "    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n",
        "    print(np.shape(test))\n",
        "\n",
        "    for n_fold, (tr_idx, val_idx, tr_orig_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n",
        "        sub_dir = os.path.join(outdir,\"{}_fold\".format(n_fold))\n",
        "        # if n_fold<3:\n",
        "        #     continue\n",
        "        if n_fold>0:\n",
        "            break\n",
        "        if not os.path.exists(sub_dir):\n",
        "            os.mkdir(sub_dir)\n",
        "        # pkl_model_filename = os.path.join(sub_dir,'rf-{}.pkl'.format(n_fold))\n",
        "        print(\"train index\")\n",
        "        print(\"tr_orig_idx max and min:\", max(tr_orig_idx), min(tr_orig_idx))\n",
        "        print(\"tr_orig_idx shape:\", np.shape(tr_orig_idx))\n",
        "        train_rfc_ = train_rfc.iloc[tr_orig_idx].copy()\n",
        "        valid_rfc_ = train_rfc.iloc[val_orig_idx].copy()\n",
        "\n",
        "        train_rfc_, valid_rfc_, test_rfc = prob_with_RF(train_rfc_, valid_rfc_, test_rfc, pkl_gen_path)\n",
        "        # print(train_rfc_.head())\n",
        "        # print(valid_rfc_.head())\n",
        "        # print(test_rfc.head())\n",
        "        print(\"train_rfc_.shape: \",train_rfc_.shape)\n",
        "        print(\"tr_idx shape: \", np.shape(tr_idx))\n",
        "        print(\"train shape:\", np.shape(train))\n",
        "\n",
        "        # if n_fold < 2:\n",
        "        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n",
        "        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n",
        "        test_ = test.copy()\n",
        "        print(\"train_x shape 1: \", np.shape(train_x))\n",
        "        for i in range(11):\n",
        "          trainprob = np.array(list(train_rfc_.groupby('group').apply(lambda x: x['Prob_{}'.format(i)].values)))\n",
        "          train_x = np.concatenate((train_x, trainprob[:,:,None]),axis = 2)\n",
        "          vprob = np.array(list(valid_rfc_.groupby('group').apply(lambda x: x['Prob_{}'.format(i)].values)))\n",
        "          valid_x = np.concatenate((valid_x, vprob[:,:,None]),axis = 2)\n",
        "          testprob = np.array(list(test_rfc.groupby('group').apply(lambda x: x['Prob_{}'.format(i)].values)))\n",
        "          test_ = np.concatenate((test_, testprob[:,:,None]),axis = 2)\n",
        "        print(\"train_x shape 2: \", np.shape(train_x))\n",
        "\n",
        "        train_x, train_y = augment(train_x, train_y)\n",
        "\n",
        "        gc.collect()\n",
        "        shape_ = (None, train_x.shape[2])\n",
        "        model = ClassifierARF(shape_)\n",
        "        print(\"model initilization done!\")\n",
        "        # cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
        "        cb_clr = CyclicLR(base_lr=1e-7, max_lr = LR, step_size= int(1.0*(train.shape[0])/(nn_batch_size*4)) , \n",
        "                          mode='exp_range', gamma=Gamma, scale_fn=None, scale_mode='cycle')\n",
        "        cb_prg = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,leave_overall_progress=False, \n",
        "                                               show_epoch_progress=False,show_overall_progress=True)\n",
        "        # cb_csv_logger= CSVLogger(os.path.join(sub_dir,'res.csv'))\n",
        "        # cb_history = LossHistory()  # 这里是使用自定义的Callback回调函数，当然本身fit函数也会返回一个history可供使用\n",
        "        \n",
        "        save_checkpoint_path = os.path.join(sub_dir,'checkpoint-modelonly-{}.h5'.format(n_fold))\n",
        "        save_finalmodel_path = os.path.join(sub_dir,'fmodel-modelonly-{}.h5'.format(n_fold))\n",
        "        save_bestf1macro_path = os.path.join(sub_dir,'checkpoint-{}.h5'.format(n_fold)) \n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_checkpoint_path,\n",
        "                                              monitor='val_accuracy',\n",
        "                                              mode = 'max',            \n",
        "                                              save_weights_only=True,\n",
        "                                              save_best_only=True,           \n",
        "                                              verbose=1)\n",
        "\n",
        "        history = model.fit(train_x,train_y,\n",
        "                            epochs=nn_epochs,\n",
        "                            callbacks=[cb_prg, cb_clr, cp_callback,\n",
        "                                        MacroF1ES(model, valid_x, valid_y, patience=70, delta=0, \n",
        "                                                  checkpoint_path=save_bestf1macro_path)],\n",
        "                            batch_size=nn_batch_size,\n",
        "                            verbose=1,\n",
        "                            validation_data=(valid_x,valid_y))\n",
        "        \n",
        "        pd.DataFrame(history.history).to_csv(os.path.join(sub_dir,'{}-len{}-lr{}-{}-log-{}.csv'.format(MODELNAME,BATCHSIZE,LR,timestampStr, n_fold)), float_format='%.4f')\n",
        "        # print('\\nhistory dict:', history.history)\n",
        "        model.save_weights(save_finalmodel_path)\n",
        "        model.load_weights(save_bestf1macro_path)\n",
        "        preds_f = model.predict(valid_x)\n",
        "        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  \n",
        "                             np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n",
        "        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n",
        "        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
        "        oof_[val_orig_idx,:] += preds_f\n",
        "        te_preds = model.predict(test_)\n",
        "        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n",
        "        preds_ += te_preds / SPLITS\n",
        "\n",
        "    f1_score_ =f1_score(np.argmax(train_tr, axis=2).reshape(-1),  np.argmax(oof_, axis=1), average = 'macro')\n",
        "    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n",
        "    sample_submission['open_channels'] = np.argmax(preds_, axis=1).astype(int)\n",
        "    sample_submission.to_csv(os.path.join(outdir,'{}_pred.csv'.format(VERSION)), index=False, float_format='%.4f')\n",
        "    display(sample_submission.head())\n",
        "    # np.save(os.path.join(outdir,'oof.npy'), oof_)\n",
        "    # np.save(os.path.join(outdir,'preds.npy'), preds_)\n",
        "\n",
        "    return \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.485692Z",
          "start_time": "2020-04-11T02:14:24.479620Z"
        },
        "id": "dqt0VJAS2ucB",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    if epoch < 40:\n",
        "        lr = LR\n",
        "    elif epoch < 50:\n",
        "        lr = LR / 3\n",
        "    elif epoch < 60:\n",
        "        lr = LR / 6\n",
        "    elif epoch < 75:\n",
        "        lr = LR / 9\n",
        "    elif epoch < 85:\n",
        "        lr = LR / 12\n",
        "    elif epoch < 100:\n",
        "        lr = LR / 15\n",
        "    else:\n",
        "        lr = LR / 50\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaClcd8zJIyC",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.499088Z",
          "start_time": "2020-04-11T02:14:24.488223Z"
        },
        "id": "SjbrEQSa2ucN",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mish(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Mish, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs * K.tanh(K.softplus(inputs))\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(Mish, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "def mish(x):\n",
        "\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n",
        " \n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "# from tensorflow.keras.layers import Activation\n",
        "get_custom_objects().update({'mish': Activation(mish)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.521416Z",
          "start_time": "2020-04-11T02:14:24.502541Z"
        },
        "id": "dM8RiNfL2ucY",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import constraints\n",
        "\n",
        "class Attention(Layer):\n",
        "    \"\"\"Multi-headed attention layer.\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size, \n",
        "                 num_heads = 8, \n",
        "                 attention_dropout=.1,\n",
        "                 trainable=True,\n",
        "                 name='Attention'):\n",
        "        \n",
        "        if hidden_size % num_heads != 0:\n",
        "            raise ValueError(\"Hidden size must be evenly divisible by the number of heads.\")\n",
        "            \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.trainable = trainable\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.dense = tf.keras.layers.Dense(self.hidden_size, use_bias=False)\n",
        "        self.time_dense = TimeDistributed(Dense(self.hidden_size, use_bias=False))\n",
        "        super(Attention, self).__init__(name=name)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"Split x into different heads, and transpose the resulting value.\n",
        "        The tensor is transposed to insure the inner dimensions hold the correct\n",
        "        values during the matrix multiplication.\n",
        "        Args:\n",
        "          x: A tensor with shape [batch_size, length, hidden_size]\n",
        "        Returns:\n",
        "          A tensor with shape [batch_size, num_heads, length, hidden_size/num_heads]\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"split_heads\"):\n",
        "            batch_size = tf.shape(x)[0]\n",
        "            length = tf.shape(x)[1]\n",
        "\n",
        "            # Calculate depth of last dimension after it has been split.\n",
        "            depth = (self.hidden_size // self.num_heads)\n",
        "\n",
        "            # Split the last dimension\n",
        "            x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n",
        "\n",
        "            # Transpose the result\n",
        "            return tf.transpose(x, [0, 2, 1, 3])\n",
        "    \n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"Combine tensor that has been split.\n",
        "        Args:\n",
        "          x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n",
        "        Returns:\n",
        "          A tensor with shape [batch_size, length, hidden_size]\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"combine_heads\"):\n",
        "            batch_size = tf.shape(x)[0]\n",
        "            length = tf.shape(x)[2]\n",
        "            x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n",
        "            return tf.reshape(x, [batch_size, length, self.hidden_size])        \n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Apply attention mechanism to inputs.\n",
        "        Args:\n",
        "          inputs: a tensor with shape [batch_size, length_x, hidden_size]\n",
        "        Returns:\n",
        "          Attention layer output with shape [batch_size, length_x, hidden_size]\n",
        "        \"\"\"\n",
        "        # Google developper use tf.layer.Dense to linearly project the queries, keys, and values.\n",
        "        q = self.dense(inputs)\n",
        "        k = self.dense(inputs)\n",
        "        v = self.dense(inputs)\n",
        "\n",
        "        q = self.split_heads(q)\n",
        "        k = self.split_heads(k)\n",
        "        v = self.split_heads(v)\n",
        "        \n",
        "        # Scale q to prevent the dot product between q and k from growing too large.\n",
        "        depth = (self.hidden_size // self.num_heads)\n",
        "        q *= depth ** -0.5\n",
        "        \n",
        "        logits = tf.matmul(q, k, transpose_b=True)\n",
        "        # logits += self.bias\n",
        "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
        "        \n",
        "        if self.trainable:\n",
        "            weights = tf.nn.dropout(weights, 1.0 - self.attention_dropout)\n",
        "        \n",
        "        attention_output = tf.matmul(weights, v)\n",
        "        attention_output = self.combine_heads(attention_output)\n",
        "        attention_output = self.time_dense(attention_output)\n",
        "        attention_output = Dropout(self.attention_dropout)(attention_output)\n",
        "        return attention_output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tf.TensorShape(input_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(Attention, self).get_config()\n",
        "        config = {'hidden_size' : self.hidden_size,\n",
        "                    'num_heads' : self.num_heads,\n",
        "                    'trainable' : self.trainable,\n",
        "                    'attention_dropout' : self.attention_dropout,\n",
        "                    'name':'Attention'}\n",
        "        # config = {'name':'Attention'}\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrzikcyx8ptJ",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# focal loss with one-hot labels, multiclass\n",
        "def cb_focal_loss(classes_weights, gamma=2., alpha=.25, w=3e-4):\n",
        "    # classes_weights contains weights of each classes\n",
        "    def cb_focal_loss_fixed(target_tensor, prediction_tensor):\n",
        "        '''\n",
        "        prediction_tensor is the output tensor with shape [None, 100], where 100 is the number of classes\n",
        "        target_tensor is the label tensor, same shape as predcition_tensor\n",
        "        '''\n",
        "\n",
        "        #1# get focal loss with no balanced weight which presented in paper function (4)\n",
        "        zeros = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n",
        "        one_minus_p = array_ops.where(tf.greater(target_tensor,zeros), target_tensor - prediction_tensor, zeros)\n",
        "        FT = -1 * (one_minus_p ** gamma) * tf.math.log(tf.clip_by_value(prediction_tensor, 1e-8, 1.0))\n",
        "\n",
        "        #2# get balanced weight alpha\n",
        "        classes_weight = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n",
        "        classes_w_tensor = tf.convert_to_tensor(classes_weights, dtype=prediction_tensor.dtype)\n",
        "        classes_weight += classes_w_tensor\n",
        "\n",
        "        alpha = array_ops.where(tf.greater(target_tensor, zeros), classes_weight, zeros)\n",
        "\n",
        "        #3# get balanced focal loss\n",
        "        balanced_fl = alpha * FT\n",
        "        balanced_fl = tf.reduce_mean(balanced_fl)\n",
        "\n",
        "        #4# add other op to prevent overfit\n",
        "        # reference : https://spaces.ac.cn/archives/4493\n",
        "        nb_classes = len(classes_weights)\n",
        "        fianal_loss = (1-w) * balanced_fl + w * K.categorical_crossentropy(K.ones_like(prediction_tensor)/nb_classes, prediction_tensor)\n",
        "\n",
        "        return fianal_loss\n",
        "    return cb_focal_loss_fixed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM2iC9UwFd-c",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_category_focal_loss2(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    focal loss for multi category of multi label problem\n",
        "    适用于多分类或多标签问题的focal loss\n",
        "    alpha控制真值y_true为1/0时的权重\n",
        "        1的权重为alpha, 0的权重为1-alpha\n",
        "    当你的模型欠拟合，学习存在困难时，可以尝试适用本函数作为loss\n",
        "    当模型过于激进(无论何时总是倾向于预测出1),尝试将alpha调小\n",
        "    当模型过于惰性(无论何时总是倾向于预测出0,或是某一个固定的常数,说明没有学到有效特征)\n",
        "        尝试将alpha调大,鼓励模型进行预测出1。\n",
        "    Usage:\n",
        "     model.compile(loss=[multi_category_focal_loss2(alpha=0.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "    \"\"\"\n",
        "    epsilon = 1.e-7\n",
        "    gamma = float(gamma)\n",
        "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
        "\n",
        "    def multi_category_focal_loss2_fixed(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "    \n",
        "        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
        "        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
        "        ce = -tf.math.log(y_t)\n",
        "        weight = tf.pow(tf.subtract(1., y_t), gamma)\n",
        "        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
        "        loss = tf.reduce_mean(fl)\n",
        "        return loss\n",
        "    return multi_category_focal_loss2_fixed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6d4mH_0Omj2",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.549703Z",
          "start_time": "2020-04-11T02:14:24.533479Z"
        },
        "id": "YSESlpaNwkkx",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def WaveNetResidualConv1D(num_filters, kernel_size, stacked_layer):\n",
        "\n",
        "    def build_residual_block(l_input):\n",
        "        resid_input = l_input\n",
        "        for dilation_rate in [2**i for i in range(stacked_layer)]:\n",
        "            l_sigmoid_conv1d = Conv1D(\n",
        "              num_filters, kernel_size, dilation_rate=dilation_rate,\n",
        "              padding='same', activation='sigmoid')(l_input)\n",
        "            l_tanh_conv1d = Conv1D(\n",
        "             num_filters, kernel_size, dilation_rate=dilation_rate,\n",
        "             padding='same', activation='mish')(l_input)\n",
        "            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n",
        "            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n",
        "            resid_input = Add()([resid_input ,l_input])\n",
        "        return resid_input\n",
        "    return build_residual_block\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEn4iIRTaslB",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Classifier(shape_):\n",
        "    num_filters_ = 16\n",
        "    kernel_size_ = 3\n",
        "    stacked_layers_ = [12, 8, 4, 1]\n",
        "    l_input = Input(shape=(shape_))\n",
        "    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n",
        "    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n",
        "    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n",
        "    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n",
        "    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n",
        "    l_output = Dense(11, activation='softmax')(x)\n",
        "    model = models.Model(inputs=[l_input], outputs=[l_output])\n",
        "    opt = Adam(lr=LR)\n",
        "    opt = tfa.optimizers.SWA(opt)\n",
        "#      MacroF1(model, valid_x,valid_y)\n",
        "    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, \n",
        "                  metrics=['accuracy', Precision(), Recall(), get_f1])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZymKtoVo8F06",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ClassifierW(shape_, weights_):\n",
        "    num_filters_ = 16\n",
        "    kernel_size_ = 3\n",
        "    stacked_layers_ = [12, 8, 4, 1]\n",
        "    l_input = Input(shape=(shape_))\n",
        "    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n",
        "    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n",
        "    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n",
        "    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n",
        "    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n",
        "    l_output = Dense(11, activation='softmax')(x)\n",
        "    model = models.Model(inputs=[l_input], outputs=[l_output])\n",
        "    opt = Adam(lr=LR)\n",
        "    opt = tfa.optimizers.SWA(opt)\n",
        "\n",
        "#      MacroF1(model, valid_x,valid_y)\n",
        "    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n",
        "                  metrics=['accuracy', Precision(), Recall(), get_f1])\n",
        "\n",
        "    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n",
        "    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n",
        "\n",
        "#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n",
        "#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.563702Z",
          "start_time": "2020-04-11T02:14:24.556844Z"
        },
        "id": "9Cckpy4I2uco",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Classifierx(shape_):        \n",
        "    # dsize = [256, 512, 256, 128, 11] # try 2: batchsize = 64, len = 1000\n",
        "    dsize = [128, 256, 128, 64, 11] # try3: batchsize = 32, len = 1000\n",
        "    # dsize = [32, 64, 32, 32, 11]  # try 1: batchsize=10, len=1000\n",
        "    inp = Input(shape=(shape_))\n",
        "    x = Bidirectional(GRU(dsize[0], return_sequences=True))(inp)\n",
        "    x = Attention(dsize[1])(x)\n",
        "    x = TimeDistributed(Dense(dsize[2], activation='mish'))(x)\n",
        "    x = TimeDistributed(Dense(dsize[3], activation='mish'))(x)\n",
        "    out = TimeDistributed(Dense(dsize[4], activation='softmax', name='out'))(x)\n",
        "    \n",
        "    model = models.Model(inputs=inp, outputs=out) \n",
        "    \n",
        "    opt = Adam(lr=LR)\n",
        "    opt = tfa.optimizers.SWA(opt)\n",
        "    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n",
        "                  metrics=['accuracy', Precision(), Recall(), get_f1])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnggxiGxh4Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Classifierx4(shape_):        \n",
        "    # dsize = [256, 512, 256, 128, 11] # try 2: batchsize = 64, len = 1000\n",
        "    dsize = [128, 256, 128, 64, 11] # try3: batchsize = 32, len = 1000\n",
        "    # dsize = [32, 64, 32, 32, 11]  # try 1: batchsize=10, len=1000\n",
        "    inp = Input(shape=(shape_))\n",
        "    x = Bidirectional(GRU(dsize[0], return_sequences=True))(inp)\n",
        "    x = Attention(dsize[1])(x)\n",
        "    x = TimeDistributed(Dense(dsize[2], activation='mish'))(x)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    x = TimeDistributed(Dense(dsize[3], activation='mish'))(x)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    out = TimeDistributed(Dense(dsize[4], activation='softmax', name='out'))(x)\n",
        "    \n",
        "    model = models.Model(inputs=inp, outputs=out) \n",
        "    \n",
        "    opt = tfa.optimizers.AdamW(learning_rate=LR, weight_decay=WD)\n",
        "    opt = tfa.optimizers.SWA(opt)\n",
        "    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n",
        "                  metrics=['accuracy', Precision(), Recall(), get_f1])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBAOXcJY7tOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ClassifierARF(shape_):        \n",
        "    # dsize = [256, 512, 256, 128, 11] # try 2: batchsize = 64, len = 1000\n",
        "    dsize = [128, 256, 128, 64, 11] # try3: batchsize = 32, len = 1000\n",
        "    # dsize = [32, 64, 32, 32, 11]  # try 1: batchsize=10, len=1000\n",
        "    nlayer = 2\n",
        "    inp = Input(shape=(shape_))\n",
        "    x = Bidirectional(GRU(dsize[0], return_sequences=True))(inp)\n",
        "    x = Attention(dsize[1], num_heads=4, attention_dropout=0.1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = TimeDistributed(Dense(dsize[2], activation='mish'))(x)\n",
        "    # x = SpatialDropout1D(0.2)(x)\n",
        "    # x = TimeDistributed(Dense(dsize[3], activation='mish'))(x)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    out = TimeDistributed(Dense(dsize[4], activation='softmax', name='out'))(x)\n",
        "    \n",
        "    model = models.Model(inputs=inp, outputs=out) \n",
        "    \n",
        "    opt = tfa.optimizers.AdamW(learning_rate=LR, weight_decay=WD)\n",
        "    opt = tfa.optimizers.SWA(opt)\n",
        "    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n",
        "                  metrics=['accuracy', Precision(), Recall(), get_f1])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F5IuBAI-iEzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ClassifierPD(shape_):\n",
        "    num_filters_ = 16\n",
        "    kernel_size_ = 3\n",
        "    stacked_layers_ = [12, 8, 4, 1]\n",
        "    l_input = Input(shape=(shape_))\n",
        "    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n",
        "    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n",
        "    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n",
        "    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n",
        "    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n",
        "    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n",
        "    DENSE_HIDDEN_UNITS = num_filters_*8\n",
        "    hidden = Add()([x, Dense(DENSE_HIDDEN_UNITS, activation='mish')(x)])\n",
        "    hidden = Add()([hidden, Dense(DENSE_HIDDEN_UNITS, activation='mish')(hidden)])\n",
        "    \n",
        "    x =  SpatialDropout1D(0.3)(hidden)\n",
        "    l_output = Dense(11, activation='softmax')(hidden)\n",
        "    model = models.Model(inputs=[l_input], outputs=[l_output])\n",
        "    opt = Adam(lr=LR)\n",
        "    opt = tfa.optimizers.SWA(opt)\n",
        "\n",
        "#      MacroF1(model, valid_x,valid_y)\n",
        "    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n",
        "                  metrics=['accuracy', Precision(), Recall(), get_f1])\n",
        "\n",
        "    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n",
        "    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n",
        "\n",
        "#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n",
        "#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLrqe5Fc7qpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.579491Z",
          "start_time": "2020-04-11T02:14:24.567764Z"
        },
        "trusted": true,
        "id": "L4cQ0L8ucOgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MacroF1ES(Callback):\n",
        "    def __init__(self, model, inputs, targets, \n",
        "                 patience=5, delta=0, checkpoint_path='checkpoint.h5', is_maximize=True):\n",
        "        \n",
        "        self.model = model\n",
        "        self.inputs = inputs\n",
        "        self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
        "        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n",
        "        self.counter, self.best_score = 0, None\n",
        "        self.is_maximize = is_maximize\n",
        "        self.stopped_epoch = 0\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
        "        score = f1_score(self.targets, pred, average=\"macro\")\n",
        "        logger.info(f'\\n epoch:{epoch:03d}, F1Macro: {score:.5f}')   \n",
        "        \n",
        "        if self.best_score is None or \\\n",
        "        (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n",
        "            self.model.save_weights(self.checkpoint_path)\n",
        "#             torch.save(model.state_dict(), self.checkpoint_path)\n",
        "            self.best_score, self.counter = score, 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience: ##stop training\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                \n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "              logger.info('Epoch %05d: early stopping' % (self.stopped_epoch + 1))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.589759Z",
          "start_time": "2020-04-11T02:14:24.582640Z"
        },
        "id": "1yfpldH_2ucw",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MacroF1(Callback):\n",
        "    def __init__(self, model, inputs, targets):\n",
        "        self.model = model\n",
        "        self.inputs = inputs\n",
        "        self.targets = np.argmax(targets, axis=2).reshape(-1)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n",
        "        score = f1_score(self.targets, pred, average=\"macro\")\n",
        "        print(f' F1Macro: {score:.5f}')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.596693Z",
          "start_time": "2020-04-11T02:14:24.591931Z"
        },
        "id": "skru4lPt2uc6",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(train, test):\n",
        "    \n",
        "    train_input_mean = train.signal.mean()\n",
        "    train_input_sigma = train.signal.std()\n",
        "    train['signal'] = (train.signal-train_input_mean)/train_input_sigma\n",
        "    test['signal'] = (test.signal-train_input_mean)/train_input_sigma\n",
        "\n",
        "    return train, test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:14:24.609602Z",
          "start_time": "2020-04-11T02:14:24.600166Z"
        },
        "id": "02TDJtejfS0_",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def run_everything(fe_config : List) -> NoReturn:\n",
        "    not_feats_cols = ['time']\n",
        "    target_col = ['open_channels']\n",
        "    init_logger()\n",
        "    with timer(f'Reading Data'):\n",
        "        logger.info('Reading Data Started ...')\n",
        "        base = os.path.abspath(PATH+'liverpool-ion-switching/')\n",
        "        train, test, sample_submission = read_data(base)\n",
        "        train, test = normalize(train, test)    \n",
        "        logger.info('Reading and Normalizing Data Completed ...')\n",
        "    with timer(f'Creating Features'):\n",
        "        logger.info('Feature Enginnering Started ...')\n",
        "        for config in fe_config:\n",
        "            train = run_feat_enginnering(train, create_all_data_feats=config[0], batch_size=config[1])\n",
        "            train = reduce_mem_usage(train)\n",
        "            test  = run_feat_enginnering(test,  create_all_data_feats=config[0], batch_size=config[1])\n",
        "            test = reduce_mem_usage(test)\n",
        "        train, test, feats = feature_selection(train, test)\n",
        "        logger.info('Feature Enginnering Completed ...')\n",
        "\n",
        "    with timer(f'Running Wavenet model'):\n",
        "        logger.info(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started ...')\n",
        "        run_cv_model_by_batch(train, test, splits=SPLITS, batch_col='group', feats=feats, \n",
        "                              sample_submission=sample_submission, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n",
        "        logger.info(f'Training completed ...')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-11T02:15:53.901793Z",
          "start_time": "2020-04-11T02:14:24.612467Z"
        },
        "id": "_w4_OJJtfS1K",
        "trusted": true,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_everything(fe_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CMgZCYC5Yh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjHu5CkUrorQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbDSqah3O6Q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIkAcnCRRN-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}