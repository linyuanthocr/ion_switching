{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wavenetaug.ipynb","provenance":[{"file_id":"1NF0qCQjMoILzlhlwdhKOvHicu35-x10V","timestamp":1589409362301},{"file_id":"1ozHf8vFMp67-DwQYrECBbZvLH9iDROuk","timestamp":1589238924723},{"file_id":"1Z-r7RvJVlH8UiWrYAMFrPAkJDu-RVgu9","timestamp":1589152984298},{"file_id":"1eLltIeX2TxKSQPrQKXLgImtcH35NMl37","timestamp":1588968304965},{"file_id":"1j5uOsr_vIOinvEsNvGuxpRA7aATgfPHD","timestamp":1588814277869},{"file_id":"1AI_WQ0xVmMSrBNRfCEAz08SnkeWyVMm_","timestamp":1588563626234}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"huixWaL42uZi","colab_type":"text"},"source":["The validation scheme is based on [seq2seq-rnn-with-gru](https://www.kaggle.com/brandenkmurray/seq2seq-rnn-with-gru/output), and cleaned data is from [data-without-drift](https://www.kaggle.com/cdeotte/data-without-drift) and Kalman filter is from [https://www.kaggle.com/teejmahal20/single-model-lgbm-kalman-filter](single-model-lgbm-kalman-filter) and the added feature is from [wavenet-with-1-more-feature](wavenet-with-1-more-feature). I also used ragnar's data in this version [clean-kalman](https://www.kaggle.com/ragnar123/clean-kalman). The Wavenet is based on [https://github.com/philipperemy/keras-tcn](https://github.com/philipperemy/keras-tcn), [https://github.com/peustr/wavenet](https://github.com/peustr/wavenet) and [https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet) and also [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm). If any refrence is not mentioned it was not intentional, please add them in comments.\n","\n","Previous versions were mainly based on [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm)  "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.121605Z","start_time":"2020-04-11T02:14:22.792317Z"},"_kg_hide-input":true,"id":"LqmWjeYJ2uZn","trusted":true,"colab_type":"code","colab":{}},"source":["!pip install --no-warn-conflicts -q tensorflow-addons"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DbSxqaBWiqQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.152214Z","start_time":"2020-04-11T02:14:24.125295Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"y1qOuodBfSxN","trusted":true,"colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import (TimeDistributed, Dropout, BatchNormalization, Flatten, Convolution1D, Activation, Input, Dense, LSTM, Lambda, Bidirectional,\n","                                     Add, AveragePooling1D, Multiply, GRU, GRUCell, LSTMCell, SimpleRNNCell, SimpleRNN, TimeDistributed, RNN,SpatialDropout1D,\n","                                     RepeatVector, Conv1D, MaxPooling1D, GlobalMaxPooling1D,Concatenate, GlobalAveragePooling1D, UpSampling1D, Reshape)\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler, CSVLogger\n","from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy, mean_squared_error\n","# from tensorflow.keras.experimental import export_saved_model, load_from_saved_model\n","from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n","from tensorflow.keras.utils import Sequence, to_categorical\n","from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n","from tensorflow.keras import losses, models, optimizers\n","from tensorflow.keras import backend as K\n","from tensorflow.python.ops import array_ops\n","import tensorflow as tf\n","from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n","from sklearn.metrics import f1_score, cohen_kappa_score, mean_squared_error\n","from logging import getLogger, Formatter, StreamHandler, FileHandler, INFO, DEBUG, NOTSET\n","from sklearn.model_selection import KFold, GroupKFold\n","from tqdm import tqdm_notebook as tqdm\n","from contextlib import contextmanager\n","from joblib import Parallel, delayed\n","from IPython.display import display\n","from sklearn import preprocessing\n","from sklearn.utils import class_weight\n","import tensorflow_addons as tfa\n","import scipy.stats as stats\n","import random as rn\n","import pandas as pd\n","import numpy as np\n","import scipy as sp\n","import itertools\n","import warnings\n","import time\n","import pywt\n","import os\n","import gc\n","\n","from tensorflow.keras.metrics import Precision, Recall\n","# from tensorflow_addons.metrics import F1Score\n","\n","warnings.simplefilter('ignore')\n","warnings.filterwarnings('ignore')\n","pd.set_option('display.max_columns', 1000)\n","pd.set_option('display.max_rows', 500)\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"AVvcAEP0iEyp","colab_type":"code","colab":{}},"source":["Kaggle = False\n","Colab = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LC8XqE14cSRm","trusted":true,"colab_type":"code","outputId":"4fa89e19-dfd4-4518-dfcb-33cecc8832e1","executionInfo":{"status":"ok","timestamp":1589423487862,"user_tz":420,"elapsed":5150,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os, sys\n","from pathlib import Path\n","\n","if Colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    path = \"/content/drive/My Drive\"\n","\n","    os.chdir(path)\n","    os.listdir(path)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z0fJcaGLoPY3","trusted":true,"colab_type":"code","colab":{}},"source":["# a = np.random.rand(2,4,3)\n","# print(a)\n","# b = np.reshape(a,(4,2,3))\n","# print(b)\n","# c = np.reshape(b, (2,4,3))\n","# print(c)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-sYfdXdWyTI","trusted":true,"colab_type":"code","colab":{}},"source":["# sys.path.append('ion_switch/keras-one-cycle')\n","# # os.listdir(patholr)\n","# from clr import OneCycleLR"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.159766Z","start_time":"2020-04-11T02:14:24.155918Z"},"trusted":true,"id":"zdosugVWcOf0","colab_type":"code","colab":{}},"source":["if Kaggle:\n","    PATH = '/kaggle/input/'\n","    outdir = '.'\n","# PATH = '/Users/helen/Desktop/Data/'\n","else:\n","    PATH = 'ion_switch/'\n","    outdir = Path(PATH+'res')\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)\n","    outdir = Path(PATH+'res/wavenet-dlr-res')\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.168634Z","start_time":"2020-04-11T02:14:24.163612Z"},"id":"UafJMtyefSxU","trusted":true,"colab_type":"code","colab":{}},"source":["EPOCHS=180\n","NNBATCHSIZE= 16\n","BATCHSIZE = 4000\n","FEASIZE = 21\n","SEED = 321\n","SELECT = True\n","SPLITS = 5\n","LR = 0.001\n","WD = 1e-5\n","Gamma = 1.0 #0.99994\n","BETA = 0.99996\n","fe_config = [\n","    (True, BATCHSIZE),\n","]\n","TREES = 100\n","DEPTH = 12\n","COMPETITION = 'ION-Switching'\n","logger = getLogger(COMPETITION)\n","LOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\n","MODELNAME = 'ClassifierCBRWaug-Prob11-NESCAL'\n","weights = []\n","# TRAINEDMODEL = os.path.join(outdir, 'wavenet-es-v1/wavenet_es_f0_checkpoint.h5')\n","\n","VERSION = '{}'.format(MODELNAME)\n","outdir = os.path.join(outdir, VERSION)\n","if not os.path.exists(outdir):\n","    os.mkdir(outdir)\n","\n","from datetime import datetime\n","dateTimeObj = datetime.now()\n","# timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H\")\n","timestampStr = 'all'\n","outdir = os.path.join(outdir, timestampStr)\n","if not os.path.exists(outdir):\n","    os.mkdir(outdir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fouKVpNaFNKQ","trusted":true,"colab_type":"code","colab":{}},"source":["\n","@contextmanager\n","def timer(name : Text):\n","    t0 = time.time()\n","    yield\n","    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.177661Z","start_time":"2020-04-11T02:14:24.171732Z"},"id":"EE4v8h1tfSxb","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def init_logger():\n","\n","    handler = StreamHandler()\n","    handler.setLevel(INFO)\n","    handler.setFormatter(Formatter(LOGFORMAT))\n","    fh_handler = FileHandler(os.path.join(outdir,'{}-len{}-lr{}-{}-bf1pre.log'.format(MODELNAME,BATCHSIZE,LR,timestampStr)))\n","    fh_handler.setFormatter(Formatter(LOGFORMAT))\n","    logger.setLevel(INFO)\n","    logger.addHandler(handler)\n","    logger.addHandler(fh_handler)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.358957Z","start_time":"2020-04-11T02:14:24.187096Z"},"id":"OC5DOcDifSxx","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def seed_everything(seed : int) -> NoReturn :\n","    \n","    rn.seed(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    tf.random.set_seed(seed)\n","    # os.environ['TF_CUDNN_DETERMINISTIC'] = str(seed) \n","\n","seed_everything(SEED)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5tInVDxBn-lQ","trusted":true,"colab_type":"code","colab":{}},"source":["class CyclicLR(tf.keras.callbacks.Callback):\n","\n","    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n","                 gamma=1., scale_fn=None, scale_mode='cycle'):\n","        super(CyclicLR, self).__init__()\n","\n","        self.base_lr = base_lr\n","        self.max_lr = max_lr\n","        self.step_size = step_size\n","        self.mode = mode\n","        self.gamma = gamma\n","        if scale_fn == None:\n","            if self.mode == 'triangular':\n","                self.scale_fn = lambda x: 1.\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'triangular2':\n","                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'exp_range':\n","                self.scale_fn = lambda x: gamma ** (x)\n","                self.scale_mode = 'iterations'\n","        else:\n","            self.scale_fn = scale_fn\n","            self.scale_mode = scale_mode\n","        self.clr_iterations = 0.\n","        self.trn_iterations = 0.\n","        self.history = {}\n","\n","        self._reset()\n","\n","    def _reset(self, new_base_lr=None, new_max_lr=None,\n","               new_step_size=None):\n","        \"\"\"Resets cycle iterations.\n","        Optional boundary/step size adjustment.\n","        \"\"\"\n","        if new_base_lr != None:\n","            self.base_lr = new_base_lr\n","        if new_max_lr != None:\n","            self.max_lr = new_max_lr\n","        if new_step_size != None:\n","            self.step_size = new_step_size\n","        self.clr_iterations = 0.\n","\n","    def clr(self):\n","        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n","        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n","        if self.scale_mode == 'cycle':\n","            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n","        else:\n","            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n","                self.clr_iterations)\n","\n","    def on_train_begin(self, logs={}):\n","        logs = logs or {}\n","\n","        if self.clr_iterations == 0:\n","            K.set_value(self.model.optimizer.lr, self.base_lr)\n","        else:\n","            K.set_value(self.model.optimizer.lr, self.clr())\n","\n","    def on_batch_end(self, epoch, logs=None):\n","\n","        logs = logs or {}\n","        self.trn_iterations += 1\n","        self.clr_iterations += 1\n","\n","        K.set_value(self.model.optimizer.lr, self.clr())\n","        # print(\"learning rate- self.model.optimizer.lr: \", self.model.optimizer.lr)\n","\n","    # def on_epoch_end(self, epoch, logs=None):\n","\n","    #     logs = logs or {}\n","    #     self.trn_iterations += 1\n","    #     self.clr_iterations += 1\n","\n","    #     K.set_value(self.model.optimizer.lr, self.clr())\n","    #     logger.info(f'epoch:{epoch:03d},'+str(K.eval(self.model.optimizer.lr)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmKgNg1Mmzrk","trusted":true,"colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","# 写一个LossHistory类，保存训练集的loss和acc\n","# 当然我也可以完全不这么做，可以直接使用model.fit()方法返回的 history对象去做\n","'''Callback有6个常用的方法，这里实现其中的四个\n","    def on_epoch_begin(self, epoch, logs=None):\n","    def on_epoch_end(self, epoch, logs=None):\n","    def on_batch_begin(self, batch, logs=None):\n","    def on_batch_end(self, batch, logs=None):\n","    def on_train_begin(self, logs=None):\n","    def on_train_end(self, logs=None):\n","'''\n","class LossHistory(Callback):  # 继承自Callback类\n"," \n","    '''\n","    在模型开始的时候定义四个属性，每一个属性都是字典类型，存储相对应的值和epoch\n","    '''\n","    def on_train_begin(self, logs={}):\n","        self.losses = {'batch':[], 'epoch':[]}\n","        self.accuracy = {'batch':[], 'epoch':[]}\n","        self.val_loss = {'batch':[], 'epoch':[]}\n","        self.val_acc = {'batch':[], 'epoch':[]}\n"," \n","    # 在每一个batch结束后记录相应的值\n","    def on_batch_end(self, batch, logs={}):\n","        self.losses['batch'].append(logs.get('loss'))\n","        self.accuracy['batch'].append(logs.get('accuracy'))\n","        self.val_loss['batch'].append(logs.get('val_loss'))\n","        self.val_acc['batch'].append(logs.get('val_accuracy'))\n","    \n","    # 在每一个epoch之后记录相应的值\n","    def on_epoch_end(self, epoch, logs={}):\n","        trloss, tracc, vloss, vacc = logs.get('loss'), logs.get('accuracy'), logs.get('val_loss'), logs.get('val_accuracy')\n","        self.losses['epoch'].append(trloss)\n","        self.accuracy['epoch'].append(tracc)\n","        self.val_loss['epoch'].append(vloss)\n","        self.val_acc['epoch'].append(vacc)\n","        logger.info(\"epoch:{:03d}, train_loss:{:1.5f}, train_acc:{:1.5f}, val_loss:{:1.5f}, val_acc:{:1.5f}\".format(epoch, \n","                                                                                                                trloss, tracc, vloss, vacc))\n"," \n","    def loss_plot(self, loss_type, pngname):\n","        '''\n","        loss_type：指的是 'epoch'或者是'batch'，分别表示是一个batch之后记录还是一个epoch之后记录\n","        '''\n","        iters = range(len(self.losses[loss_type]))\n","        plt.figure()\n","        # acc\n","        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n","        # loss\n","        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n","        if loss_type == 'epoch':\n","            # val_acc\n","            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n","            # val_loss\n","            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n","        plt.grid(True)\n","        plt.xlabel(loss_type)\n","        plt.ylabel('acc-loss')\n","        plt.legend(loc=\"upper right\")\n","        plt.savefig(pngname)\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.368249Z","start_time":"2020-04-11T02:14:24.362616Z"},"id":"adUHGQUTfSyA","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def read_data(base : os.path.abspath) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n","    \n","    train = pd.read_csv(PATH+'clean-kalman/train_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n","    test  = pd.read_csv(PATH+'clean-kalman/test_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32})\n","    sub  = pd.read_csv(PATH+'liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n","    \n","    return train, test, sub\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.376185Z","start_time":"2020-04-11T02:14:24.371687Z"},"id":"HpDaJQ5yfSyI","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def batching(df : pd.DataFrame,\n","             batch_size : int) -> pd.DataFrame :\n","    \n","    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n","    df['group'] = df['group'].astype(np.uint16)\n","        \n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.391087Z","start_time":"2020-04-11T02:14:24.378989Z"},"id":"iQxOYF3tfSyj","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def reduce_mem_usage(df: pd.DataFrame,\n","                     verbose: bool = True) -> pd.DataFrame:\n","    \n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2\n","\n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","\n","            if str(col_type)[:3] == 'int':\n","\n","                if (c_min > np.iinfo(np.int32).min\n","                      and c_max < np.iinfo(np.int32).max):\n","                    df[col] = df[col].astype(np.int32)\n","                elif (c_min > np.iinfo(np.int64).min\n","                      and c_max < np.iinfo(np.int64).max):\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                if (c_min > np.finfo(np.float16).min\n","                        and c_max < np.finfo(np.float16).max):\n","                    df[col] = df[col].astype(np.float16)\n","                elif (c_min > np.finfo(np.float32).min\n","                      and c_max < np.finfo(np.float32).max):\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    reduction = (start_mem - end_mem) / start_mem\n","\n","    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n","    if verbose:\n","        print(msg)\n","\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.402186Z","start_time":"2020-04-11T02:14:24.393787Z"},"id":"dytmNmL_fSzj","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def lag_with_pct_change(df : pd.DataFrame,\n","                        shift_sizes : Optional[List]=[1, 2],\n","                        add_pct_change : Optional[bool]=False,\n","                        add_pct_change_lag : Optional[bool]=False,\n","                        add_diff : Optional[bool]=False) -> pd.DataFrame:\n","    \n","    for shift_size in shift_sizes:    \n","        df['signal_shift_pos_'+str(shift_size)] = df.groupby('group')['signal'].shift(shift_size).fillna(0)\n","        df['signal_shift_neg_'+str(shift_size)] = df.groupby('group')['signal'].shift(-1*shift_size).fillna(0)\n","\n","    if add_pct_change:\n","        df['pct_change'] = df['signal'].pct_change()\n","        if add_pct_change_lag:\n","            for shift_size in shift_sizes:    \n","                df['pct_change_shift_pos_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(shift_size).fillna(0)\n","                df['pct_change_shift_neg_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(-1*shift_size).fillna(0)\n","    if add_diff:\n","        for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'group', 'index']]:\n","            df[c+'_msignal'] = df[c] - df['signal']\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.409381Z","start_time":"2020-04-11T02:14:24.404800Z"},"id":"m-ULWLF_fS0B","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def run_feat_enginnering(df : pd.DataFrame,\n","                         create_all_data_feats : bool,\n","                         batch_size : int) -> pd.DataFrame:\n","    \n","    df = batching(df, batch_size=batch_size)\n","    if create_all_data_feats:\n","        df = lag_with_pct_change(df, [1, 2, 3],  add_pct_change=False, add_pct_change_lag=False, add_diff=False)\n","    df['signal_2'] = df['signal'] ** 2\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.419991Z","start_time":"2020-04-11T02:14:24.412368Z"},"id":"6E87jcu2fS0O","trusted":true,"colab_type":"code","colab":{}},"source":["def feature_selection(df : pd.DataFrame,\n","                      df_test : pd.DataFrame) -> Tuple[pd.DataFrame , pd.DataFrame, List]:\n","    use_cols = [col for col in df.columns if col not in ['index','group', 'open_channels', 'time']]\n","    print(use_cols)\n","    df = df.replace([np.inf, -np.inf], np.nan)\n","    df_test = df_test.replace([np.inf, -np.inf], np.nan)\n","    for col in use_cols:\n","        col_mean = pd.concat([df[col], df_test[col]], axis=0).mean()\n","        df[col] = df[col].fillna(col_mean)\n","        df_test[col] = df_test[col].fillna(col_mean)\n","   \n","    gc.collect()\n","    return df, df_test, use_cols\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"41LbkJYqxJlV","trusted":true,"colab_type":"code","colab":{}},"source":["def jitter(x, sigma=0.03):\n","    # print(\"jitter x shape:\", x.shape)\n","    # https://arxiv.org/pdf/1706.00527.pdf\n","    return x + np.random.normal(loc=0., scale=sigma, size=x.shape)\n","\n","def scaling(x, sigma=0.1):\n","    # https://arxiv.org/pdf/1706.00527.pdf\n","    factor = np.random.normal(loc=1., scale=sigma, size=(x.shape[0],x.shape[2]))\n","    return np.multiply(x, factor[:,np.newaxis,:])\n","\n","def rotation(x):\n","    flip = np.random.choice([-1, 1], size=(x.shape[0],x.shape[2]))\n","    rotate_axis = np.arange(x.shape[2])\n","    np.random.shuffle(rotate_axis)    \n","    return flip[:,np.newaxis,:] * x[:,:,rotate_axis]\n","\n","def rotation2d(x, sigma=0.2):\n","    thetas = np.random.normal(loc=0, scale=sigma, size=(x.shape[0]))\n","    c = np.cos(thetas)\n","    s = np.sin(thetas)\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        rot = np.array(((c[i], -s[i]), (s[i], c[i])))\n","        ret[i] = np.dot(pat, rot)\n","    return ret\n","\n","def permutation(x, max_segments=5, seg_mode=\"equal\"):\n","    orig_steps = np.arange(x.shape[1])\n","    \n","    num_segs = np.random.randint(1, max_segments, size=(x.shape[0]))\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        if num_segs[i] > 1:\n","            if seg_mode == \"random\":\n","                split_points = np.random.choice(x.shape[1]-2, num_segs[i]-1, replace=False)\n","                split_points.sort()\n","                splits = np.split(orig_steps, split_points)\n","            else:\n","                splits = np.array_split(orig_steps, num_segs[i])\n","            warp = np.concatenate(np.random.permutation(splits)).ravel()\n","            ret[i] = pat[warp]\n","        else:\n","            ret[i] = pat\n","    return ret\n","\n","def magnitude_warp(x, sigma=0.2, knot=4):\n","    from scipy.interpolate import CubicSpline\n","    orig_steps = np.arange(x.shape[1])\n","    \n","    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n","    warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        warper = np.array([CubicSpline(warp_steps[:,dim], random_warps[i,:,dim])(orig_steps) for dim in range(x.shape[2])]).T\n","        ret[i] = pat * warper\n","\n","    return ret\n","\n","def time_warp(x, sigma=0.2, knot=4):\n","    from scipy.interpolate import CubicSpline\n","    orig_steps = np.arange(x.shape[1])\n","    \n","    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(x.shape[0], knot+2, x.shape[2]))\n","    warp_steps = (np.ones((x.shape[2],1))*(np.linspace(0, x.shape[1]-1., num=knot+2))).T\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        for dim in range(x.shape[2]):\n","            time_warp = CubicSpline(warp_steps[:,dim], warp_steps[:,dim] * random_warps[i,:,dim])(orig_steps)\n","            scale = (x.shape[1]-1)/time_warp[-1]\n","            ret[i,:,dim] = np.interp(orig_steps, np.clip(scale*time_warp, 0, x.shape[1]-1), pat[:,dim]).T\n","    return ret\n","\n","def window_slice(x, reduce_ratio=0.9):\n","    # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n","    target_len = np.ceil(reduce_ratio*x.shape[1]).astype(int)\n","    if target_len >= x.shape[1]:\n","        return x\n","    starts = np.random.randint(low=0, high=x.shape[1]-target_len, size=(x.shape[0])).astype(int)\n","    ends = (target_len + starts).astype(int)\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        for dim in range(x.shape[2]):\n","            ret[i,:,dim] = np.interp(np.linspace(0, target_len, num=x.shape[1]), np.arange(target_len), pat[starts[i]:ends[i],dim]).T\n","    return ret\n","\n","def window_warp(x, window_ratio=0.1, scales=[0.5, 2.]):\n","    # https://halshs.archives-ouvertes.fr/halshs-01357973/document\n","    warp_scales = np.random.choice(scales, x.shape[0])\n","    warp_size = np.ceil(window_ratio*x.shape[1]).astype(int)\n","    window_steps = np.arange(warp_size)\n","        \n","    window_starts = np.random.randint(low=1, high=x.shape[1]-warp_size-1, size=(x.shape[0])).astype(int)\n","    window_ends = (window_starts + warp_size).astype(int)\n","            \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(x):\n","        for dim in range(x.shape[2]):\n","            start_seg = pat[:window_starts[i],dim]\n","            window_seg = np.interp(np.linspace(0, warp_size-1, num=int(warp_size*warp_scales[i])), window_steps, pat[window_starts[i]:window_ends[i],dim])\n","            end_seg = pat[window_ends[i]:,dim]\n","            warped = np.concatenate((start_seg, window_seg, end_seg))                \n","            ret[i,:,dim] = np.interp(np.arange(x.shape[1]), np.linspace(0, x.shape[1]-1., num=warped.size), warped).T\n","    return ret\n","\n","def spawner(x, labels, sigma=0.05, verbose=0):\n","    # https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6983028/\n","    \n","    import utils.dtw as dtw\n","    random_points = np.random.randint(low=1, high=x.shape[1]-1, size=x.shape[0])\n","    window = np.ceil(x.shape[1] / 10.).astype(int)\n","    orig_steps = np.arange(x.shape[1])\n","    l = np.argmax(labels, axis=1) if labels.ndim > 1 else labels\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(tqdm(x)):\n","        # guarentees that same one isnt selected\n","        choices = np.delete(np.arange(x.shape[0]), i)\n","        # remove ones of different classes\n","        choices = np.where(l[choices] == l[i])[0]\n","        if choices.size > 0:     \n","            random_sample = x[np.random.choice(choices)]\n","            # SPAWNER splits the path into two randomly\n","            path1 = dtw.dtw(pat[:random_points[i]], random_sample[:random_points[i]], dtw.RETURN_PATH, slope_constraint=\"symmetric\", window=window)\n","            path2 = dtw.dtw(pat[random_points[i]:], random_sample[random_points[i]:], dtw.RETURN_PATH, slope_constraint=\"symmetric\", window=window)\n","            combined = np.concatenate((np.vstack(path1), np.vstack(path2+random_points[i])), axis=1)\n","            if verbose:\n","                print(random_points[i])\n","                dtw_value, cost, DTW_map, path = dtw.dtw(pat, random_sample, return_flag = dtw.RETURN_ALL, slope_constraint=slope_constraint, window=window)\n","                dtw.draw_graph1d(cost, DTW_map, path, pat, random_sample)\n","                dtw.draw_graph1d(cost, DTW_map, combined, pat, random_sample)\n","            mean = np.mean([pat[combined[0]], random_sample[combined[1]]], axis=0)\n","            for dim in range(x.shape[2]):\n","                ret[i,:,dim] = np.interp(orig_steps, np.linspace(0, x.shape[1]-1., num=mean.shape[0]), mean[:,dim]).T\n","        else:\n","            print(\"There is only one pattern of class %d, skipping pattern average\"%l[i])\n","            ret[i,:] = pat\n","    return jitter(ret, sigma=sigma)\n","\n","def wdba(x, labels, batch_size=6, slope_constraint=\"symmetric\", use_window=True):\n","    # https://ieeexplore.ieee.org/document/8215569\n","    \n","    import utils.dtw as dtw\n","    \n","    if use_window:\n","        window = np.ceil(x.shape[1] / 10.).astype(int)\n","    else:\n","        window = None\n","    orig_steps = np.arange(x.shape[1])\n","    l = np.argmax(labels, axis=1) if labels.ndim > 1 else labels\n","        \n","    ret = np.zeros_like(x)\n","    for i in tqdm(range(ret.shape[0])):\n","        # get the same class as i\n","        choices = np.where(l == l[i])[0]\n","        if choices.size > 0:        \n","            # pick random intra-class pattern\n","            k = min(choices.size, batch_size)\n","            random_prototypes = x[np.random.choice(choices, k, replace=False)]\n","            \n","            # calculate dtw between all\n","            dtw_matrix = np.zeros((k, k))\n","            for p, prototype in enumerate(random_prototypes):\n","                for s, sample in enumerate(random_prototypes):\n","                    if p == s:\n","                        dtw_matrix[p, s] = 0.\n","                    else:\n","                        dtw_matrix[p, s] = dtw.dtw(prototype, sample, dtw.RETURN_VALUE, slope_constraint=slope_constraint, window=window)\n","                        \n","            # get medoid\n","            medoid_id = np.argsort(np.sum(dtw_matrix, axis=1))[0]\n","            nearest_order = np.argsort(dtw_matrix[medoid_id])\n","            medoid_pattern = random_prototypes[medoid_id]\n","            \n","            # start weighted DBA\n","            average_pattern = np.zeros_like(medoid_pattern)\n","            weighted_sums = np.zeros((medoid_pattern.shape[0]))\n","            for nid in nearest_order:\n","                if nid == medoid_id or dtw_matrix[medoid_id, nearest_order[1]] == 0.:\n","                    average_pattern += medoid_pattern \n","                    weighted_sums += np.ones_like(weighted_sums) \n","                else:\n","                    path = dtw.dtw(medoid_pattern, random_prototypes[nid], dtw.RETURN_PATH, slope_constraint=slope_constraint, window=window)\n","                    dtw_value = dtw_matrix[medoid_id, nid]\n","                    warped = random_prototypes[nid, path[1]]\n","                    weight = np.exp(np.log(0.5)*dtw_value/dtw_matrix[medoid_id, nearest_order[1]])\n","                    average_pattern[path[0]] += weight * warped\n","                    weighted_sums[path[0]] += weight \n","            \n","            ret[i,:] = average_pattern / weighted_sums[:,np.newaxis]\n","        else:\n","            print(\"There is only one pattern of class %d, skipping pattern average\"%l[i])\n","            ret[i,:] = x[i]\n","    return ret\n","\n","# Proposed\n","\n","def random_guided_warp(x, labels, slope_constraint=\"symmetric\", use_window=True, dtw_type=\"normal\"):\n","    import utils.dtw as dtw\n","    \n","    if use_window:\n","        window = np.ceil(x.shape[1] / 10.).astype(int)\n","    else:\n","        window = None\n","    orig_steps = np.arange(x.shape[1])\n","    l = np.argmax(labels, axis=1) if labels.ndim > 1 else labels\n","    \n","    ret = np.zeros_like(x)\n","    for i, pat in enumerate(tqdm(x)):\n","        # guarentees that same one isnt selected\n","        choices = np.delete(np.arange(x.shape[0]), i)\n","        # remove ones of different classes\n","        choices = np.where(l[choices] == l[i])[0]\n","        if choices.size > 0:        \n","            # pick random intra-class pattern\n","            random_prototype = x[np.random.choice(choices)]\n","            \n","            if dtw_type == \"shape\":\n","                path = dtw.shape_dtw(random_prototype, pat, dtw.RETURN_PATH, slope_constraint=slope_constraint, window=window)\n","            else:\n","                path = dtw.dtw(random_prototype, pat, dtw.RETURN_PATH, slope_constraint=slope_constraint, window=window)\n","                            \n","            # Time warp\n","            warped = pat[path[1]]\n","            for dim in range(x.shape[2]):\n","                ret[i,:,dim] = np.interp(orig_steps, np.linspace(0, x.shape[1]-1., num=warped.shape[0]), warped[:,dim]).T\n","        else:\n","            print(\"There is only one pattern of class %d, skipping timewarping\"%l[i])\n","            ret[i,:] = pat\n","    return ret\n","\n","def discriminative_guided_warp(x, labels, batch_size=6, slope_constraint=\"symmetric\", use_window=True, dtw_type=\"normal\", use_variable_slice=True):\n","    import utils.dtw as dtw\n","    \n","    if use_window:\n","        window = np.ceil(x.shape[1] / 10.).astype(int)\n","    else:\n","        window = None\n","    orig_steps = np.arange(x.shape[1])\n","    l = np.argmax(labels, axis=1) if labels.ndim > 1 else labels\n","    \n","    positive_batch = np.ceil(batch_size / 2).astype(int)\n","    negative_batch = np.floor(batch_size / 2).astype(int)\n","        \n","    ret = np.zeros_like(x)\n","    warp_amount = np.zeros(x.shape[0])\n","    for i, pat in enumerate(tqdm(x)):\n","        # guarentees that same one isnt selected\n","        choices = np.delete(np.arange(x.shape[0]), i)\n","        \n","        # remove ones of different classes\n","        positive = np.where(l[choices] == l[i])[0]\n","        negative = np.where(l[choices] != l[i])[0]\n","        \n","        if positive.size > 0 and negative.size > 0:\n","            pos_k = min(positive.size, positive_batch)\n","            neg_k = min(negative.size, negative_batch)\n","            positive_prototypes = x[np.random.choice(positive, pos_k, replace=False)]\n","            negative_prototypes = x[np.random.choice(negative, neg_k, replace=False)]\n","                        \n","            # vector embedding and nearest prototype in one\n","            pos_aves = np.zeros((pos_k))\n","            neg_aves = np.zeros((pos_k))\n","            if dtw_type == \"shape\":\n","                for p, pos_prot in enumerate(positive_prototypes):\n","                    for ps, pos_samp in enumerate(positive_prototypes):\n","                        if p != ps:\n","                            pos_aves[p] += (1./(pos_k-1.))*dtw.shape_dtw(pos_prot, pos_samp, dtw.RETURN_VALUE, slope_constraint=slope_constraint, window=window)\n","                    for ns, neg_samp in enumerate(negative_prototypes):\n","                        neg_aves[p] += (1./neg_k)*dtw.shape_dtw(pos_prot, neg_samp, dtw.RETURN_VALUE, slope_constraint=slope_constraint, window=window)\n","                selected_id = np.argmax(neg_aves - pos_aves)\n","                path = dtw.shape_dtw(positive_prototypes[selected_id], pat, dtw.RETURN_PATH, slope_constraint=slope_constraint, window=window)\n","            else:\n","                for p, pos_prot in enumerate(positive_prototypes):\n","                    for ps, pos_samp in enumerate(positive_prototypes):\n","                        if p != ps:\n","                            pos_aves[p] += (1./(pos_k-1.))*dtw.dtw(pos_prot, pos_samp, dtw.RETURN_VALUE, slope_constraint=slope_constraint, window=window)\n","                    for ns, neg_samp in enumerate(negative_prototypes):\n","                        neg_aves[p] += (1./neg_k)*dtw.dtw(pos_prot, neg_samp, dtw.RETURN_VALUE, slope_constraint=slope_constraint, window=window)\n","                selected_id = np.argmax(neg_aves - pos_aves)\n","                path = dtw.dtw(positive_prototypes[selected_id], pat, dtw.RETURN_PATH, slope_constraint=slope_constraint, window=window)\n","                   \n","            # Time warp\n","            warped = pat[path[1]]\n","            warp_path_interp = np.interp(orig_steps, np.linspace(0, x.shape[1]-1., num=warped.shape[0]), path[1])\n","            warp_amount[i] = np.sum(np.abs(orig_steps-warp_path_interp))\n","            for dim in range(x.shape[2]):\n","                ret[i,:,dim] = np.interp(orig_steps, np.linspace(0, x.shape[1]-1., num=warped.shape[0]), warped[:,dim]).T\n","        else:\n","            print(\"There is only one pattern of class %d\"%l[i])\n","            ret[i,:] = pat\n","            warp_amount[i] = 0.\n","    if use_variable_slice:\n","        max_warp = np.max(warp_amount)\n","        if max_warp == 0:\n","            # unchanged\n","            ret = window_slice(ret, reduce_ratio=0.95)\n","        else:\n","            for i, pat in enumerate(ret):\n","                # Variable Sllicing\n","                ret[i] = window_slice(pat[np.newaxis,:,:], reduce_ratio=0.95+0.05*warp_amount[i]/max_warp)[0]\n","    return ret"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9yEnd2lT4RQb","trusted":true,"colab_type":"code","colab":{}},"source":["def feature_roll_diff(x):\n","    for i in range(3):\n","      pos_x = np.roll(x[:,:,0][:,:,None], i)\n","      neg_x = np.roll(x[:,:,0][:,:,None], -i)\n","      # print(\"pos_x.shape\", pos_x.shape)\n","      x =  np.concatenate((x, pos_x),axis = 2)\n","      x =  np.concatenate((x, neg_x),axis = 2)\n","      # print(\"x.shape 1 \", x.shape)\n","      x =  np.concatenate((x, pos_x - x[:,:,0][:,:,None]),axis = 2) \n","      x =  np.concatenate((x, neg_x - x[:,:,0][:,:,None]),axis = 2)\n","      # print(\"x.shape 2 \", x.shape)\n","    x =  np.concatenate((x, x[:,:,0][:,:,None]**2),axis = 2)\n","      # print(\"x.shape 3 \", x.shape) \n","    return x       "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AdlC7eijQ1TI","trusted":true,"colab_type":"code","colab":{}},"source":["# @tf.function(input_signature=[tf.TensorSpec(None, tf.float32)]) \n","# def tf_feature_roll_diff(input): \n","#   y = tf.numpy_function(feature_roll_diff, [input], tf.float32) \n","#   return y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.427483Z","start_time":"2020-04-11T02:14:24.422535Z"},"trusted":true,"id":"Ft7_2jVZcOgQ","colab_type":"code","colab":{}},"source":["\n","def augment(X: np.array, y:np.array) -> Tuple[np.array, np.array]:\n","    X_aug = np.flip(X, axis=1)\n","    # print(\"X_aug shape:\", X_aug.shape)\n","    X = np.vstack((X, X_aug))\n","\n","    y = np.vstack((y, np.flip(y, axis=1)))\n","    \n","    return X, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVJepJOdUGqK","colab_type":"code","colab":{}},"source":["def augment_label(X: np.array, y:np.array, Xtest : np.array, Xval : np.array) -> Tuple[np.array, np.array, np.array, np.array]:\n","    X_aug = np.flip(X, axis=1)\n","    print(\"X_aug shape 1:\", X_aug.shape)\n","    X = np.concatenate((X, np.zeros_like(X[:,:,0][:,:,np.newaxis],dtype=np.uint8)),axis = 2)\n","    Xtest = np.concatenate((Xtest, np.zeros_like(Xtest[:,:,0][:,:,np.newaxis],dtype=np.uint8)),axis = 2)\n","    Xval = np.concatenate((Xval, np.zeros_like(Xval[:,:,0][:,:,np.newaxis],dtype=np.uint8)),axis = 2)   \n","    \n","    X_aug = np.concatenate((X_aug, np.ones_like(X_aug[:,:,0][:,:,np.newaxis],dtype=np.uint8)),axis = 2)\n","\n","    print(\"X_aug shape 2:\", X_aug.shape)\n","    X = np.vstack((X, X_aug))\n","    y = np.vstack((y, np.flip(y, axis=1)))\n","    \n","    return X, y, Xtest, Xval"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.432964Z","start_time":"2020-04-11T02:14:24.430264Z"},"trusted":true,"id":"Y-nODxu7cOgS","colab_type":"code","colab":{}},"source":["# Add ops to save and restore all the variables.\n","# saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.438940Z","start_time":"2020-04-11T02:14:24.436211Z"},"trusted":true,"id":"VRzoz-eacOgU","colab_type":"code","colab":{}},"source":["# # %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:24:41.652529Z\",\"start_time\":\"2020-04-03T23:24:41.645025Z\"}}\n","# class EarlyStopping:\n","#     def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n","#         self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n","#         self.counter, self.best_score = 0, None\n","#         self.is_maximize = is_maximize\n","\n","#     def load_best_weights(self, sess):\n","#         saver.restore(sess, self.checkpoint_path)\n","\n","#     def __call__(self, score, sess):\n","#         if self.best_score is None or \\\n","#         (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n","#             saver.save(sess, self.checkpoint_path)\n","#             self.best_score, self.counter = score, 0\n","#         else:\n","#             self.counter += 1\n","#             if self.counter >= self.patience:\n","#                 return True\n","#         return False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yzq5iOKjuWUY","trusted":true,"colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.pipeline import make_pipeline\n","from sklearn.base import BaseEstimator, TransformerMixin\n","\n","\n","class ShiftedFeatureMaker(BaseEstimator, TransformerMixin):\n","    \n","    def __init__(self, periods=[1], column=\"signal\", add_minus=False, fill_value=None, copy=True):\n","        self.periods = periods\n","        self.column = column\n","        self.add_minus = add_minus\n","        self.fill_value = fill_value\n","        self.copy = copy\n","        \n","    def fit(self, X, y):\n","        \"\"\"Mock method\"\"\"\n","        return self\n","    \n","    def transform(self, X: pd.DataFrame, y=None):\n","        periods = np.asarray(self.periods, dtype=np.int32)\n","        \n","        if self.add_minus:\n","            periods = np.append(periods, -periods)\n","        \n","        X_transformed = X.copy() if self.copy else X\n","        \n","        for p in periods:\n","            X_transformed[f\"{self.column}_shifted_{p}\"] = X_transformed[self.column].shift(\n","                periods=p, fill_value=self.fill_value\n","            )\n","            \n","        return X_transformed\n","\n","\n","class ColumnDropper(BaseEstimator, TransformerMixin):\n","    \n","    def __init__(self, columns=None):\n","        self.columns = columns\n","    \n","    def fit(self, X, y):\n","        \"\"\"Mock method\"\"\"\n","        return self\n","    \n","    def transform(self, X: pd.DataFrame, y=None):\n","        return X[[c for c in X.columns if c not in self.columns]]\n","\n","\n","def add_category(train, test):\n","    train[\"category\"] = 0\n","    test[\"category\"] = 0\n","    \n","    # train segments with more then 9 open channels classes\n","    train.loc[2_000_000:2_500_000-1, 'category'] = 1\n","    train.loc[4_500_000:5_000_000-1, 'category'] = 1\n","    \n","    # # delete very noized part \n","    # train.loc[3_650_000:3_820_000, \"category\"] = -1\n","    # train = train[train.category != -1].reset_index(drop=True)\n","    \n","    # test segments with more then 9 open channels classes (potentially)\n","    test.loc[500_000:600_000-1, \"category\"] = 1\n","    test.loc[700_000:800_000-1, \"category\"] = 1\n","    \n","    return train, test\n","\n","def add_5_category(train, test):\n","\n","    train[\"category\"] = 0\n","    test[\"category\"] = 0\n","\n","    #slow open channel\n","    batch = 1; a = 500000*(batch-1); b = 500000*batch\n","    batch = 2; c = 500000*(batch-1); d = 500000*batch\n","    train.loc[a:b, \"category\"] = 1\n","    train.loc[c:d, \"category\"] = 1 \n","\n","    # fast open channel\n","    batch = 3; a = 500000*(batch-1); b = 500000*batch\n","    batch = 7; c = 500000*(batch-1); d = 500000*batch\n","    train.loc[a:b, \"category\"] = 2\n","    train.loc[c:d, \"category\"] = 2\n","\n","    # 3 channel\n","    batch = 4; a = 500000*(batch-1); b = 500000*batch\n","    batch = 8; c = 500000*(batch-1); d = 500000*batch\n","    train.loc[a:b, \"category\"] = 3\n","    train.loc[c:d, \"category\"] = 3\n","\n","    # 5 open channel\n","    batch = 6; a = 500000*(batch-1); b = 500000*batch\n","    batch = 9; c = 500000*(batch-1); d = 500000*batch\n","    train.loc[a:b, \"category\"] = 4\n","    train.loc[c:d, \"category\"] = 4\n","\n","    # 10 open channel\n","    batch = 5; a = 500000*(batch-1); b = 500000*batch\n","    batch = 10; c = 500000*(batch-1); d = 500000*batch\n","    train.loc[a:b, \"category\"] = 5\n","    train.loc[c:d, \"category\"] = 5\n","\n","    a = 0 # SUBSAMPLE A, Model 1s\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 1 \n","\n","    a = 1 # SUBSAMPLE B, Model 3\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 3\n","\n","    a = 2 # SUBSAMPLE C, Model 5\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 4\n","\n","    a = 3 # SUBSAMPLE D, Model 1s\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 1\n","\n","    a = 4 # SUBSAMPLE E, Model 1f\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 2\n","\n","    a = 5 # SUBSAMPLE F, Model 10\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 5\n","\n","    a = 6 # SUBSAMPLE G, Model 5\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 4\n","\n","    a = 7 # SUBSAMPLE H, Model 10\n","    test[\"category\"].iloc[100000*a:100000*(a+1)]= 5\n","\n","    a = 8 # SUBSAMPLE I, Model 1s\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 1\n","\n","    a = 9 # SUBSAMPLE J, Model 3\n","    test[\"category\"].iloc[100000*a:100000*(a+1)] = 3\n","\n","    # BATCHES 3 AND 4, Model 1s\n","    test[\"category\"].iloc[1000000:2000000] = 1\n","\n","    return train, test   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6l0nc-llBdy","trusted":true,"colab_type":"code","colab":{}},"source":["import pickle\n","\n","def prob_with_RF(train_rfc, val_rfc, test_rfc, pkl_model_filename, force_build=False):\n","    shifted_rfc = make_pipeline(\n","        ShiftedFeatureMaker(\n","            periods=range(1, 20),\n","            add_minus=True,\n","            fill_value=0\n","        ),\n","        ColumnDropper(\n","            columns=[\"open_channels\", \"time\", \"group\"]\n","        ),\n","        # RandomForestClassifier(\n","        #     n_estimators=120,\n","        #     max_depth=19,\n","        #     max_features=10,\n","        #     random_state=42,\n","        #     n_jobs=20,\n","        #     verbose=2\n","        # )\n","        RandomForestClassifier(\n","            n_estimators=TREES,\n","            max_depth=DEPTH,\n","            max_features=10,\n","            random_state=42,\n","            n_jobs=-1,\n","            verbose=2\n","        )\n","    )\n","    if force_build or (not os.path.exists(pkl_model_filename)):\n","        print(\"Train RF model:\", pkl_model_filename)\n","        shifted_rfc.fit(train_rfc, train_rfc.open_channels)\n","        # Save to file in the current working directory\n","        # pkl_filename = \"pickle_model.pkl\"\n","        with open(pkl_model_filename, 'wb') as file:\n","          pickle.dump(shifted_rfc, file)\n","          print(\"Save RF model: \", pkl_model_filename)\n","\n","    # Load from file\n","    with open(pkl_model_filename, 'rb') as file:\n","        pickle_model = pickle.load(file)\n","        print(\"Loading RF model: \", pkl_model_filename)\n","    train_predictions = pickle_model.predict_proba(train_rfc)\n","    # print(\"train predic\")\n","    # print(train_predictions[:10])\n","    test_predictions = pickle_model.predict_proba(test_rfc)\n","    val_predictions = pickle_model.predict_proba(val_rfc)\n","    for i in range(11):\n","        train_rfc['Prob_{}'.format(i)] = train_predictions[:,i]\n","        test_rfc['Prob_{}'.format(i)] = test_predictions[:,i]\n","        val_rfc['Prob_{}'.format(i)] = val_predictions[:,i]\n","    \n","    train_rfc = reduce_mem_usage(train_rfc)\n","    test_rfc = reduce_mem_usage(test_rfc)\n","    val_rfc = reduce_mem_usage(val_rfc)\n","    return train_rfc, val_rfc, test_rfc\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xuj2XnjrBJmg","trusted":true,"colab_type":"code","colab":{}},"source":["# a = np.random.randn(3,4,2)\n","# print(a)\n","# print(a.shape)\n","# b = np.random.randn(3,4)\n","# print(b)\n","# print(b.shape)\n","# print(b[:,:,None].shape)\n","# c = np.concatenate((a,b[:,:,None]), axis=2)\n","# print(c)\n","# print(c.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MY_Z_SDD1kkJ","trusted":true,"colab_type":"code","colab":{}},"source":["# class CustomGen(TimeseriesGenerator):\n","#     def __getitem__(self, idx):\n","#         x, y = super().__getitem__(idx)\n","#         print(\"x.shape 1:\", x.shape)\n","#         x = np.squeeze(x)\n","#         # y = np.squeeze(y)\n","#         # print(y[0])\n","#         # timelen = x.shape[1]\n","#         # jitterscale = 0.1\n","#         # mulscale = 0.1\n","#         ## do processing here\n","#         # if (np.random.rand()<0):\n","#         #     if(np.random.rand()<0.5):\n","#         #         # augid = np.random.randint(low = 0, high = timelen, size = np.ceil(timelen*jitterscale).astype(int))\n","#         #         # augid = np.unique(augid)\n","#         #         x[:,:,0] = np.squeeze(jitter(x[:,:,0][:,:,None], sigma = 0.01))\n","#         #     else:\n","#         #         # augid = np.random.randint(low = 0, high = timelen, size = np.ceil(timelen*mulscale).astype(int))\n","#         #         # augid = np.unique(augid)\n","#         #         x[:,:,0] = np.squeeze(scaling(x[:,:,0][:,:,None], sigma = 0.02))\n","#         #     for i in range(3):\n","#         #         pos_x = np.roll(x[:,:,0], i)\n","#         #         neg_x = np.roll(x[:,:,0], -i)\n","#         #         x[:,:,12+i*4] = pos_x\n","#         #         x[:,:,12+i*4+1] = neg_x\n","#         #         x[:,:,12+i*4+3] = pos_x - x[:,:,0]\n","#         #         x[:,:,12+i*4+4] = neg_x - x[:,:,0]\n","#         #     x[:,:,24] = x[:,:,0]**2\n","#         # print(\"x.shape 2:\", x.shape)\n","#         return x, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RXy4eGuFM4Gq","trusted":true,"colab_type":"code","colab":{}},"source":["from tensorflow.python.keras.utils.data_utils import Sequence\n","\n","class CustomSequence(Sequence):\n","    def __init__(self, x_set, y_set, batch_size, augmentations=None):\n","        self.x, self.y = x_set, y_set\n","        self.batch_size = batch_size\n","        self.augment = augmentations\n","        self.indices = np.arange(self.x.shape[0])\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.x) / float(self.batch_size)))\n","\n","    def my_aug(self, x, y):\n","        if(np.random.rand()<0.3):\n","            shift = (np.random.randint(2000, size=1))[0]\n","            x = tf.roll(x, shift=shift, axis=1)\n","            y = tf.roll(y, shift=shift, axis=1)\n","        # if(np.random.rand()<0.5):\n","        #     x[:,:,0] = np.squeeze(jitter(x[:,:,0][:,:,None], sigma = 0.01))\n","        # else:\n","        #     x[:,:,0] = np.squeeze(scaling(x[:,:,0][:,:,None], sigma = 0.02))\n","        # for i in range(3):\n","        #     pos_x = np.roll(x[:,:,0], i)\n","        #     neg_x = np.roll(x[:,:,0], -i)\n","        #     x[:,:,12+i*4] = pos_x\n","        #     x[:,:,12+i*4+1] = neg_x\n","        #     x[:,:,12+i*4+3] = pos_x - x[:,:,0]\n","        #     x[:,:,12+i*4+4] = neg_x - x[:,:,0]\n","        # x[:,:,24] = x[:,:,0]**2\n","        return x, y\n","\n","    def __getitem__(self, idx):\n","        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        batch_x = self.x[inds]\n","        batch_y = self.y[inds]\n","        if (np.random.rand()<0.1):\n","            batch_x, batch_y = self.my_aug(batch_x, batch_y)\n","\n","        return np.array(batch_x), np.array(batch_y)\n","        # return np.stack([\n","        #     self.augment(image=x)[\"image\"] for x in batch_x\n","        # ], axis=0), np.array(batch_y)\n","    def on_epoch_end(self):\n","          np.random.shuffle(self.indices)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cZcailO_uCR","trusted":true,"colab_type":"code","colab":{}},"source":["# a = np.random.randn(8,3)\n","# print(a)\n","# b = np.random.randn(8,5)\n","# print(b)\n","# data_gen = CustomSequence(a,b, batch_size=2)\n","# print(\"get custom data\")\n","# tx, ty = data_gen[0]\n","# print(tx.shape)\n","# print(ty.shape)\n","# print(tx)\n","# print(ty)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"amY5F_7BqFHc","trusted":true,"colab_type":"code","colab":{}},"source":["def generate_sample_weights(training_data): \n","    sample_weights = np.where(training_data[:,:,8] == 0, 1, 3.0)\n","    print(\"unique sample:\", np.unique(sample_weights))\n","    print(\"3 count:\", np.count_nonzero(np.asarray(sample_weights)-1))\n","    print(\"1 count:\", np.count_nonzero(np.asarray(sample_weights)-3))\n","    return np.asarray(sample_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.477074Z","start_time":"2020-04-11T02:14:24.441269Z"},"code_folding":[],"id":"e4QulGxHfS0n","trusted":true,"colab_type":"code","colab":{}},"source":["from tensorflow.keras.utils import plot_model\n","def run_cv_model_by_batch(train : pd.DataFrame,\n","                          test : pd.DataFrame,\n","                          splits : int,\n","                          batch_col : Text,\n","                          feats : List,\n","                          sample_submission: pd.DataFrame,\n","                          nn_epochs : int,\n","                          nn_batch_size : int) -> NoReturn:\n","    seed_everything(SEED)\n","    K.clear_session()\n","    if not os.path.exists(outdir):\n","      os.mkdir(outdir)\n","    print(outdir)\n","    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n","    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n","    tf.compat.v1.keras.backend.set_session(sess)\n","\n","    train_rfc = pd.DataFrame(train, columns=['time', 'signal', 'open_channels', 'group'])\n","\n","    test_rfc = pd.DataFrame(test, columns=['time', 'signal', 'group'])\n","    train_rfc, test_rfc = add_category(train_rfc, test_rfc)\n","    print(train_rfc.shape[0],train_rfc.shape[1])\n","    print(train_rfc.head())\n","    oof_ = np.zeros((len(train), 11))\n","    preds_ = np.zeros((len(test), 11))\n","    target = ['open_channels']\n","    group = train['group']\n","    kf = GroupKFold(n_splits=5)\n","    splits = [x for x in kf.split(train, train[target], group)]\n","\n","    # pkl_path = os.path.join(PATH,'res/RFC/pickle_model_rfc_20_10.pkl')\n","    # pkl_gen_path = os.path.join(PATH,'res/RFC/pickle_model_rfc_20_10_run3.pkl')\n","\n","    new_splits = []\n","    for sp in splits:\n","        new_split = []\n","        new_split.append(np.unique(group[sp[0]]))\n","        new_split.append(np.unique(group[sp[1]]))\n","        new_split.append(sp[0])  \n","        new_split.append(sp[1])    \n","        new_splits.append(new_split)\n","\n","    # Calculate the weights for each class so that we can balance the data\n","    weights_ = class_weight.compute_class_weight('balanced',\n","                                                np.unique(train.open_channels),\n","                                                train.open_channels)\n","    print(\"weights_:\", weights_)\n","\n","    nums_ = train.open_channels.value_counts(sort=False)\n","    print(\"nums_:\", nums_)\n","    beta = BETA\n","    effective_num = 1.0 - np.power(beta, nums_)\n","    weights = (1.0 - beta) / np.array(effective_num)\n","    cb_weights_ = weights / np.sum(weights) * 11\n","    print(\"cb_weights_:\", cb_weights_)   \n","        \n","    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n","\n","    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n","    # print(tr.head())\n","    target_cols = ['target_'+str(i) for i in range(11)]\n","    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n","    print(np.shape(train_tr))\n","    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n","    print(np.shape(train))\n","    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n","    # print(\"test shape:\", np.shape(test))\n","    for n_fold, (tr_idx, val_idx, tr_orig_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n","        sub_dir = os.path.join(outdir,\"{}_fold\".format(n_fold))\n","        # if n_fold<3:\n","        #     continue\n","        # if n_fold== 4:\n","        #     break\n","        if not os.path.exists(sub_dir):\n","            os.mkdir(sub_dir)\n","        # pkl_model_filename = os.path.join(sub_dir,'rf-{}-tn{}-dn{}.pkl'.format(n_fold, TREES, DEPTH))\n","        pkl_model_filename = os.path.join(PATH,'res/RFC/RF100-D12/rf-f{}-tn{}-dn{}.pkl'.format(n_fold, TREES, DEPTH))\n","        # pkl_model_filename = os.path.join(PATH,'myrfres/rf-f{}-tn{}-dn{}.pkl'.format(n_fold, TREES, DEPTH))\n","        # print(\"train index\")\n","        # print(\"tr_orig_idx max and min:\", max(tr_orig_idx), min(tr_orig_idx))\n","        # print(\"tr_orig_idx shape:\", np.shape(tr_orig_idx))\n","        train_rfc_ = train_rfc.iloc[tr_orig_idx].copy()\n","        valid_rfc_ = train_rfc.iloc[val_orig_idx].copy()\n","        test_rfc_ = test_rfc.copy()\n","        train_rfc_, valid_rfc_, test_rfc_ = prob_with_RF(train_rfc_, valid_rfc_, test_rfc_, pkl_model_filename)\n","        # # print(train_rfc_.head())\n","        # # print(valid_rfc_.head())\n","        # # print(test_rfc.head())\n","        # print(\"train_rfc_.shape: \",train_rfc_.shape)\n","        # print(\"tr_idx shape: \", np.shape(tr_idx))\n","        # print(\"train shape:\", np.shape(train))\n","\n","        # if n_fold < 2:\n","        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n","        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n","        test_ = test.copy()\n","\n","        print(\"train_x shape 1: \", np.shape(train_x))\n","        for i in range(11):\n","          trainprob = np.array(list(train_rfc_.groupby('group').apply(lambda x: x['Prob_{}'.format(i)].values)))\n","          train_x = np.concatenate((train_x, trainprob[:,:,None]),axis = 2)\n","          vprob = np.array(list(valid_rfc_.groupby('group').apply(lambda x: x['Prob_{}'.format(i)].values)))\n","          valid_x = np.concatenate((valid_x, vprob[:,:,None]),axis = 2)\n","          testprob = np.array(list(test_rfc_.groupby('group').apply(lambda x: x['Prob_{}'.format(i)].values)))\n","          test_ = np.concatenate((test_, testprob[:,:,None]),axis = 2)\n","        print(\"train_x shape 2: \", np.shape(train_x))\n","\n","\n","        # train_x, train_y = augment(train_x, train_y)\n","        train_x, train_y, test_, valid_x = augment_label(train_x, train_y, test_, valid_x)\n","\n","        print(\"unique train_x[8]:\", np.unique(train_x[:,:,8]))\n","        # train_x = feature_roll_diff(train_x)\n","        # test_ = feature_roll_diff(test_)\n","        # valid_x = feature_roll_diff(valid_x)\n","        print(\"test shape after roll diff:\", np.shape(test_))\n","\n","        print(\"train_x shape 3: \", np.shape(train_x))\n","        print(\"train_y shape 3: \", np.shape(train_y))\n","        gc.collect()\n","        shape_ = (BATCHSIZE, train_x.shape[2])\n","        model = ClassifierCBRW(shape_)\n","        # plot_model(model, to_file='model.png')\n","        print(\"model initilization done!\")\n","        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n","        # cb_clr = CyclicLR(base_lr=1e-7, max_lr = LR, step_size= int(1.0*(train.shape[0])/(nn_batch_size*4)) , \n","        #                   mode='exp_range', gamma=Gamma, scale_fn=None, scale_mode='cycle')\n","        cb_prg = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,leave_overall_progress=False, \n","                                               show_epoch_progress=False,show_overall_progress=True)\n","        cb_reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n","                              patience=5, min_lr=1e-6, verbose = 1)\n","        # cb_csv_logger= CSVLogger(os.path.join(sub_dir,'res.csv'))\n","        # cb_history = LossHistory()  # 这里是使用自定义的Callback回调函数，当然本身fit函数也会返回一个history可供使用\n","        \n","        save_checkpoint_path = os.path.join(sub_dir,'checkpoint-modelonly-{}.h5'.format(n_fold))\n","        save_finalmodel_path = os.path.join(sub_dir,'fmodel-modelonly-{}.h5'.format(n_fold))\n","        save_bestf1macro_path = os.path.join(sub_dir,'checkpoint-{}.h5'.format(n_fold)) \n","        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_checkpoint_path,\n","                                              monitor='val_accuracy',\n","                                              mode = 'max',            \n","                                              save_weights_only=True,\n","                                              save_best_only=True,           \n","                                              verbose=1)\n","        # print(train_y[0])\n","        data_gen = CustomSequence(train_x,train_y, batch_size = nn_batch_size)\n","        # history = model.fit_generator(generator = data_gen,\n","        #     epochs=nn_epochs,\n","        #     callbacks=[cb_prg, cb_clr, cp_callback,\n","        #                 MacroF1ES(model, valid_x, valid_y, patience=70, delta=0, \n","        #                           checkpoint_path=save_bestf1macro_path)],\n","        #     verbose=1,\n","        #     validation_data=(valid_x,valid_y))\n","#         history = model.fit(x = data_gen,\n","#                     epochs=nn_epochs,\n","#                     callbacks=[cb_prg, cb_lr_schedule, cp_callback,\n","#                                 MacroF1ES(model, valid_x, valid_y, patience=70, delta=0, \n","#                                           checkpoint_path=save_bestf1macro_path)],\n","#                     batch_size=nn_batch_size,\n","#                     verbose=1,\n","#                     validation_data=(valid_x,valid_y))\n","# #         history = model.fit(train_x, train_y,\n","# #             epochs=nn_epochs,\n","# #             callbacks=[cb_prg, cb_lr_schedule, cp_callback,\n","# #                         MacroF1ES(model, valid_x, valid_y, patience=70, delta=0, \n","# #                                   checkpoint_path=save_bestf1macro_path)],\n","# #             batch_size=nn_batch_size,\n","# # #             sample_weights = generate_sample_weights(train_x),\n","# #             verbose=1,\n","# #             validation_data=(valid_x,valid_y))\n","#         pd.DataFrame(history.history).to_csv(os.path.join(sub_dir,'{}-len{}-lr{}-{}-log-{}.csv'.format(MODELNAME,BATCHSIZE,LR,timestampStr, n_fold)), float_format='%.4f')\n","#         # print('\\nhistory dict:', history.history)\n","#         model.save_weights(save_finalmodel_path)\n","        model.load_weights(save_bestf1macro_path)\n","        # model.load_weights(save_checkpoint_path)\n","        preds_f = model.predict(valid_x)\n","        # preds_f = np.squeeze(preds_f)\n","        print(\"preds_f shape:\", preds_f.shape)\n","        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  \n","                             np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n","        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n","        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n","        oof_[val_orig_idx,:] += preds_f\n","        te_preds = model.predict(test_)\n","        # te_preds = np.squeeze(te_preds)\n","        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n","        preds_ += te_preds / SPLITS\n","\n","    # oof_ = np.array([oof_[k]/sum(oof_[k]) for k in range(len(oof_))])\n","    # print(np.all(abs(1-np.sum(oof_,axis = 1))<1e-6))\n","    # oofres = pd.DataFrame({f'tProb{k}': oof_[:, k] for k in range(11)})\n","    # oofres.to_csv(os.path.join(outdir,'{}_tranformerprobv2.csv'.format(VERSION)), float_format='%.4f')\n","    # display(oofres.head())\n","\n","    # preds_ = np.array([preds_[k]/sum(preds_[k]) for k in range(len(preds_))])\n","    # print(np.all(abs(1-np.sum(preds_,axis = 1))<1e-6))\n","    # predsres = pd.DataFrame({f'tProb{k}': preds_[:, k] for k in range(11)})\n","    # predsres.to_csv(os.path.join(outdir,'{}_test_tranformerprobv2.csv'.format(VERSION)), float_format='%.4f')\n","    # display(predsres.head())\n","\n","    f1_score_ =f1_score(np.argmax(train_tr, axis=2).reshape(-1),  np.argmax(oof_, axis=1), average = 'macro')\n","    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n","    sample_submission['open_channels'] = np.argmax(preds_, axis=1).astype(int)\n","    sample_submission.to_csv(os.path.join(outdir,'{}_bf1_pred.csv'.format(VERSION)), index=False, float_format='%.4f')\n","    display(sample_submission.head())\n","    # np.save(os.path.join(outdir,'oof.npy'), oof_)\n","    # np.save(os.path.join(outdir,'preds.npy'), preds_)\n","\n","    return \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.485692Z","start_time":"2020-04-11T02:14:24.479620Z"},"id":"dqt0VJAS2ucB","trusted":true,"colab_type":"code","colab":{}},"source":["\n","# def lr_schedule(epoch):\n","#     if epoch < 40:\n","#         lr = LR\n","#     elif epoch < 50:\n","#         lr = LR / 3\n","#     elif epoch < 60:\n","#         lr = LR / 6\n","#     elif epoch < 75:\n","#         lr = LR / 9\n","#     elif epoch < 85:\n","#         lr = LR / 12\n","#     elif epoch < 100:\n","#         lr = LR / 15\n","#     else:\n","#         lr = LR / 50\n","#     return lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aaClcd8zJIyC","trusted":true,"colab_type":"code","colab":{}},"source":["# # function that decrease the learning as epochs increase (i also change this part of the code)\n","def lr_schedule(epoch):\n","    if epoch < 30:\n","        lr = LR\n","    elif epoch < 40:\n","        lr = LR / 3\n","    elif epoch < 50:\n","        lr = LR / 5\n","    elif epoch < 60:\n","        lr = LR / 7\n","    elif epoch < 70:\n","        lr = LR / 9\n","    elif epoch < 80:\n","        lr = LR / 11\n","    elif epoch < 90:\n","        lr = LR / 13\n","    elif epoch < 150:\n","        lr = LR / 100\n","    else:\n","        lr = LR / 1000\n","    return lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AeyLImbpAKdx","colab_type":"code","colab":{}},"source":["# function that decrease the learning as epochs increase (i also change this part of the code)\n","# def lr_schedule(epoch):\n","#     if epoch < 30:\n","#         lr = LR\n","#     elif epoch < 40:\n","#         lr = LR / 5\n","#     elif epoch < 50:\n","#         lr = LR / 9\n","#     elif epoch < 60:\n","#         lr = LR / 13\n","#     elif epoch < 70:\n","#         lr = LR / 15\n","#     elif epoch < 80:\n","#         lr = LR / 100\n","#     else:\n","#         lr = LR / 1000\n","#     return lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nzrGUesf5vYU","colab_type":"code","colab":{}},"source":["# # function that decrease the learning as epochs increase (i also change this part of the code)\n","# def lr_schedule(epoch):\n","#     if epoch < 20:\n","#         lr = LR\n","#     elif epoch < 30:\n","#         lr = LR / 3\n","#     elif epoch < 40:\n","#         lr = LR / 5\n","#     elif epoch < 50:\n","#         lr = LR / 9\n","#     elif epoch < 60:\n","#         lr = LR / 13\n","#     elif epoch < 70:\n","#         lr = LR / 15\n","#     elif epoch < 80:\n","#         lr = LR / 100\n","#     else:\n","#         lr = LR / 1000\n","#     return lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.499088Z","start_time":"2020-04-11T02:14:24.488223Z"},"id":"SjbrEQSa2ucN","trusted":true,"colab_type":"code","colab":{}},"source":["class Mish(tf.keras.layers.Layer):\n","\n","    def __init__(self, **kwargs):\n","        super(Mish, self).__init__(**kwargs)\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        return inputs * K.tanh(K.softplus(inputs))\n","\n","    def get_config(self):\n","        base_config = super(Mish, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","def mish(x):\n","\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n"," \n","from tensorflow.keras.utils import get_custom_objects\n","# from tensorflow.keras.layers import Activation\n","get_custom_objects().update({'mish': Activation(mish)})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.521416Z","start_time":"2020-04-11T02:14:24.502541Z"},"id":"dM8RiNfL2ucY","trusted":true,"colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras import initializers\n","from tensorflow.keras import regularizers\n","from tensorflow.keras import constraints\n","\n","# class Attention(Layer):\n","#     \"\"\"Multi-headed attention layer.\"\"\"\n","    \n","#     def __init__(self, hidden_size, \n","#                  num_heads = 8, \n","#                  attention_dropout=.1,\n","#                  trainable=True,\n","#                  name='Attention'):\n","        \n","#         if hidden_size % num_heads != 0:\n","#             raise ValueError(\"Hidden size must be evenly divisible by the number of heads.\")\n","            \n","#         self.hidden_size = hidden_size\n","#         self.num_heads = num_heads\n","#         self.trainable = trainable\n","#         self.attention_dropout = attention_dropout\n","#         self.dense = tf.keras.layers.Dense(self.hidden_size, use_bias=False)\n","#         self.time_dense = TimeDistributed(Dense(self.hidden_size, use_bias=False))\n","#         super(Attention, self).__init__(name=name)\n","\n","#     def split_heads(self, x):\n","#         \"\"\"Split x into different heads, and transpose the resulting value.\n","#         The tensor is transposed to insure the inner dimensions hold the correct\n","#         values during the matrix multiplication.\n","#         Args:\n","#           x: A tensor with shape [batch_size, length, hidden_size]\n","#         Returns:\n","#           A tensor with shape [batch_size, num_heads, length, hidden_size/num_heads]\n","#         \"\"\"\n","#         with tf.name_scope(\"split_heads\"):\n","#             batch_size = tf.shape(x)[0]\n","#             length = tf.shape(x)[1]\n","\n","#             # Calculate depth of last dimension after it has been split.\n","#             depth = (self.hidden_size // self.num_heads)\n","\n","#             # Split the last dimension\n","#             x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n","\n","#             # Transpose the result\n","#             return tf.transpose(x, [0, 2, 1, 3])\n","    \n","#     def combine_heads(self, x):\n","#         \"\"\"Combine tensor that has been split.\n","#         Args:\n","#           x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n","#         Returns:\n","#           A tensor with shape [batch_size, length, hidden_size]\n","#         \"\"\"\n","#         with tf.name_scope(\"combine_heads\"):\n","#             batch_size = tf.shape(x)[0]\n","#             length = tf.shape(x)[2]\n","#             x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n","#             return tf.reshape(x, [batch_size, length, self.hidden_size])        \n","\n","#     def call(self, inputs):\n","#         \"\"\"Apply attention mechanism to inputs.\n","#         Args:\n","#           inputs: a tensor with shape [batch_size, length_x, hidden_size]\n","#         Returns:\n","#           Attention layer output with shape [batch_size, length_x, hidden_size]\n","#         \"\"\"\n","#         # Google developper use tf.layer.Dense to linearly project the queries, keys, and values.\n","#         q = self.dense(inputs)\n","#         k = self.dense(inputs)\n","#         v = self.dense(inputs)\n","\n","#         q = self.split_heads(q)\n","#         k = self.split_heads(k)\n","#         v = self.split_heads(v)\n","        \n","#         # Scale q to prevent the dot product between q and k from growing too large.\n","#         depth = (self.hidden_size // self.num_heads)\n","#         q *= depth ** -0.5\n","        \n","#         logits = tf.matmul(q, k, transpose_b=True)\n","#         # logits += self.bias\n","#         weights = tf.nn.softmax(logits, name=\"attention_weights\")\n","        \n","#         if self.trainable:\n","#             weights = tf.nn.dropout(weights, 1.0 - self.attention_dropout)\n","        \n","#         attention_output = tf.matmul(weights, v)\n","#         attention_output = self.combine_heads(attention_output)\n","#         attention_output = self.time_dense(attention_output)\n","#         attention_output = Dropout(self.attention_dropout)(attention_output)\n","#         return attention_output\n","        \n","#     def compute_output_shape(self, input_shape):\n","#         return tf.TensorShape(input_shape)\n","\n","#     def get_config(self):\n","#         base_config = super(Attention, self).get_config()\n","#         config = {'hidden_size' : self.hidden_size,\n","#                     'num_heads' : self.num_heads,\n","#                     'trainable' : self.trainable,\n","#                     'attention_dropout' : self.attention_dropout,\n","#                     'name':'Attention'}\n","#         # config = {'name':'Attention'}\n","#         return dict(list(base_config.items()) + list(config.items()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vrzikcyx8ptJ","trusted":true,"colab_type":"code","colab":{}},"source":["# focal loss with one-hot labels, multiclass\n","def cb_focal_loss(classes_weights, gamma=2., alpha=.25, w=3e-4):\n","    # classes_weights contains weights of each classes\n","    def cb_focal_loss_fixed(target_tensor, prediction_tensor):\n","        '''\n","        prediction_tensor is the output tensor with shape [None, 100], where 100 is the number of classes\n","        target_tensor is the label tensor, same shape as predcition_tensor\n","        '''\n","\n","        #1# get focal loss with no balanced weight which presented in paper function (4)\n","        zeros = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n","        one_minus_p = array_ops.where(tf.greater(target_tensor,zeros), target_tensor - prediction_tensor, zeros)\n","        FT = -1 * (one_minus_p ** gamma) * tf.math.log(tf.clip_by_value(prediction_tensor, 1e-8, 1.0))\n","\n","        #2# get balanced weight alpha\n","        classes_weight = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n","        classes_w_tensor = tf.convert_to_tensor(classes_weights, dtype=prediction_tensor.dtype)\n","        classes_weight += classes_w_tensor\n","\n","        alpha = array_ops.where(tf.greater(target_tensor, zeros), classes_weight, zeros)\n","\n","        #3# get balanced focal loss\n","        balanced_fl = alpha * FT\n","        balanced_fl = tf.reduce_mean(balanced_fl)\n","\n","        #4# add other op to prevent overfit\n","        # reference : https://spaces.ac.cn/archives/4493\n","        nb_classes = len(classes_weights)\n","        fianal_loss = (1-w) * balanced_fl + w * K.categorical_crossentropy(K.ones_like(prediction_tensor)/nb_classes, prediction_tensor)\n","\n","        return fianal_loss\n","    return cb_focal_loss_fixed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JM2iC9UwFd-c","trusted":true,"colab_type":"code","colab":{}},"source":["def multi_category_focal_loss2(gamma=2., alpha=.25):\n","    \"\"\"\n","    focal loss for multi category of multi label problem\n","    适用于多分类或多标签问题的focal loss\n","    alpha控制真值y_true为1/0时的权重\n","        1的权重为alpha, 0的权重为1-alpha\n","    当你的模型欠拟合，学习存在困难时，可以尝试适用本函数作为loss\n","    当模型过于激进(无论何时总是倾向于预测出1),尝试将alpha调小\n","    当模型过于惰性(无论何时总是倾向于预测出0,或是某一个固定的常数,说明没有学到有效特征)\n","        尝试将alpha调大,鼓励模型进行预测出1。\n","    Usage:\n","     model.compile(loss=[multi_category_focal_loss2(alpha=0.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n","    \"\"\"\n","    epsilon = 1.e-7\n","    gamma = float(gamma)\n","    alpha = tf.constant(alpha, dtype=tf.float32)\n","\n","    def multi_category_focal_loss2_fixed(y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n","    \n","        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n","        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n","        ce = -tf.math.log(y_t)\n","        weight = tf.pow(tf.subtract(1., y_t), gamma)\n","        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n","        loss = tf.reduce_mean(fl)\n","        return loss\n","    return multi_category_focal_loss2_fixed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6d4mH_0Omj2","trusted":true,"colab_type":"code","colab":{}},"source":["def get_f1(y_true, y_pred): #taken from old keras source code\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.549703Z","start_time":"2020-04-11T02:14:24.533479Z"},"id":"YSESlpaNwkkx","trusted":true,"colab_type":"code","colab":{}},"source":["def WaveNetResidualConv1D(num_filters, kernel_size, stacked_layer):\n","\n","    def build_residual_block(l_input):\n","        resid_input = l_input\n","        for dilation_rate in [2**i for i in range(stacked_layer)]:\n","            l_sigmoid_conv1d = Conv1D(\n","              num_filters, kernel_size, dilation_rate=dilation_rate,\n","              padding='same', activation='sigmoid')(l_input)\n","            l_tanh_conv1d = Conv1D(\n","             num_filters, kernel_size, dilation_rate=dilation_rate,\n","             padding='same', activation='mish')(l_input)\n","            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n","            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n","            resid_input = Add()([resid_input ,l_input])\n","        return resid_input\n","    return build_residual_block\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEn4iIRTaslB","trusted":true,"colab_type":"code","colab":{}},"source":["def Classifier(shape_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    l_output = Dense(11, activation='softmax')(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZymKtoVo8F06","trusted":true,"colab_type":"code","colab":{}},"source":["def ClassifierW(shape_, weights_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    l_output = Dense(11, activation='softmax')(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","\n","    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n","    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n","\n","#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n","#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.563702Z","start_time":"2020-04-11T02:14:24.556844Z"},"id":"9Cckpy4I2uco","trusted":true,"colab_type":"code","colab":{}},"source":["def Classifierx(shape_):        \n","    # dsize = [256, 512, 256, 128, 11] # try 2: batchsize = 64, len = 1000\n","    dsize = [128, 256, 128, 64, 11] # try3: batchsize = 32, len = 1000\n","    # dsize = [32, 64, 32, 32, 11]  # try 1: batchsize=10, len=1000\n","    inp = Input(shape=(shape_))\n","    x = Bidirectional(GRU(dsize[0], return_sequences=True))(inp)\n","    x = Attention(dsize[1])(x)\n","    x = TimeDistributed(Dense(dsize[2], activation='mish'))(x)\n","    x = TimeDistributed(Dense(dsize[3], activation='mish'))(x)\n","    out = TimeDistributed(Dense(dsize[4], activation='softmax', name='out'))(x)\n","    \n","    model = models.Model(inputs=inp, outputs=out) \n","    \n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMi9nBlRtw_-","colab_type":"code","colab":{}},"source":["try:\n","    from dataloader import TokenList, pad_to_longest\n","    # for transformer\n","except: pass\n","\n","\n","class LayerNormalization(Layer):\n","    def __init__(self, eps=1e-6, **kwargs):\n","        self.eps = eps\n","        super(LayerNormalization, self).__init__(**kwargs)\n","    def build(self, input_shape):\n","        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n","                                     initializer=tf.ones_initializer(), trainable=True)\n","        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n","                                    initializer=tf.zeros_initializer(), trainable=True)\n","        super(LayerNormalization, self).build(input_shape)\n","    def call(self, x):\n","        mean = K.mean(x, axis=-1, keepdims=True)\n","        std = K.std(x, axis=-1, keepdims=True)\n","        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","class ScaledDotProductAttention():\n","    def __init__(self, d_model, attn_dropout=0.1):\n","        self.temper = np.sqrt(d_model)\n","        self.dropout = Dropout(attn_dropout)\n","    def __call__(self, q, k, v, mask):\n","        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n","        if mask is not None:\n","            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n","            attn = Add()([attn, mmask])\n","        attn = Activation('softmax')(attn)\n","        attn = self.dropout(attn)\n","        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n","        return output, attn\n","\n","class MultiHeadAttention():\n","    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n","    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n","        self.mode = mode\n","        self.n_head = n_head\n","        self.d_k = d_k\n","        self.d_v = d_v\n","        self.dropout = dropout\n","        if mode == 0:\n","            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n","            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n","            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n","        elif mode == 1:\n","            self.qs_layers = []\n","            self.ks_layers = []\n","            self.vs_layers = []\n","            for _ in range(n_head):\n","                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n","                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n","                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n","        self.attention = ScaledDotProductAttention(d_model)\n","        self.layer_norm = LayerNormalization() if use_norm else None\n","        self.w_o = TimeDistributed(Dense(d_model))\n","\n","    def __call__(self, q, k, v, mask=None):\n","        d_k, d_v = self.d_k, self.d_v\n","        n_head = self.n_head\n","\n","        if self.mode == 0:\n","            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n","            ks = self.ks_layer(k)\n","            vs = self.vs_layer(v)\n","\n","            def reshape1(x):\n","                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n","                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n","                x = tf.transpose(x, [2, 0, 1, 3])  \n","                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n","                return x\n","            qs = Lambda(reshape1)(qs)\n","            ks = Lambda(reshape1)(ks)\n","            vs = Lambda(reshape1)(vs)\n","\n","            if mask is not None:\n","                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n","            head, attn = self.attention(qs, ks, vs, mask=mask)  \n","                \n","            def reshape2(x):\n","                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n","                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n","                x = tf.transpose(x, [1, 2, 0, 3])\n","                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n","                return x\n","            head = Lambda(reshape2)(head)\n","        elif self.mode == 1:\n","            heads = []; attns = []\n","            for i in range(n_head):\n","                qs = self.qs_layers[i](q)   \n","                ks = self.ks_layers[i](k) \n","                vs = self.vs_layers[i](v) \n","                head, attn = self.attention(qs, ks, vs, mask)\n","                heads.append(head); attns.append(attn)\n","            head = Concatenate()(heads) if n_head > 1 else heads[0]\n","            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n","\n","        outputs = self.w_o(head)\n","        outputs = Dropout(self.dropout)(outputs)\n","        if not self.layer_norm: return outputs, attn\n","        # outputs = Add()([outputs, q]) # sl: fix\n","        return self.layer_norm(outputs), attn\n","\n","class PositionwiseFeedForward():\n","    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n","        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n","        self.w_2 = Conv1D(d_hid, 1)\n","        self.layer_norm = LayerNormalization()\n","        self.dropout = Dropout(dropout)\n","    def __call__(self, x):\n","        output = self.w_1(x) \n","        output = self.w_2(output)\n","        output = self.dropout(output)\n","        output = Add()([output, x])\n","        return self.layer_norm(output)\n","\n","class EncoderLayer():\n","    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n","        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n","        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n","    def __call__(self, enc_input, mask=None):\n","        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n","        output = self.pos_ffn_layer(output)\n","        return output, slf_attn\n","\n","\n","def GetPosEncodingMatrix(max_len, d_emb):\n","    pos_enc = np.array([\n","        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n","        if pos != 0 else np.zeros(d_emb) \n","            for pos in range(max_len)\n","            ])\n","    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n","    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n","    return pos_enc\n","\n","def GetPadMask(q, k):\n","    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n","    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n","    mask = K.batch_dot(ones, mask, axes=[2,1])\n","    return mask\n","\n","def GetSubMask(s):\n","    len_s = tf.shape(s)[1]\n","    bs = tf.shape(s)[:1]\n","    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n","    return mask"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9ik1QsWt23L","colab_type":"code","colab":{}},"source":["# def CnnTransformerModel(shape_):\n","#     inp = Input(shape = (shape_))\n","    \n","#     x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', \n","#                            gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n","#                            beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(inp)\n","\n","#     x = Convolution1D( 8, kernel_size = 7, padding='same', activation='relu')(x)\n","    \n","#     x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', \n","#                            gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n","#                            beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(x)\n","\n","#     x = Convolution1D(16, kernel_size = 5, padding='same', activation='relu')(x)\n","    \n","#     x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', \n","#                            gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n","#                            beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(x)\n","\n","#     x = Convolution1D(32, kernel_size = 5, padding='same', activation='relu')(x)\n","    \n","#     x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', \n","#                            gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n","#                            beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(x)\n","\n","#     x = Convolution1D(64, kernel_size = 5, padding='same', activation='relu')(x)\n","\n","#     x = Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128, return_sequences = True, return_state = False))(x)\n","    \n","#     x = Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(64, return_sequences = True, return_state = False))(x)\n","    \n","#     x, slf_attn = MultiHeadAttention(n_head=4, d_model=128, d_k=64, d_v=64, dropout=0.3)(x, x, x)\n","#     # avg_pool = GlobalAveragePooling1D()(x)\n","    \n","#     # avg_pool = Dense(60,activation = 'relu')(avg_pool)\n","#     x = Dropout(0.2)(x)\n","#     out = Dense(11, activation = 'softmax', name = 'out')(x)\n","    \n","#     model = models.Model(inputs = inp, outputs = out)\n","    \n","#     opt = Adam(lr = LR)\n","#     opt = tfa.optimizers.SWA(opt)\n","#     model.compile(loss=losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n","#     return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"08yqygQjDQB9","colab_type":"code","colab":{}},"source":["def CnnTransformerModel(shape_):\n","    inp = Input(shape = (shape_))\n","    \n","    x = Bidirectional(GRU(208, return_sequences=True))(inp)\n","    x, slf_attn = MultiHeadAttention(n_head=8, d_model=416, d_k=64, d_v=64, dropout=0.05)(x, x, x)\n","    x = TimeDistributed(Dense(208, activation='mish'))(x)\n","    x = Dropout(0.2)(x)\n","    out = Dense(11, activation = 'softmax', name = 'out')(x)\n","    \n","    model = models.Model(inputs = inp, outputs = out)\n","    \n","    opt = Adam(lr = LR)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(loss=losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"57U9mCWQ7amw","colab_type":"code","colab":{}},"source":["def WaveCnnTransformerModel(shape_):\n","    def cbr(x, out_layer, kernel, stride, dilation):\n","        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n","        x = BatchNormalization()(x)\n","        x = Activation(\"relu\")(x)\n","        return x\n","    \n","    def wave_block(x, filters, kernel_size, n):\n","        dilation_rates = [2**i for i in range(n)]\n","        x = Conv1D(filters = filters,\n","                   kernel_size = 1,\n","                   padding = 'same')(x)\n","        res_x = x\n","        for dilation_rate in dilation_rates:\n","            tanh_out = Conv1D(filters = filters,\n","                              kernel_size = kernel_size,\n","                              padding = 'same', \n","                              activation = 'tanh', \n","                              dilation_rate = dilation_rate)(x)\n","            sigm_out = Conv1D(filters = filters,\n","                              kernel_size = kernel_size,\n","                              padding = 'same',\n","                              activation = 'sigmoid', \n","                              dilation_rate = dilation_rate)(x)\n","            x = Multiply()([tanh_out, sigm_out])\n","            x = Conv1D(filters = filters,\n","                       kernel_size = 1,\n","                       padding = 'same')(x)\n","            res_x = Add()([res_x, x])\n","        return res_x\n","    \n","    inp = Input(shape = (shape_))\n","    x = cbr(inp, 64, 7, 1, 1)\n","    x = BatchNormalization()(x)\n","    x = wave_block(x, 16, 3, 10)\n","    x = BatchNormalization()(x)\n","    x = wave_block(x, 32, 3, 8)\n","    x = BatchNormalization()(x)\n","    # x = wave_block(x, 64, 3, 4)\n","    # x = BatchNormalization()(x)\n","    # x = wave_block(x, 128, 3, 1)\n","    # x = cbr(x, 32, 7, 1, 1)\n","    # x = BatchNormalization()(x)\n","    # x = Bidirectional(GRU(128, return_sequences=True))(x)\n","    x = Bidirectional(GRU(64, return_sequences=True))(x)\n","    x, slf_attn = MultiHeadAttention(n_head=2, d_model=96, d_k=48, d_v=48, dropout=0.05)(x, x, x)\n","    x1 = MaxPooling1D(data_format='channels_first')(x)\n","    x2 = AveragePooling1D(data_format='channels_first')(x)\n","    x = Concatenate()([x1,x2])\n","\n","    # x = Dense(32, activation='mish')(x)\n","    # x = Add()([x1,x])\n","    # x = BatchNormalization()(x)\n","    x = Dropout(0.2)(x)\n","    out = Dense(11, activation = 'softmax', name = 'out')(x)\n","    \n","    model = models.Model(inputs = inp, outputs = out)\n","    \n","    opt = Adam(lr = LR)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(loss=losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0lPAcchdy7-","trusted":true,"colab_type":"code","colab":{}},"source":["def CnnGRUTransformer(shape_):\n","    def cbr(x, out_layer, kernel, stride, dilation):\n","      x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n","      x = BatchNormalization()(x)\n","      x = Activation(\"relu\")(x)\n","      return x\n","    inp = Input(shape = (shape_))\n","    x = cbr(inp, 64, 1, 1, 1)\n","    x1 = MaxPooling1D(data_format='channels_first')(x)\n","    x2 = AveragePooling1D(data_format='channels_first')(x)\n","    x = Concatenate()([x1,x2])\n","    x = Bidirectional(GRU(128, return_sequences=True))(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.2)(x)\n","    x = Bidirectional(GRU(64, return_sequences=True))(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.2)(x)\n","    x, slf_attn = MultiHeadAttention(n_head=4, d_model=128, d_k=64, d_v=64, dropout=0.1)(x, x, x)\n","    x1 = MaxPooling1D(data_format='channels_first')(x)\n","    x2 = AveragePooling1D(data_format='channels_first')(x)\n","    x = Concatenate()([x1,x2])\n","    x = Dropout(0.2)(x)\n","    out = Dense(11, activation = 'softmax', name = 'out')(x)\n","    \n","    model = models.Model(inputs = inp, outputs = out)\n","    \n","    opt = Adam(lr = LR)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(loss=losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"91lX-JW5PFh5","colab_type":"code","colab":{}},"source":["# a = np.random.rand(1,2,3)\n","# print(a)\n","# b = a.reshape((-1,2,3,1))\n","# print(b)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLrqe5Fc7qpl","trusted":true,"colab_type":"code","colab":{}},"source":["def ClassifierCBRW(shape_):\n","    \n","    def cbr(x, out_layer, kernel, stride, dilation):\n","        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n","        x = BatchNormalization()(x)\n","        x = Activation(\"relu\")(x)\n","        return x\n","    \n","    def wave_block(x, filters, kernel_size, n):\n","        dilation_rates = [2**i for i in range(n)]\n","        x = Conv1D(filters = filters,\n","                   kernel_size = 1,\n","                   padding = 'same')(x)\n","        res_x = x\n","        for dilation_rate in dilation_rates:\n","            tanh_out = Conv1D(filters = filters,\n","                              kernel_size = kernel_size,\n","                              padding = 'same', \n","                              activation = 'tanh', \n","                              dilation_rate = dilation_rate)(x)\n","            sigm_out = Conv1D(filters = filters,\n","                              kernel_size = kernel_size,\n","                              padding = 'same',\n","                              activation = 'sigmoid', \n","                              dilation_rate = dilation_rate)(x)\n","            x = Multiply()([tanh_out, sigm_out])\n","            x = Conv1D(filters = filters,\n","                       kernel_size = 1,\n","                       padding = 'same')(x)\n","            res_x = Add()([res_x, x])\n","        return res_x\n","    \n","    inp = Input(shape = (shape_))\n","    x = cbr(inp, 64, 7, 1, 1)\n","    x = BatchNormalization()(x)\n","    x = wave_block(x, 16, 3, 12)\n","    x = BatchNormalization()(x)\n","    x = wave_block(x, 32, 3, 8)\n","    x = BatchNormalization()(x)\n","    x = wave_block(x, 64, 3, 4)\n","    x = BatchNormalization()(x)\n","    x = wave_block(x, 128, 3, 1)\n","    x = cbr(x, 32, 7, 1, 1)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.2)(x)\n","    out = Dense(11, activation = 'softmax', name = 'out')(x)\n","    \n","    model = models.Model(inputs = inp, outputs = out)\n","    \n","    opt = Adam(lr = LR)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(loss=losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zt1qkf9Lsp0W","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OH0upYDmmbIp","colab_type":"code","colab":{}},"source":["# def ClassifierCBRWmulti(shape_):\n","    \n","#     def cbr(x, out_layer, kernel, stride, dilation):\n","#         x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n","#         x = BatchNormalization()(x)\n","#         x = Activation(\"relu\")(x)\n","#         return x\n","    \n","#     def wave_block(x, filters, kernel_size, n):\n","#         dilation_rates = [2**i for i in range(n)]\n","#         x = Conv1D(filters = filters,\n","#                    kernel_size = 1,\n","#                    padding = 'same')(x)\n","#         res_x = x\n","#         for dilation_rate in dilation_rates:\n","#             tanh_out = Conv1D(filters = filters,\n","#                               kernel_size = kernel_size,\n","#                               padding = 'same', \n","#                               activation = 'tanh', \n","#                               dilation_rate = dilation_rate)(x)\n","#             sigm_out = Conv1D(filters = filters,\n","#                               kernel_size = kernel_size,\n","#                               padding = 'same',\n","#                               activation = 'sigmoid', \n","#                               dilation_rate = dilation_rate)(x)\n","#             x = Multiply()([tanh_out, sigm_out])\n","#             x = Conv1D(filters = filters,\n","#                        kernel_size = 1,\n","#                        padding = 'same')(x)\n","#             res_x = Add()([res_x, x])\n","#         return res_x\n","    \n","#     def channelPool(x):\n","#         return K.max(x,axis=-1)   \n","\n","#     def wave_process(inp, name):\n","#         x = cbr(inp, 64, 7, 1, 1)\n","#         x = BatchNormalization()(x)\n","#         x = wave_block(x, 16, 3, 12)\n","#         x = BatchNormalization()(x)\n","#         x = wave_block(x, 32, 3, 8)\n","#         x = BatchNormalization()(x)\n","#         x = wave_block(x, 64, 3, 4)\n","#         x = BatchNormalization()(x)\n","#         x = wave_block(x, 128, 3, 1)\n","#         x = cbr(x, 32, 7, 1, 1)\n","#         x = BatchNormalization()(x)\n","#         x = Dropout(0.2)(x)\n","#         x = Dense(11, activation = 'softmax', name = name)(x)\n","#         return x\n","#     print(\"shape_:\", shape_)\n","#     inp = Input(shape = (shape_))\n","#     out1 = wave_process(inp, 'out1')\n","#     out1 = K.reshape(out1, (-1, shape_[0], 11, 1))\n","\n","#     inp2 = K.reshape(inp, (-1, shape_[0]//2, shape_[1]))\n","#     out2 = wave_process(inp2, 'out2')\n","#     out2 = K.reshape(out2, (-1, shape_[0], 11, 1))\n","    \n","#     inp3 = K.reshape(inp, (-1, shape_[0]//4, shape_[1]))\n","#     out3 = wave_process(inp3, 'out3')\n","#     out3 = K.reshape(out3, (-1, shape_[0], 11, 1))\n","\n","#     inp4 = K.reshape(inp, (-1, shape_[0]//8, shape_[1]))\n","#     out4 = wave_process(inp4, 'out4')\n","#     out4 = K.reshape(out4, (-1, shape_[0], 11, 1))\n","\n","#     out = Concatenate()([out1, out2, out3, out4])\n","#     out = Dense(11, activation = 'softmax', name = 'out')(out)\n","#     out = Lambda(channelPool)(out)\n","#     out = K.reshape(out, (-1, shape_[0], 11))\n","#     out = Activation('softmax')(out)\n","#     model = models.Model(inputs = inp, outputs = out)\n","    \n","#     opt = Adam(lr = LR)\n","#     opt = tfa.optimizers.SWA(opt)\n","#     model.compile(loss=losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n","#     return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDkxS_gC9rzc","colab_type":"code","colab":{}},"source":["def ClassifierCBRWmulti(shape_):\n","    \n","    def cbr(x, out_layer, kernel, stride, dilation):\n","        x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n","        x = BatchNormalization()(x)\n","        x = Activation(\"relu\")(x)\n","        return x\n","    \n","    def wave_block(x, filters, kernel_size, n):\n","        dilation_rates = [2**i for i in range(n)]\n","        x = Conv1D(filters = filters,\n","                   kernel_size = 1,\n","                   padding = 'same')(x)\n","        res_x = x\n","        for dilation_rate in dilation_rates:\n","            tanh_out = Conv1D(filters = filters,\n","                              kernel_size = kernel_size,\n","                              padding = 'same', \n","                              activation = 'tanh', \n","                              dilation_rate = dilation_rate)(x)\n","            sigm_out = Conv1D(filters = filters,\n","                              kernel_size = kernel_size,\n","                              padding = 'same',\n","                              activation = 'sigmoid', \n","                              dilation_rate = dilation_rate)(x)\n","            x = Multiply()([tanh_out, sigm_out])\n","            x = Conv1D(filters = filters,\n","                       kernel_size = 1,\n","                       padding = 'same')(x)\n","            res_x = Add()([res_x, x])\n","        return res_x\n","    \n","    def channelPool(x):\n","        return K.max(x,axis=-1)   \n","\n","    def wave_process(inp, name):\n","        x = cbr(inp, 64, 7, 1, 1)\n","        x = BatchNormalization()(x)\n","        x = wave_block(x, 16, 3, 12)\n","        x = BatchNormalization()(x)\n","        x = wave_block(x, 32, 3, 8)\n","        x = BatchNormalization()(x)\n","        x = wave_block(x, 64, 3, 4)\n","        x = BatchNormalization()(x)\n","        x = wave_block(x, 128, 3, 1)\n","        x = cbr(x, 32, 7, 1, 1)\n","        x = BatchNormalization()(x)\n","        x = Dropout(0.2)(x)\n","        x = Dense(11, activation = 'softmax', name = name)(x)\n","        return x\n","    print(\"shape_:\", shape_)\n","    inp = Input(shape = (shape_))\n","    out1 = wave_process(inp, 'out1')\n","\n","    inp2 = K.reshape(inp, (-1, shape_[0]//2, shape_[1]))\n","    out2 = wave_process(inp2, 'out2')\n","    out2 = K.reshape(out2, (-1, shape_[0], 11))\n","    \n","    # inp3 = K.reshape(inp, (-1, shape_[0]//4, shape_[1]))\n","    # out3 = wave_process(inp3, 'out3')\n","    # out3 = K.reshape(out3, (-1, shape_[0], 11))\n","\n","    # inp4 = K.reshape(inp, (-1, shape_[0]//8, shape_[1]))\n","    # out4 = wave_process(inp4, 'out4')\n","    # out4 = K.reshape(out4, (-1, shape_[0], 11))\n","\n","    out = Concatenate()([out1, out2])\n","    out = Dense(11, activation = 'softmax', name = 'out')(out)\n","    model = models.Model(inputs = inp, outputs = out)\n","    \n","    opt = Adam(lr = LR)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(loss=losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.579491Z","start_time":"2020-04-11T02:14:24.567764Z"},"trusted":true,"id":"L4cQ0L8ucOgn","colab_type":"code","colab":{}},"source":["\n","class MacroF1ES(Callback):\n","    def __init__(self, model, inputs, targets, \n","                 patience=5, delta=0, checkpoint_path='checkpoint.h5', is_maximize=True):\n","        \n","        self.model = model\n","        self.inputs = inputs\n","        self.targets = np.argmax(targets, axis=2).reshape(-1)\n","        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n","        self.counter, self.best_score = 0, None\n","        self.is_maximize = is_maximize\n","        self.stopped_epoch = 0\n","        \n","    def on_epoch_end(self, epoch, logs):\n","        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n","        score = f1_score(self.targets, pred, average=\"macro\")\n","        logger.info(f'\\n epoch:{epoch:03d}, F1Macro: {score:.5f}')   \n","        \n","        if self.best_score is None or \\\n","        (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n","            self.model.save_weights(self.checkpoint_path)\n","#             torch.save(model.state_dict(), self.checkpoint_path)\n","            self.best_score, self.counter = score, 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience: ##stop training\n","                self.stopped_epoch = epoch\n","                self.model.stop_training = True\n","                \n","    def on_train_end(self, logs=None):\n","        if self.stopped_epoch > 0:\n","              logger.info('Epoch %05d: early stopping' % (self.stopped_epoch + 1))   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.589759Z","start_time":"2020-04-11T02:14:24.582640Z"},"id":"1yfpldH_2ucw","trusted":true,"colab_type":"code","colab":{}},"source":["class MacroF1(Callback):\n","    def __init__(self, model, inputs, targets):\n","        self.model = model\n","        self.inputs = inputs\n","        self.targets = np.argmax(targets, axis=2).reshape(-1)\n","\n","    def on_epoch_end(self, epoch, logs):\n","        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n","        score = f1_score(self.targets, pred, average=\"macro\")\n","        print(f' F1Macro: {score:.5f}')    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.596693Z","start_time":"2020-04-11T02:14:24.591931Z"},"id":"skru4lPt2uc6","trusted":true,"colab_type":"code","colab":{}},"source":["def normalize(train, test):\n","    \n","    train_input_mean = train.signal.mean()\n","    train_input_sigma = train.signal.std()\n","    train['signal'] = (train.signal-train_input_mean)/train_input_sigma\n","    test['signal'] = (test.signal-train_input_mean)/train_input_sigma\n","\n","    return train, test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.609602Z","start_time":"2020-04-11T02:14:24.600166Z"},"id":"02TDJtejfS0_","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def run_everything(fe_config : List) -> NoReturn:\n","    not_feats_cols = ['time']\n","    target_col = ['open_channels']\n","    init_logger()\n","    with timer(f'Reading Data'):\n","        logger.info('Reading Darta Started ...')\n","        base = os.path.abspath(PATH+'liverpool-ion-switching/')\n","        train, test, sample_submission = read_data(base)\n","        train, test = normalize(train, test)    \n","        logger.info('Reading and Normalizing Data Completed ...')\n","    with timer(f'Creating Features'):\n","        logger.info('Feature Enginnering Started ...')\n","        for config in fe_config:\n","            train = run_feat_enginnering(train, create_all_data_feats=config[0], batch_size=config[1])\n","            test  = run_feat_enginnering(test,  create_all_data_feats=config[0], batch_size=config[1])\n","        train, test = add_category(train, test)\n","        train, test, feats = feature_selection(train, test)\n","        train = reduce_mem_usage(train)\n","        test = reduce_mem_usage(test)\n","        logger.info('Feature Enginnering Completed ...')\n","\n","    with timer(f'Running Wavenet model'):\n","        logger.info(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started ...')\n","        run_cv_model_by_batch(train, test, splits=SPLITS, batch_col='group', feats=feats, \n","                              sample_submission=sample_submission, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n","        logger.info(f'Training completed ...')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:15:53.901793Z","start_time":"2020-04-11T02:14:24.612467Z"},"id":"_w4_OJJtfS1K","trusted":true,"colab_type":"code","outputId":"b6ebc774-7014-482a-fff3-8cc97db5c5aa","executionInfo":{"status":"ok","timestamp":1589424117995,"user_tz":420,"elapsed":634846,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["run_everything(fe_config)"],"execution_count":61,"outputs":[{"output_type":"stream","text":["2020-05-14 02:31:31,043 INFO Reading Darta Started ...\n","2020-05-14 02:31:33,585 INFO Reading and Normalizing Data Completed ...\n","2020-05-14 02:31:33,589 INFO [Reading Data] done in 3 s\n","2020-05-14 02:31:33,593 INFO Feature Enginnering Started ...\n"],"name":"stderr"},{"output_type":"stream","text":["['signal', 'signal_shift_pos_1', 'signal_shift_neg_1', 'signal_shift_pos_2', 'signal_shift_neg_2', 'signal_shift_pos_3', 'signal_shift_neg_3', 'signal_2', 'category']\n","Mem. usage decreased to 133.51 MB (65.9 % reduction)\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-14 02:31:39,434 INFO Feature Enginnering Completed ...\n","2020-05-14 02:31:39,435 INFO [Creating Features] done in 6 s\n","2020-05-14 02:31:39,439 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n"],"name":"stderr"},{"output_type":"stream","text":["Mem. usage decreased to 45.78 MB (69.2 % reduction)\n","ion_switch/res/wavenet-dlr-res/ClassifierCBRWaug-Prob11-NESCAL/all\n","5000000 5\n","     time    signal  open_channels  group  category\n","0  0.0001 -1.148438              0      0         0\n","1  0.0002 -1.183594              0      0         0\n","2  0.0003 -1.012695              0      0         0\n","3  0.0004 -1.297852              0      0         0\n","4  0.0005 -1.303711              0      0         0\n","weights_: [ 0.36652399  0.46106257  0.82059173  0.67983748  1.12675802  1.63577934\n","  2.41635544  1.71516878  1.85390282  3.33929955 12.72060713]\n","nums_: 0     1240152\n","1      985865\n","2      553924\n","3      668609\n","4      403410\n","5      277877\n","6      188112\n","7      265015\n","8      245183\n","9      136120\n","10      35733\n","Name: open_channels, dtype: int64\n","cb_weights_: [0.97174558 0.97174558 0.97174558 0.97174558 0.97174567 0.97176004\n"," 0.97227024 0.97176977 0.97179906 0.97596    1.27771292]\n","(1250, 4000, 11)\n","(1250, 4000, 9)\n","Loading RF model:  ion_switch/res/RFC/RF100-D12/rf-f0-tn100-dn12.pkl\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   18.1s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   47.2s finished\n","[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    8.4s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   22.3s finished\n","[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    4.7s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   12.1s finished\n"],"name":"stderr"},{"output_type":"stream","text":["Mem. usage decreased to 167.85 MB (61.4 % reduction)\n","Mem. usage decreased to 61.04 MB (68.6 % reduction)\n","Mem. usage decreased to 41.96 MB (61.4 % reduction)\n","train_x shape 1:  (1000, 4000, 9)\n","train_x shape 2:  (1000, 4000, 20)\n","X_aug shape 1: (1000, 4000, 20)\n","X_aug shape 2: (1000, 4000, 21)\n","unique train_x[8]: [0. 1.]\n","test shape after roll diff: (500, 4000, 21)\n","train_x shape 3:  (2000, 4000, 21)\n","train_y shape 3:  (2000, 4000, 11)\n","model initilization done!\n","preds_f shape: (250, 4000, 11)\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-14 02:33:44,686 INFO Training fold 1 completed. macro f1 score : 0.94278\n"],"name":"stderr"},{"output_type":"stream","text":["Loading RF model:  ion_switch/res/RFC/RF100-D12/rf-f1-tn100-dn12.pkl\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   18.2s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   47.1s finished\n","[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    8.2s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   21.8s finished\n","[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    4.5s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   11.8s finished\n"],"name":"stderr"},{"output_type":"stream","text":["Mem. usage decreased to 167.85 MB (61.4 % reduction)\n","Mem. usage decreased to 61.04 MB (68.6 % reduction)\n","Mem. usage decreased to 41.96 MB (61.4 % reduction)\n","train_x shape 1:  (1000, 4000, 9)\n","train_x shape 2:  (1000, 4000, 20)\n","X_aug shape 1: (1000, 4000, 20)\n","X_aug shape 2: (1000, 4000, 21)\n","unique train_x[8]: [0. 1.]\n","test shape after roll diff: (500, 4000, 21)\n","train_x shape 3:  (2000, 4000, 21)\n","train_y shape 3:  (2000, 4000, 11)\n","model initilization done!\n","preds_f shape: (250, 4000, 11)\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-14 02:35:41,869 INFO Training fold 2 completed. macro f1 score : 0.94368\n"],"name":"stderr"},{"output_type":"stream","text":["Loading RF model:  ion_switch/res/RFC/RF100-D12/rf-f2-tn100-dn12.pkl\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   19.2s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   49.6s finished\n","[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    8.6s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   22.4s finished\n","[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    4.7s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   12.3s finished\n"],"name":"stderr"},{"output_type":"stream","text":["Mem. usage decreased to 167.85 MB (61.4 % reduction)\n","Mem. usage decreased to 61.04 MB (68.6 % reduction)\n","Mem. usage decreased to 41.96 MB (61.4 % reduction)\n","train_x shape 1:  (1000, 4000, 9)\n","train_x shape 2:  (1000, 4000, 20)\n","X_aug shape 1: (1000, 4000, 20)\n","X_aug shape 2: (1000, 4000, 21)\n","unique train_x[8]: [0. 1.]\n","test shape after roll diff: (500, 4000, 21)\n","train_x shape 3:  (2000, 4000, 21)\n","train_y shape 3:  (2000, 4000, 11)\n","model initilization done!\n","preds_f shape: (250, 4000, 11)\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-14 02:37:43,345 INFO Training fold 3 completed. macro f1 score : 0.94188\n"],"name":"stderr"},{"output_type":"stream","text":["Loading RF model:  ion_switch/res/RFC/RF100-D12/rf-f3-tn100-dn12.pkl\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   19.3s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   50.9s finished\n","[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    8.8s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   22.8s finished\n","[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    4.9s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   12.7s finished\n"],"name":"stderr"},{"output_type":"stream","text":["Mem. usage decreased to 167.85 MB (61.4 % reduction)\n","Mem. usage decreased to 61.04 MB (68.6 % reduction)\n","Mem. usage decreased to 41.96 MB (61.4 % reduction)\n","train_x shape 1:  (1000, 4000, 9)\n","train_x shape 2:  (1000, 4000, 20)\n","X_aug shape 1: (1000, 4000, 20)\n","X_aug shape 2: (1000, 4000, 21)\n","unique train_x[8]: [0. 1.]\n","test shape after roll diff: (500, 4000, 21)\n","train_x shape 3:  (2000, 4000, 21)\n","train_y shape 3:  (2000, 4000, 11)\n","model initilization done!\n","preds_f shape: (250, 4000, 11)\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-14 02:39:45,857 INFO Training fold 4 completed. macro f1 score : 0.94212\n"],"name":"stderr"},{"output_type":"stream","text":["Loading RF model:  ion_switch/res/RFC/RF100-D12/rf-f4-tn100-dn12.pkl\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   19.7s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   50.7s finished\n","[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    8.7s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   22.7s finished\n","[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    4.8s\n","[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   12.6s finished\n"],"name":"stderr"},{"output_type":"stream","text":["Mem. usage decreased to 167.85 MB (61.4 % reduction)\n","Mem. usage decreased to 61.04 MB (68.6 % reduction)\n","Mem. usage decreased to 41.96 MB (61.4 % reduction)\n","train_x shape 1:  (1000, 4000, 9)\n","train_x shape 2:  (1000, 4000, 20)\n","X_aug shape 1: (1000, 4000, 20)\n","X_aug shape 2: (1000, 4000, 21)\n","unique train_x[8]: [0. 1.]\n","test shape after roll diff: (500, 4000, 21)\n","train_x shape 3:  (2000, 4000, 21)\n","train_y shape 3:  (2000, 4000, 11)\n","model initilization done!\n","preds_f shape: (250, 4000, 11)\n"],"name":"stdout"},{"output_type":"stream","text":["2020-05-14 02:41:48,707 INFO Training fold 5 completed. macro f1 score : 0.94236\n","2020-05-14 02:41:52,002 INFO Training completed. oof macro f1 score : 0.94259\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>time</th>\n","      <th>open_channels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>500.000092</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>500.000214</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>500.000305</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>500.000397</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>500.000488</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         time  open_channels\n","0  500.000092              0\n","1  500.000214              0\n","2  500.000305              0\n","3  500.000397              0\n","4  500.000488              0"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2020-05-14 02:41:57,486 INFO Training completed ...\n","2020-05-14 02:41:57,488 INFO [Running Wavenet model] done in 618 s\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2CMgZCYC5Yh6","trusted":true,"colab_type":"code","outputId":"e0a2a05a-b01e-4c29-c3aa-63f2786394be","executionInfo":{"status":"ok","timestamp":1589424117996,"user_tz":420,"elapsed":634822,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"test\")"],"execution_count":62,"outputs":[{"output_type":"stream","text":["test\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SrDMsCg1CxUV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-u3pTgK3mNFt","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjHu5CkUrorQ","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZHizylS_pMhI","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"EbDSqah3O6Q4","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIkAcnCRRN-O","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}