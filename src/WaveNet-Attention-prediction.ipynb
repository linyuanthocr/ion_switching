{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"WaveNet-Attention-prediction.ipynb","provenance":[{"file_id":"1Jv4Ky42oIyr50jBI7BtD1UcxXmwsXCB_","timestamp":1587314730837},{"file_id":"1bDQMNN2UAzXO6ehBVBbw3AuqzJ80GN8W","timestamp":1587170640284},{"file_id":"1nySJhqwOqrxx0VKGVInFmFz7y4hbX1IX","timestamp":1587148513193},{"file_id":"1A5WIcpZZ9xduSmZnjMBsGs9uAGbSpdop","timestamp":1587065256025},{"file_id":"1zzX2uECCSZ07CdB1zEn3Jde9U4aNPaRc","timestamp":1586811806520},{"file_id":"1Irja7LH63cq-nyiKtCzQnKbCnmN9o_10","timestamp":1586796690140}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"huixWaL42uZi","colab_type":"text"},"source":["The validation scheme is based on [seq2seq-rnn-with-gru](https://www.kaggle.com/brandenkmurray/seq2seq-rnn-with-gru/output), and cleaned data is from [data-without-drift](https://www.kaggle.com/cdeotte/data-without-drift) and Kalman filter is from [https://www.kaggle.com/teejmahal20/single-model-lgbm-kalman-filter](single-model-lgbm-kalman-filter) and the added feature is from [wavenet-with-1-more-feature](wavenet-with-1-more-feature). I also used ragnar's data in this version [clean-kalman](https://www.kaggle.com/ragnar123/clean-kalman). The Wavenet is based on [https://github.com/philipperemy/keras-tcn](https://github.com/philipperemy/keras-tcn), [https://github.com/peustr/wavenet](https://github.com/peustr/wavenet) and [https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet) and also [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm). If any refrence is not mentioned it was not intentional, please add them in comments.\n","\n","Previous versions were mainly based on [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm)  "]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.121605Z","start_time":"2020-04-11T02:14:22.792317Z"},"_kg_hide-input":true,"id":"LqmWjeYJ2uZn","trusted":true,"colab_type":"code","colab":{}},"source":["!pip install --no-warn-conflicts -q tensorflow-addons"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.152214Z","start_time":"2020-04-11T02:14:24.125295Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"y1qOuodBfSxN","trusted":true,"colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import (TimeDistributed, Dropout, BatchNormalization, Flatten, Convolution1D, Activation, Input, Dense, LSTM, Lambda, Bidirectional,\n","                                     Add, AveragePooling1D, Multiply, GRU, GRUCell, LSTMCell, SimpleRNNCell, SimpleRNN, TimeDistributed, RNN,\n","                                     RepeatVector, Conv1D, MaxPooling1D, Concatenate, GlobalAveragePooling1D, UpSampling1D,SpatialDropout1D)\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler, CSVLogger\n","from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy, mean_squared_error\n","# from tensorflow.keras.experimental import export_saved_model, load_from_saved_model\n","from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n","from tensorflow.keras.utils import Sequence, to_categorical\n","from tensorflow.keras import losses, models, optimizers\n","from tensorflow.keras import backend as K\n","import tensorflow as tf\n","from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n","from sklearn.metrics import f1_score, cohen_kappa_score, mean_squared_error\n","from logging import getLogger, Formatter, StreamHandler, FileHandler, INFO, DEBUG, NOTSET\n","from sklearn.model_selection import KFold, GroupKFold\n","from tqdm import tqdm_notebook as tqdm\n","from contextlib import contextmanager\n","from joblib import Parallel, delayed\n","from IPython.display import display\n","from sklearn import preprocessing\n","from sklearn.utils import class_weight\n","import tensorflow_addons as tfa\n","import scipy.stats as stats\n","import random as rn\n","import pandas as pd\n","import numpy as np\n","import scipy as sp\n","import itertools\n","import warnings\n","import time\n","import pywt\n","import os\n","import gc\n","\n","from tensorflow.keras.metrics import Precision, Recall\n","# from tensorflow_addons.metrics import F1Score\n","\n","warnings.simplefilter('ignore')\n","warnings.filterwarnings('ignore')\n","pd.set_option('display.max_columns', 1000)\n","pd.set_option('display.max_rows', 500)\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LC8XqE14cSRm","colab_type":"code","outputId":"102069ec-ed10-436b-9162-7e974d97633b","executionInfo":{"status":"ok","timestamp":1587327823850,"user_tz":420,"elapsed":37964,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":853}},"source":["import os, sys\n","from google.colab import drive\n","from pathlib import Path\n","drive.mount('/content/drive')\n","\n","path = \"/content/drive/My Drive\"\n","\n","os.chdir(path)\n","os.listdir(path)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['Smart and Connected Everything_LTO2015_April 2 2014.pptx',\n"," '无标题绘图.gdraw',\n"," 'Smart and Connected Everything_LTO2015_April 2 2014.pptx.gslides',\n"," 'Class mytest.gdoc',\n"," '简历.gdoc',\n"," '2018.10.14.pdf',\n"," 'Learning notebook',\n"," 'Verification',\n"," 'da6ca21e-d8e3-4f08-816e-da2ac02f192f.pdf',\n"," 'Colab Notebooks',\n"," 'Course1FinalProject',\n"," 'CARLA',\n"," 'resulting_data.csv.gsheet',\n"," 'Patents.gdoc',\n"," 'Coursera_Self-Driving Cars_Toronto',\n"," 'Python programming',\n"," 'Tensorflow_CS20SI',\n"," 'Paper reading',\n"," '无标题文档 (2).gdoc',\n"," 'forms for H4.pdf',\n"," 'forms for H4.gdoc',\n"," '无标题文档 (1).gdoc',\n"," 'SVO.gdoc',\n"," '无标题文档.gdoc',\n"," 'YuanLin_CV_update2.pdf',\n"," 'YuanLin_CV.pdf',\n"," 'CV.gdoc',\n"," '博士论文_HC.doc',\n"," '博士论文_HC.pdf',\n"," 'models',\n"," 'CenterNetMask2-Aug-DrLr.ipynb',\n"," 'predictions-b4augmaskonelr40th-0.85.csv',\n"," 'setup.ipynb',\n"," 'CES2020 - VE Tech Forum.key',\n"," 'log-64-1e-3-aug.csv',\n"," 'log-1e-4-128-aug.csv',\n"," 'log.csv',\n"," 'tmp',\n"," 'ion_switch',\n"," 'predictions',\n"," 'CSDN',\n"," 'Untitled',\n"," 'training.log']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"M-sYfdXdWyTI","colab_type":"code","colab":{}},"source":["# sys.path.append('ion_switch/keras-one-cycle')\n","# # os.listdir(patholr)\n","# from clr import OneCycleLR"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Og8glrlAs6SZ","colab_type":"code","colab":{}},"source":["# a = np.random.rand(3,4,5)\n","# print(a)\n","# a = a/np.sum(a, axis = 2, keepdims=True)\n","# print(\"Normalized A:\")\n","# print(a)\n","# c = np.max(a, axis=1)\n","# print(\"C \")\n","# print(c)\n","# b = np.argmax(c, axis=1)\n","# print(\"RES\")\n","# print(b)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.159766Z","start_time":"2020-04-11T02:14:24.155918Z"},"trusted":true,"id":"zdosugVWcOf0","colab_type":"code","colab":{}},"source":["# PATH = '/kaggle/input/'\n","# PATH = '/Users/helen/Desktop/Data/'\n","PATH = 'ion_switch/'\n","outdir = Path(PATH+'res')\n","if not os.path.exists(outdir):\n","    os.mkdir(outdir)\n","outdir = Path(PATH+'res/wavenet-attention-v3')\n","if not os.path.exists(outdir):\n","    os.mkdir(outdir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.168634Z","start_time":"2020-04-11T02:14:24.163612Z"},"id":"UafJMtyefSxU","trusted":true,"colab_type":"code","colab":{}},"source":["EPOCHS=120\n","# NNBATCHSIZE=20\n","# BATCHSIZE = 4000\n","NNBATCHSIZE= 32\n","BATCHSIZE = 1000\n","SEED = 321\n","SELECT = True\n","SPLITS = 5\n","LR = 0.001\n","Prediction = True\n","\n","use_average = True\n","weight_exp = -0.01\n","add_weights = False and (not use_average)\n","\n","timestampStr = ''\n","fe_config = [\n","    (True, BATCHSIZE),\n","]\n","\n","COMPETITION = 'ION-Switching'\n","logger = getLogger(COMPETITION)\n","LOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\n","MODELNAME = 'WaveNet-TimeDense-DP-CLR-FOCALLOSS-no2'\n","if add_weights:\n","  VERSION = '{}_len{}_lr{}_exp{}'.format(MODELNAME, BATCHSIZE, LR, weight_exp)\n","else:\n","  VERSION = '{}_len{}_lr{}'.format(MODELNAME, BATCHSIZE, LR)\n","if not Prediction:\n","  outdir = os.path.join(outdir, VERSION)\n","  if not os.path.exists(outdir):\n","      os.mkdir(outdir)\n","\n","from datetime import datetime\n","\n","if Prediction:\n","    outdir = os.path.join(outdir, timestampStr)\n","else:\n","  dateTimeObj = datetime.now()\n","  timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H_%M_%S\")\n","  outdir = os.path.join(outdir, timestampStr)\n","  if not os.path.exists(outdir):\n","      os.mkdir(outdir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ht7DVPk0_Nf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o47ttF9R1OFw","colab_type":"code","outputId":"8654a768-43a2-4110-baad-640d71fd539f","executionInfo":{"status":"ok","timestamp":1587327824333,"user_tz":420,"elapsed":38421,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["aflag = False\n","a = '{}'.format('test' if aflag else 'train')\n","print(a)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fouKVpNaFNKQ","colab_type":"code","colab":{}},"source":["\n","@contextmanager\n","def timer(name : Text):\n","    t0 = time.time()\n","    yield\n","    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.177661Z","start_time":"2020-04-11T02:14:24.171732Z"},"id":"EE4v8h1tfSxb","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def init_logger():\n","\n","    handler = StreamHandler()\n","    handler.setLevel(INFO)\n","    handler.setFormatter(Formatter(LOGFORMAT))\n","    fh_handler = FileHandler(os.path.join(outdir,'{}-len{}-lr{}-{}-{}.log'.format(MODELNAME,BATCHSIZE,LR,timestampStr,\n","                                                                                  (\"prediction\" if Prediction else \"train\"))))\n","    fh_handler.setFormatter(Formatter(LOGFORMAT))\n","    logger.setLevel(INFO)\n","    logger.addHandler(handler)\n","    logger.addHandler(fh_handler)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.358957Z","start_time":"2020-04-11T02:14:24.187096Z"},"id":"OC5DOcDifSxx","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def seed_everything(seed : int) -> NoReturn :\n","    \n","    rn.seed(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    tf.random.set_seed(seed)\n","    # os.environ['TF_CUDNN_DETERMINISTIC'] = str(seed) \n","\n","seed_everything(SEED)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5tInVDxBn-lQ","colab_type":"code","colab":{}},"source":["class CyclicLR(tf.keras.callbacks.Callback):\n","\n","    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n","                 gamma=1., scale_fn=None, scale_mode='cycle'):\n","        super(CyclicLR, self).__init__()\n","\n","        self.base_lr = base_lr\n","        self.max_lr = max_lr\n","        self.step_size = step_size\n","        self.mode = mode\n","        self.gamma = gamma\n","        if scale_fn == None:\n","            if self.mode == 'triangular':\n","                self.scale_fn = lambda x: 1.\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'triangular2':\n","                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'exp_range':\n","                self.scale_fn = lambda x: gamma ** (x)\n","                self.scale_mode = 'iterations'\n","        else:\n","            self.scale_fn = scale_fn\n","            self.scale_mode = scale_mode\n","        self.clr_iterations = 0.\n","        self.trn_iterations = 0.\n","        self.history = {}\n","\n","        self._reset()\n","\n","    def _reset(self, new_base_lr=None, new_max_lr=None,\n","               new_step_size=None):\n","        \"\"\"Resets cycle iterations.\n","        Optional boundary/step size adjustment.\n","        \"\"\"\n","        if new_base_lr != None:\n","            self.base_lr = new_base_lr\n","        if new_max_lr != None:\n","            self.max_lr = new_max_lr\n","        if new_step_size != None:\n","            self.step_size = new_step_size\n","        self.clr_iterations = 0.\n","\n","    def clr(self):\n","        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n","        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n","        if self.scale_mode == 'cycle':\n","            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n","        else:\n","            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n","                self.clr_iterations)\n","\n","    def on_train_begin(self, logs={}):\n","        logs = logs or {}\n","\n","        if self.clr_iterations == 0:\n","            K.set_value(self.model.optimizer.lr, self.base_lr)\n","        else:\n","            K.set_value(self.model.optimizer.lr, self.clr())\n","\n","    def on_batch_end(self, epoch, logs=None):\n","\n","        logs = logs or {}\n","        self.trn_iterations += 1\n","        self.clr_iterations += 1\n","\n","        K.set_value(self.model.optimizer.lr, self.clr())\n","        # print(\"learning rate- self.model.optimizer.lr: \", self.model.optimizer.lr)\n","\n","    # def on_epoch_end(self, epoch, logs=None):\n","\n","    #     logs = logs or {}\n","    #     self.trn_iterations += 1\n","    #     self.clr_iterations += 1\n","\n","    #     K.set_value(self.model.optimizer.lr, self.clr())\n","    #     logger.info(f'epoch:{epoch:03d},'+str(K.eval(self.model.optimizer.lr)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmKgNg1Mmzrk","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","# 写一个LossHistory类，保存训练集的loss和acc\n","# 当然我也可以完全不这么做，可以直接使用model.fit()方法返回的 history对象去做\n","'''Callback有6个常用的方法，这里实现其中的四个\n","    def on_epoch_begin(self, epoch, logs=None):\n","    def on_epoch_end(self, epoch, logs=None):\n","    def on_batch_begin(self, batch, logs=None):\n","    def on_batch_end(self, batch, logs=None):\n","    def on_train_begin(self, logs=None):\n","    def on_train_end(self, logs=None):\n","'''\n","class LossHistory(Callback):  # 继承自Callback类\n"," \n","    '''\n","    在模型开始的时候定义四个属性，每一个属性都是字典类型，存储相对应的值和epoch\n","    '''\n","    def on_train_begin(self, logs={}):\n","        self.losses = {'batch':[], 'epoch':[]}\n","        self.accuracy = {'batch':[], 'epoch':[]}\n","        self.val_loss = {'batch':[], 'epoch':[]}\n","        self.val_acc = {'batch':[], 'epoch':[]}\n"," \n","    # 在每一个batch结束后记录相应的值\n","    def on_batch_end(self, batch, logs={}):\n","        self.losses['batch'].append(logs.get('loss'))\n","        self.accuracy['batch'].append(logs.get('accuracy'))\n","        self.val_loss['batch'].append(logs.get('val_loss'))\n","        self.val_acc['batch'].append(logs.get('val_accuracy'))\n","    \n","    # 在每一个epoch之后记录相应的值\n","    def on_epoch_end(self, epoch, logs={}):\n","        trloss, tracc, vloss, vacc = logs.get('loss'), logs.get('accuracy'), logs.get('val_loss'), logs.get('val_accuracy')\n","        self.losses['epoch'].append(trloss)\n","        self.accuracy['epoch'].append(tracc)\n","        self.val_loss['epoch'].append(vloss)\n","        self.val_acc['epoch'].append(vacc)\n","        logger.info(\"epoch:{:03d}, train_loss:{:1.5f}, train_acc:{:1.5f}, val_loss:{:1.5f}, val_acc:{:1.5f}\".format(epoch, \n","                                                                                                                trloss, tracc, vloss, vacc))\n"," \n","    def loss_plot(self, loss_type, pngname):\n","        '''\n","        loss_type：指的是 'epoch'或者是'batch'，分别表示是一个batch之后记录还是一个epoch之后记录\n","        '''\n","        iters = range(len(self.losses[loss_type]))\n","        plt.figure()\n","        # acc\n","        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n","        # loss\n","        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n","        if loss_type == 'epoch':\n","            # val_acc\n","            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n","            # val_loss\n","            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n","        plt.grid(True)\n","        plt.xlabel(loss_type)\n","        plt.ylabel('acc-loss')\n","        plt.legend(loc=\"upper right\")\n","        plt.savefig(pngname)\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.368249Z","start_time":"2020-04-11T02:14:24.362616Z"},"id":"adUHGQUTfSyA","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def read_data(base : os.path.abspath) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n","    \n","    train = pd.read_csv(PATH+'clean-kalman/train_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n","    test  = pd.read_csv(PATH+'clean-kalman/test_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32})\n","    sub  = pd.read_csv(PATH+'liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n","    \n","    return train, test, sub\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.376185Z","start_time":"2020-04-11T02:14:24.371687Z"},"id":"HpDaJQ5yfSyI","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def batching(df : pd.DataFrame,\n","             batch_size : int) -> pd.DataFrame :\n","    \n","    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n","    df['group'] = df['group'].astype(np.uint16)\n","        \n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.391087Z","start_time":"2020-04-11T02:14:24.378989Z"},"id":"iQxOYF3tfSyj","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def reduce_mem_usage(df: pd.DataFrame,\n","                     verbose: bool = True) -> pd.DataFrame:\n","    \n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2\n","\n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","\n","            if str(col_type)[:3] == 'int':\n","\n","                if (c_min > np.iinfo(np.int32).min\n","                      and c_max < np.iinfo(np.int32).max):\n","                    df[col] = df[col].astype(np.int32)\n","                elif (c_min > np.iinfo(np.int64).min\n","                      and c_max < np.iinfo(np.int64).max):\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                if (c_min > np.finfo(np.float16).min\n","                        and c_max < np.finfo(np.float16).max):\n","                    df[col] = df[col].astype(np.float16)\n","                elif (c_min > np.finfo(np.float32).min\n","                      and c_max < np.finfo(np.float32).max):\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    reduction = (start_mem - end_mem) / start_mem\n","\n","    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n","    if verbose:\n","        print(msg)\n","\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.402186Z","start_time":"2020-04-11T02:14:24.393787Z"},"id":"dytmNmL_fSzj","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def lag_with_pct_change(df : pd.DataFrame,\n","                        shift_sizes : Optional[List]=[1, 2],\n","                        add_pct_change : Optional[bool]=False,\n","                        add_pct_change_lag : Optional[bool]=False,\n","                        add_diff : Optional[bool]=False) -> pd.DataFrame:\n","    \n","    for shift_size in shift_sizes:    \n","        df['signal_shift_pos_'+str(shift_size)] = df.groupby('group')['signal'].shift(shift_size).fillna(0)\n","        df['signal_shift_neg_'+str(shift_size)] = df.groupby('group')['signal'].shift(-1*shift_size).fillna(0)\n","\n","    if add_pct_change:\n","        df['pct_change'] = df['signal'].pct_change()\n","        if add_pct_change_lag:\n","            for shift_size in shift_sizes:    \n","                df['pct_change_shift_pos_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(shift_size).fillna(0)\n","                df['pct_change_shift_neg_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(-1*shift_size).fillna(0)\n","    if add_diff:\n","        for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'group', 'index']]:\n","            df[c+'_msignal'] = df[c] - df['signal']\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.409381Z","start_time":"2020-04-11T02:14:24.404800Z"},"id":"m-ULWLF_fS0B","trusted":true,"colab_type":"code","colab":{}},"source":["def run_feat_enginnering(df : pd.DataFrame,\n","                         create_all_data_feats : bool,\n","                         batch_size : int) -> pd.DataFrame:\n","    \n","    df = batching(df, batch_size=batch_size)\n","    if create_all_data_feats:\n","        df = lag_with_pct_change(df, [1, 2, 3],  add_pct_change=False, add_pct_change_lag=False, add_diff=True)\n","    df['signal_2'] = df['signal'] ** 2\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.419991Z","start_time":"2020-04-11T02:14:24.412368Z"},"id":"6E87jcu2fS0O","trusted":true,"colab_type":"code","colab":{}},"source":["def feature_selection(df : pd.DataFrame,\n","                      df_test : pd.DataFrame) -> Tuple[pd.DataFrame , pd.DataFrame, List]:\n","    use_cols = [col for col in df.columns if col not in ['index','group', 'open_channels', 'time']]\n","    print(use_cols)\n","    df = df.replace([np.inf, -np.inf], np.nan)\n","    df_test = df_test.replace([np.inf, -np.inf], np.nan)\n","    for col in use_cols:\n","        col_mean = pd.concat([df[col], df_test[col]], axis=0).mean()\n","        df[col] = df[col].fillna(col_mean)\n","        df_test[col] = df_test[col].fillna(col_mean)\n","   \n","    gc.collect()\n","    return df, df_test, use_cols\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.427483Z","start_time":"2020-04-11T02:14:24.422535Z"},"trusted":true,"id":"Ft7_2jVZcOgQ","colab_type":"code","colab":{}},"source":["\n","def augment(X: np.array, y:np.array) -> Tuple[np.array, np.array]:\n","    \n","    X = np.vstack((X, np.flip(X, axis=1)))\n","    y = np.vstack((y, np.flip(y, axis=1)))\n","    \n","    return X, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.432964Z","start_time":"2020-04-11T02:14:24.430264Z"},"trusted":true,"id":"Y-nODxu7cOgS","colab_type":"code","colab":{}},"source":["# Add ops to save and restore all the variables.\n","# saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.438940Z","start_time":"2020-04-11T02:14:24.436211Z"},"trusted":true,"id":"VRzoz-eacOgU","colab_type":"code","colab":{}},"source":["# # %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:24:41.652529Z\",\"start_time\":\"2020-04-03T23:24:41.645025Z\"}}\n","# class EarlyStopping:\n","#     def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n","#         self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n","#         self.counter, self.best_score = 0, None\n","#         self.is_maximize = is_maximize\n","\n","#     def load_best_weights(self, sess):\n","#         saver.restore(sess, self.checkpoint_path)\n","\n","#     def __call__(self, score, sess):\n","#         if self.best_score is None or \\\n","#         (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n","#             saver.save(sess, self.checkpoint_path)\n","#             self.best_score, self.counter = score, 0\n","#         else:\n","#             self.counter += 1\n","#             if self.counter >= self.patience:\n","#                 return True\n","#         return False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6l0nc-llBdy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.477074Z","start_time":"2020-04-11T02:14:24.441269Z"},"code_folding":[],"id":"e4QulGxHfS0n","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def run_cv_model_by_batch(train : pd.DataFrame,\n","                          test : pd.DataFrame,\n","                          splits : int,\n","                          batch_col : Text,\n","                          feats : List,\n","                          sample_submission: pd.DataFrame,\n","                          nn_epochs : int,\n","                          nn_batch_size : int) -> NoReturn:\n","    seed_everything(SEED)\n","    K.clear_session()\n","    if not os.path.exists(outdir):\n","      os.mkdir(outdir)\n","\n","    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n","    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n","    tf.compat.v1.keras.backend.set_session(sess)\n","    oof_ = np.zeros((len(train), SPLITS, 11))\n","    preds_ = np.zeros((len(test), SPLITS, 11))\n","    target = ['open_channels']\n","    group = train['group']\n","    kf = GroupKFold(n_splits=5)\n","    splits = [x for x in kf.split(train, train[target], group)]\n","\n","    new_splits = []\n","    for sp in splits:\n","        new_split = []\n","        new_split.append(np.unique(group[sp[0]]))\n","        new_split.append(np.unique(group[sp[1]]))\n","        new_split.append(sp[1])    \n","        new_splits.append(new_split)\n","\n","    # Calculate the weights for each class so that we can balance the data\n","    weights_ = class_weight.compute_class_weight('balanced',\n","                                                np.unique(train.open_channels),\n","                                                train.open_channels)\n","    print(\"weights:\", weights_)\n","        \n","    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n","\n","    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n","    # print(tr.head())\n","    target_cols = ['target_'+str(i) for i in range(11)]\n","    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n","#     print(np.shape(train_tr))\n","    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n","#     print(np.shape(train))\n","    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n","#     print(np.shape(test))\n","\n","    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n","        sub_dir = os.path.join(outdir,\"{}_fold\".format(n_fold))\n","        # if n_fold == 2:\n","        #     continue\n","        if not os.path.exists(sub_dir):\n","            os.mkdir(sub_dir)\n","        \n","        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n","        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n","        \n","        # if n_fold < 2:\n","        train_x, train_y = augment(train_x, train_y)\n","\n","        gc.collect()\n","        shape_ = (None, train_x.shape[2])\n","        model = Classifierx(shape_)\n","        print(\"model initilization done!\")\n","        # save_checkpoint_path = os.path.join(sub_dir,'checkpoint-{}.h5'.format(n_fold))\n","        save_bestf1macro_path = os.path.join(sub_dir,'checkpoint.h5')\n","        if not Prediction:\n","            # cb_lr_schedule = LearningRateScheduler(lr_schedule)\n","            cb_clr = CyclicLR(base_lr=1e-7, max_lr = LR, step_size= int(1.0*(train.shape[0])/(nn_batch_size*4)) , \n","                              mode='exp_range', gamma=1.0, scale_fn=None, scale_mode='cycle')\n","            cb_prg = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,leave_overall_progress=False, \n","                                                  show_epoch_progress=False,show_overall_progress=True)\n","            # cb_csv_logger= CSVLogger(os.path.join(sub_dir,'res.csv'))\n","            # cb_history = LossHistory()  # 这里是使用自定义的Callback回调函数，当然本身fit函数也会返回一个history可供使用\n","            history = model.fit(train_x,train_y,\n","                      epochs=nn_epochs,\n","                      callbacks=[cb_prg, cb_clr, \n","                                MacroF1ES(model, valid_x, valid_y, patience=20, delta=0, \n","                                          checkpoint_path=save_checkpoint_path)],\n","                      batch_size=nn_batch_size,\n","                      verbose=1,\n","                      validation_data=(valid_x,valid_y))\n","            pd.DataFrame(history.history).to_csv(os.path.join(sub_dir,'{}-len{}-lr{}-{}-log.csv'.format(MODELNAME,BATCHSIZE,LR,timestampStr)), float_format='%.4f')\n","            # print('\\nhistory dict:', history.history)\n","            # cb_history.loss_plot('epoch',os.path.join(outdir,'{}_f{}.png'.format(VERSION, n_fold)))\n","            del model\n","        # model = tf.keras.models.load_model(save_checkpoint_path, custom_objects={'Activation': tf.keras.layers.Activation, \n","                                                                        # 'get_f1':get_f1, 'multi_category_focal_loss2_fixed':multi_category_focal_loss2})\n","        model.load_weights(save_bestf1macro_path)  \n","        preds_f = model.predict(valid_x)\n","        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  \n","                             np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n","        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n","        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n","        oof_[val_orig_idx,n_fold,:] = preds_f\n","        te_preds = model.predict(test)\n","        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n","        preds_ [:,n_fold]= te_preds\n","    \n","    predv = ''\n","    if use_average:\n","        predv = 'av'\n","        p0 = np.argmax(np.mean(oof_, axis = 1), axis = 1)\n","        p1 = np.argmax(np.mean(preds_, axis = 1), axis = 1)\n","         \n","    elif add_weights:\n","        predv = 'wmm'\n","        proba0 = np.max(oof_, axis=1)\n","        p0 = np.argmax(proba0, axis=1)\n","        proba1 = np.max(preds_, axis=1)\n","        p1 = np.argmax(proba1, axis=1)\n","\n","        EXP = weight_exp\n","        s = pd.Series(p0)\n","        vc = s.value_counts().sort_index()\n","        df = pd.DataFrame({'a':np.arange(11),'b':np.ones(11)})\n","        df.b = df.a.map(vc)\n","        df.fillna(df.b.min(),inplace=True)\n","        mat1 = np.diag(df.b.astype('float32')**EXP)\n","        print('mat1', mat1)\n","\n","        s = pd.Series(p1)\n","        vc = s.value_counts().sort_index()\n","        df = pd.DataFrame({'a':np.arange(11),'b':np.ones(11)})\n","        df.b = df.a.map(vc)\n","        df.fillna(df.b.min(),inplace=True)\n","        mat2 = np.diag(df.b.astype('float32')**EXP)\n","\n","        p0 = np.argmax( proba0.dot(mat1), axis=1)\n","        p1 = np.argmax( proba1.dot(mat2), axis=1)\n","    else:\n","        predv = 'mm'\n","        proba0 = np.max(oof_, axis=1)\n","        p0 = np.argmax(proba0, axis=1)\n","        proba1 = np.max(preds_, axis=1)\n","        p1 = np.argmax(proba1, axis=1)\n","\n","    f1_score_ =f1_score(np.argmax(train_tr, axis=2).reshape(-1),  p0, average = 'macro')\n","    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n","    sample_submission['open_channels'] = p1.astype(int)\n","    sample_submission.to_csv(os.path.join(outdir,'{}_pred-{}-all{}.csv'.format(VERSION, (\"prediction\" if Prediction else \"train\"), predv)), index=False, float_format='%.4f')\n","    display(sample_submission.head())\n","    # np.save(os.path.join(outdir,'oof.npy'), oof_)\n","    # np.save(os.path.join(outdir,'preds.npy'), preds_)\n","\n","    return \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2drz7lYE0wj9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.485692Z","start_time":"2020-04-11T02:14:24.479620Z"},"id":"dqt0VJAS2ucB","trusted":true,"colab_type":"code","colab":{}},"source":["def lr_schedule(epoch):\n","    if epoch < 40:\n","        lr = LR\n","    elif epoch < 50:\n","        lr = LR / 3\n","    elif epoch < 60:\n","        lr = LR / 6\n","    elif epoch < 75:\n","        lr = LR / 9\n","    elif epoch < 85:\n","        lr = LR / 12\n","    elif epoch < 100:\n","        lr = LR / 15\n","    else:\n","        lr = LR / 50\n","    return lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aaClcd8zJIyC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.499088Z","start_time":"2020-04-11T02:14:24.488223Z"},"id":"SjbrEQSa2ucN","trusted":true,"colab_type":"code","colab":{}},"source":["class Mish(tf.keras.layers.Layer):\n","\n","    def __init__(self, **kwargs):\n","        super(Mish, self).__init__(**kwargs)\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        return inputs * K.tanh(K.softplus(inputs))\n","\n","    def get_config(self):\n","        base_config = super(Mish, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","def mish(x):\n","\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n"," \n","from tensorflow.keras.utils import get_custom_objects\n","# from tensorflow.keras.layers import Activation\n","get_custom_objects().update({'mish': Activation(mish)})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.521416Z","start_time":"2020-04-11T02:14:24.502541Z"},"id":"dM8RiNfL2ucY","trusted":true,"colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras import initializers\n","from tensorflow.keras import regularizers\n","from tensorflow.keras import constraints\n","\n","class Attention(Layer):\n","    \"\"\"Multi-headed attention layer.\"\"\"\n","    \n","    def __init__(self, hidden_size, \n","                 num_heads = 8, \n","                 attention_dropout=.1,\n","                 trainable=True,\n","                 name='Attention'):\n","        \n","        if hidden_size % num_heads != 0:\n","            raise ValueError(\"Hidden size must be evenly divisible by the number of heads.\")\n","            \n","        self.hidden_size = hidden_size\n","        self.num_heads = num_heads\n","        self.trainable = trainable\n","        self.attention_dropout = attention_dropout\n","        self.dense = tf.keras.layers.Dense(self.hidden_size, use_bias=False)\n","        super(Attention, self).__init__(name=name)\n","\n","    def split_heads(self, x):\n","        \"\"\"Split x into different heads, and transpose the resulting value.\n","        The tensor is transposed to insure the inner dimensions hold the correct\n","        values during the matrix multiplication.\n","        Args:\n","          x: A tensor with shape [batch_size, length, hidden_size]\n","        Returns:\n","          A tensor with shape [batch_size, num_heads, length, hidden_size/num_heads]\n","        \"\"\"\n","        with tf.name_scope(\"split_heads\"):\n","            batch_size = tf.shape(x)[0]\n","            length = tf.shape(x)[1]\n","\n","            # Calculate depth of last dimension after it has been split.\n","            depth = (self.hidden_size // self.num_heads)\n","\n","            # Split the last dimension\n","            x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n","\n","            # Transpose the result\n","            return tf.transpose(x, [0, 2, 1, 3])\n","    \n","    def combine_heads(self, x):\n","        \"\"\"Combine tensor that has been split.\n","        Args:\n","          x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n","        Returns:\n","          A tensor with shape [batch_size, length, hidden_size]\n","        \"\"\"\n","        with tf.name_scope(\"combine_heads\"):\n","            batch_size = tf.shape(x)[0]\n","            length = tf.shape(x)[2]\n","            x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n","            return tf.reshape(x, [batch_size, length, self.hidden_size])        \n","\n","    def call(self, inputs):\n","        \"\"\"Apply attention mechanism to inputs.\n","        Args:\n","          inputs: a tensor with shape [batch_size, length_x, hidden_size]\n","        Returns:\n","          Attention layer output with shape [batch_size, length_x, hidden_size]\n","        \"\"\"\n","        # Google developper use tf.layer.Dense to linearly project the queries, keys, and values.\n","        q = self.dense(inputs)\n","        k = self.dense(inputs)\n","        v = self.dense(inputs)\n","\n","        q = self.split_heads(q)\n","        k = self.split_heads(k)\n","        v = self.split_heads(v)\n","        \n","        # Scale q to prevent the dot product between q and k from growing too large.\n","        depth = (self.hidden_size // self.num_heads)\n","        q *= depth ** -0.5\n","        \n","        logits = tf.matmul(q, k, transpose_b=True)\n","        # logits += self.bias\n","        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n","        \n","        if self.trainable:\n","            weights = tf.nn.dropout(weights, 1.0 - self.attention_dropout)\n","        \n","        attention_output = tf.matmul(weights, v)\n","        attention_output = self.combine_heads(attention_output)\n","        attention_output = self.dense(attention_output)\n","        return attention_output\n","        \n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape(input_shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.531056Z","start_time":"2020-04-11T02:14:24.524047Z"},"id":"bG4cGcYYNVYw","trusted":true,"colab_type":"code","colab":{}},"source":["def categorical_focal_loss(gamma=2.0, alpha=0.25):\n","    \"\"\"\n","    Implementation of Focal Loss from the paper in multiclass classification\n","    Formula:\n","        loss = -alpha*((1-p)^gamma)*log(p)\n","    Parameters:\n","        alpha -- the same as wighting factor in balanced cross entropy\n","        gamma -- focusing parameter for modulating factor (1-p)\n","    Default value:\n","        gamma -- 2.0 as mentioned in the paper\n","        alpha -- 0.25 as mentioned in the paper\n","    \"\"\"\n","    def focal_loss(y_true, y_pred):\n","        # Define epsilon so that the backpropagation will not result in NaN\n","        # for 0 divisor case\n","        epsilon = K.epsilon()\n","        # Add the epsilon to prediction value\n","        #y_pred = y_pred + epsilon\n","        # Clip the prediction value\n","        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n","        # Calculate cross entropy\n","        cross_entropy = -y_true*K.log(y_pred)\n","        # Calculate weight that consists of  modulating factor and weighting factor\n","        weight = alpha * y_true * K.pow((1-y_pred), gamma)\n","        # Calculate focal loss\n","        loss = weight * cross_entropy\n","        # Sum the losses in mini_batch\n","        loss = K.sum(loss, axis=1)\n","        return loss\n","    \n","    return focal_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ub7gDIJ2oqU7","colab_type":"code","colab":{}},"source":["def multi_category_focal_loss2(gamma=2., alpha=.25):\n","    \"\"\"\n","    focal loss for multi category of multi label problem\n","    适用于多分类或多标签问题的focal loss\n","    alpha控制真值y_true为1/0时的权重\n","        1的权重为alpha, 0的权重为1-alpha\n","    当你的模型欠拟合，学习存在困难时，可以尝试适用本函数作为loss\n","    当模型过于激进(无论何时总是倾向于预测出1),尝试将alpha调小\n","    当模型过于惰性(无论何时总是倾向于预测出0,或是某一个固定的常数,说明没有学到有效特征)\n","        尝试将alpha调大,鼓励模型进行预测出1。\n","    Usage:\n","     model.compile(loss=[multi_category_focal_loss2(alpha=0.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n","    \"\"\"\n","    epsilon = 1.e-7\n","    gamma = float(gamma)\n","    alpha = tf.constant(alpha, dtype=tf.float32)\n","\n","    def multi_category_focal_loss2_fixed(y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n","    \n","        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n","        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n","        ce = -tf.math.log(y_t)\n","        weight = tf.pow(tf.subtract(1., y_t), gamma)\n","        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n","        loss = tf.reduce_mean(fl)\n","        return loss\n","    return multi_category_focal_loss2_fixed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m8XTRWemZEoT","colab_type":"code","colab":{}},"source":["def f1v2(y_true, y_pred):\n","    def recall(y_true, y_pred):\n","        \"\"\"Recall metric.\n","\n","        Only computes a batch-wise average of recall.\n","\n","        Computes the recall, a metric for multi-label classification of\n","        how many relevant items are selected.\n","        \"\"\"\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + K.epsilon())\n","        return recall\n","\n","    def precision(y_true, y_pred):\n","        \"\"\"Precision metric.\n","\n","        Only computes a batch-wise average of precision.\n","\n","        Computes the precision, a metric for multi-label classification of\n","        how many selected items are relevant.\n","        \"\"\"\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        return precision\n","    precision = precision(y_true, y_pred)\n","    recall = recall(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Efvocyr0Okcy","colab_type":"code","colab":{}},"source":["def f1macro(y_true, y_pred):\n","    b,l,c = np.shape(y_true)\n","    y_pred = K.round(y_pred)\n","    tp = K.sum(K.sum(K.cast(y_true*y_pred, 'float'), axis=2),axis=1)\n","    tn = K.sum(K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=2),axis=1)\n","    fp = K.sum(K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=2),axis=1)\n","    fn = K.sum(K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=2),axis=1)\n","    p = tp / (tp + fp + K.epsilon())\n","    r = tp / (tp + fn + K.epsilon())\n","\n","    f1 = 2*p*r / (p+r+K.epsilon())\n","    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n","    return K.mean(f1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6d4mH_0Omj2","colab_type":"code","colab":{}},"source":["def get_f1(y_true, y_pred): #taken from old keras source code\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BuSYzMF2REK9","colab_type":"code","colab":{}},"source":["def fbeta_score_macro(y_true, y_pred, beta=1, threshold=0.5):\n","\n","    y_true = K.cast(y_true, 'float')\n","    y_pred = K.cast(K.greater(K.cast(y_pred, 'float'), threshold), 'float')\n","\n","    tp = K.sum(y_true * y_pred, axis=0)\n","    fp = K.sum((1 - y_true) * y_pred, axis=0)\n","    fn = K.sum(y_true * (1 - y_pred), axis=0)\n","\n","    p = tp / (tp + fp + K.epsilon())\n","    r = tp / (tp + fn + K.epsilon())\n","\n","    f1 = (1 + beta ** 2) * p * r / ((beta ** 2) * p + r + K.epsilon())\n","    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n","\n","    return K.mean(f1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.549703Z","start_time":"2020-04-11T02:14:24.533479Z"},"id":"YSESlpaNwkkx","trusted":true,"colab_type":"code","colab":{}},"source":["def WaveNetResidualConv1D(num_filters, kernel_size, stacked_layer):\n","\n","    def build_residual_block(l_input):\n","        resid_input = l_input\n","        for dilation_rate in [2**i for i in range(stacked_layer)]:\n","            l_sigmoid_conv1d = Conv1D(\n","              num_filters, kernel_size, dilation_rate=dilation_rate,\n","              padding='same', activation='sigmoid')(l_input)\n","            l_tanh_conv1d = Conv1D(\n","             num_filters, kernel_size, dilation_rate=dilation_rate,\n","             padding='same', activation='mish')(l_input)\n","            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n","            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n","            resid_input = Add()([resid_input ,l_input])\n","        return resid_input\n","    return build_residual_block\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1PeJF2_Iad7g","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEn4iIRTaslB","colab_type":"code","colab":{}},"source":["def Classifier(shape_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    l_output = Dense(11, activation='softmax')(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"erbbJrcxlkdS","colab_type":"code","colab":{}},"source":["def weighted_categorical_crossentropy(weights):\n","    \"\"\"\n","    A weighted version of keras.objectives.categorical_crossentropy\n","    \n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","    \n","    Usage:\n","        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n","        loss = weighted_categorical_crossentropy(weights)\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","    \n","    weights = K.variable(weights)\n","        \n","    def loss(y_true, y_pred):\n","        # scale predictions so that the class probas of each sample sum to 1\n","        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n","        # clip to prevent NaN's and Inf's\n","        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","        # calc\n","        loss = y_true * K.log(y_pred) * weights\n","        loss = -K.sum(loss, -1)\n","        return loss\n","    \n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uX3YFhsglyLz","colab_type":"code","colab":{}},"source":["# # Custom loss function with costs\n","# def w_categorical_crossentropy(y_true, y_pred, weights):\n","#     nb_cl = len(weights)\n","#     final_mask = K.zeros_like(y_pred[:, 0])\n","#     y_pred_max = K.max(y_pred, axis=1)\n","#     y_pred_max = K.expand_dims(y_pred_max, 1)\n","#     y_pred_max_mat = K.equal(y_pred, y_pred_max)\n","#     for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n","#         final_mask += (K.cast(weights[c_t, c_p],K.floatx()) * K.cast(y_pred_max_mat[:, c_p] ,K.floatx())* \n","#                        K.cast(y_true[:, c_t],K.floatx()))\n","#     return K.categorical_crossentropy(y_pred, y_true) * final_mask\n","\n","# w_array = np.ones((3,3))\n","# w_array[2,1] = 1.2\n","# w_array[1,2] = 1.2\n","# ncce = partial(w_categorical_crossentropy, weights=w_array)\n","# ncce.__name__ ='w_categorical_crossentropy'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZymKtoVo8F06","colab_type":"code","colab":{}},"source":["def ClassifierW(shape_, weights_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]*2\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    l_output = Dense(11, activation='softmax')(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","\n","    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n","    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n","\n","#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n","#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pnR2IhRSj8Gc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.563702Z","start_time":"2020-04-11T02:14:24.556844Z"},"id":"9Cckpy4I2uco","trusted":true,"colab_type":"code","colab":{}},"source":["def Classifierx(shape_):        \n","    # dsize = [256, 512, 256, 128, 11] # try 2: batchsize = 64, len = 1000\n","    dsize = [128, 256, 128, 64, 11] \n","    # dsize = [32, 64, 32, 32, 11]  # try 1: batchsize=10, len=1000\n","    inp = Input(shape=(shape_))\n","    x = Bidirectional(GRU(dsize[0], return_sequences=True))(inp)\n","    x = Attention(dsize[1])(x)\n","    x = TimeDistributed(Dense(dsize[2], activation='mish'))(x)\n","    x = TimeDistributed(Dense(dsize[3], activation='mish'))(x)\n","    out = TimeDistributed(Dense(dsize[4], activation='softmax', name='out'))(x)\n","    \n","    model = models.Model(inputs=inp, outputs=out) \n","    \n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6YzOl4B6bczN","colab_type":"code","colab":{}},"source":["def ClassifierG(shape_, weights_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]*2\n","    drop_block_size = [64, 16, 4]\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    # x = DropBlock1D(block_size=drop_block_size[0], keep_prob=0.8)(x)\n","    x = BatchNormalization()(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    x = BatchNormalization()(x)\n","    # x = DropBlock1D(block_size=drop_block_size[1], keep_prob=0.8)(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    x = BatchNormalization()(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.2)(x)\n","    # x = DropBlock1D(block_size=drop_block_size[2], keep_prob=0.8)(x)\n","    x = TimeDistributed(Dense(64, activation='mish'))(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.3)(x)\n","    l_output = TimeDistributed(Dense(11, activation='softmax'))(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","\n","    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n","    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n","\n","#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n","#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3qKxcKCwIhp","colab_type":"code","colab":{}},"source":["def ClassifierD(shape_, weights_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]*2\n","    drop_block_size = [64, 16, 4]\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    # x = SpatialDropout1D(0.3)(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    # x = SpatialDropout1D(0.3)(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    # x = SpatialDropout1D(0.2)(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    x = SpatialDropout1D(0.2)(x)\n","    x = TimeDistributed(Dense(64, activation='mish'))(x)\n","    x = SpatialDropout1D(0.3)(x)\n","    x = TimeDistributed(Dense(64, activation='mish'))(x)\n","    x = SpatialDropout1D(0.3)(x)\n","    l_output = TimeDistributed(Dense(11, activation='softmax'))(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","\n","    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n","    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n","\n","#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n","#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.579491Z","start_time":"2020-04-11T02:14:24.567764Z"},"trusted":true,"id":"L4cQ0L8ucOgn","colab_type":"code","colab":{}},"source":["\n","class MacroF1ES(Callback):\n","    def __init__(self, model, inputs, targets, \n","                 patience=5, delta=0, checkpoint_path='checkpoint.h5', is_maximize=True):\n","        \n","        self.model = model\n","        self.inputs = inputs\n","        self.targets = np.argmax(targets, axis=2).reshape(-1)\n","        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n","        self.counter, self.best_score = 0, None\n","        self.is_maximize = is_maximize\n","        self.stopped_epoch = 0\n","        \n","    def on_epoch_end(self, epoch, logs):\n","        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n","        score = f1_score(self.targets, pred, average=\"macro\")\n","        logger.info(f'\\n epoch:{epoch:03d}, F1Macro: {score:.5f}')   \n","        \n","        if self.best_score is None or \\\n","        (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n","            self.model.save(self.checkpoint_path)\n","#             torch.save(model.state_dict(), self.checkpoint_path)\n","            self.best_score, self.counter = score, 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience: ##stop training\n","                self.stopped_epoch = epoch\n","                self.model.stop_training = True\n","                \n","    def on_train_end(self, logs=None):\n","        if self.stopped_epoch > 0:\n","              logger.info('Epoch %05d: early stopping' % (self.stopped_epoch + 1))   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"415IZHTAHw5g","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.589759Z","start_time":"2020-04-11T02:14:24.582640Z"},"id":"1yfpldH_2ucw","trusted":true,"colab_type":"code","colab":{}},"source":["class MacroF1(Callback):\n","    def __init__(self, model, inputs, targets):\n","        self.model = model\n","        self.inputs = inputs\n","        self.targets = np.argmax(targets, axis=2).reshape(-1)\n","\n","    def on_epoch_end(self, epoch, logs):\n","        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n","        score = f1_score(self.targets, pred, average=\"macro\")\n","        print(f' F1Macro: {score:.5f}')    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.596693Z","start_time":"2020-04-11T02:14:24.591931Z"},"id":"skru4lPt2uc6","trusted":true,"colab_type":"code","colab":{}},"source":["def normalize(train, test):\n","    \n","    train_input_mean = train.signal.mean()\n","    train_input_sigma = train.signal.std()\n","    train['signal'] = (train.signal-train_input_mean)/train_input_sigma\n","    test['signal'] = (test.signal-train_input_mean)/train_input_sigma\n","\n","    return train, test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.609602Z","start_time":"2020-04-11T02:14:24.600166Z"},"id":"02TDJtejfS0_","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def run_everything(fe_config : List) -> NoReturn:\n","    not_feats_cols = ['time']\n","    target_col = ['open_channels']\n","    init_logger()\n","    with timer(f'Reading Data'):\n","        logger.info('Reading Data Started ...')\n","        base = os.path.abspath(PATH+'liverpool-ion-switching/')\n","        train, test, sample_submission = read_data(base)\n","        train, test = normalize(train, test)    \n","        logger.info('Reading and Normalizing Data Completed ...')\n","    with timer(f'Creating Features'):\n","        logger.info('Feature Enginnering Started ...')\n","        for config in fe_config:\n","            train = run_feat_enginnering(train, create_all_data_feats=config[0], batch_size=config[1])\n","            test  = run_feat_enginnering(test,  create_all_data_feats=config[0], batch_size=config[1])\n","        train, test, feats = feature_selection(train, test)\n","        logger.info('Feature Enginnering Completed ...')\n","\n","    with timer(f'Running Wavenet model'):\n","        logger.info(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started ...')\n","        run_cv_model_by_batch(train, test, splits=SPLITS, batch_col='group', feats=feats, \n","                              sample_submission=sample_submission, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n","        logger.info(f'Training completed ...')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:15:53.901793Z","start_time":"2020-04-11T02:14:24.612467Z"},"id":"_w4_OJJtfS1K","outputId":"5d8e4e3f-a49d-4265-dc10-a0bc862ea20d","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1587327922396,"user_tz":420,"elapsed":136280,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":615}},"source":["run_everything(fe_config)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-04-19 20:23:52,848 INFO Reading Data Started ...\n","2020-04-19 20:23:57,660 INFO Reading and Normalizing Data Completed ...\n","2020-04-19 20:23:57,662 INFO [Reading Data] done in 5 s\n","2020-04-19 20:23:57,665 INFO Feature Enginnering Started ...\n"],"name":"stderr"},{"output_type":"stream","text":["['signal', 'signal_shift_pos_1', 'signal_shift_neg_1', 'signal_shift_pos_2', 'signal_shift_neg_2', 'signal_shift_pos_3', 'signal_shift_neg_3', 'signal_shift_pos_1_msignal', 'signal_shift_neg_1_msignal', 'signal_shift_pos_2_msignal', 'signal_shift_neg_2_msignal', 'signal_shift_pos_3_msignal', 'signal_shift_neg_3_msignal', 'signal_2']\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-19 20:24:02,953 INFO Feature Enginnering Completed ...\n","2020-04-19 20:24:02,955 INFO [Creating Features] done in 5 s\n","2020-04-19 20:24:02,957 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n"],"name":"stderr"},{"output_type":"stream","text":["weights: [ 0.36652399  0.46106257  0.82059173  0.67983748  1.12675802  1.63577934\n","  2.41635544  1.71516878  1.85390282  3.33929955 12.72060713]\n","model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-19 20:24:35,113 INFO Training fold 1 completed. macro f1 score : 0.93918\n"],"name":"stderr"},{"output_type":"stream","text":["model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-19 20:24:45,874 INFO Training fold 2 completed. macro f1 score : 0.94048\n"],"name":"stderr"},{"output_type":"stream","text":["model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-19 20:24:55,924 INFO Training fold 3 completed. macro f1 score : 0.93876\n"],"name":"stderr"},{"output_type":"stream","text":["model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-19 20:25:07,127 INFO Training fold 4 completed. macro f1 score : 0.94038\n"],"name":"stderr"},{"output_type":"stream","text":["model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-19 20:25:17,340 INFO Training fold 5 completed. macro f1 score : 0.93914\n","2020-04-19 20:25:25,852 INFO Training completed. oof macro f1 score : 0.93963\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>time</th>\n","      <th>open_channels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>500.000092</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>500.000214</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>500.000305</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>500.000397</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>500.000488</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         time  open_channels\n","0  500.000092              0\n","1  500.000214              0\n","2  500.000305              0\n","3  500.000397              0\n","4  500.000488              0"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2020-04-19 20:25:30,035 INFO Training completed ...\n","2020-04-19 20:25:30,040 INFO [Running Wavenet model] done in 87 s\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Sdkyf_i5Szw0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aMGpTHZ0TCaS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"akXxb_RYTRDy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPKryOHBTftS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBz6AWf7TuWy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6LaeEgLT9AW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPSC-ngDWUO5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}