{"cells":[{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-06T19:20:21.272657Z\",\"start_time\":\"2020-04-06T19:20:19.347236Z\"}}\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:09.840829Z\",\"start_time\":\"2020-04-03T23:04:07.112516Z\"}}\n#######################################################\n# Much of this comes from https://www.kaggle.com/pradeeppathak9/gamma-log-facies-type-prediction\n# https://www.crowdanalytix.com/contests/gamma-log-facies-type-prediction\n######################################################\nimport os\nos.system('pip install pytorch_toolbelt')\nimport pandas as pd\nimport numpy as np\nimport json\npd.options.display.max_rows = 1000\npd.options.display.max_columns = 1000\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score\nimport time\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\nfrom sklearn.model_selection import KFold\nimport gc\n\nfrom tqdm import tqdm\nfrom itertools import groupby, accumulate\nfrom random import shuffle\n\nfrom sklearn.model_selection import GroupKFold, GroupShuffleSplit, LeaveOneGroupOut\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pytorch_toolbelt import losses as L\n\nfrom scipy import signal\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-06T19:20:26.556633Z\",\"start_time\":\"2020-04-06T19:20:21.275582Z\"}}\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:12.145471Z\",\"start_time\":\"2020-04-03T23:04:12.141914Z\"}}\nPATH = '/kaggle/input/'\n# PATH = '/Users/helen/Desktop/Data/'\nSEQ_LEN = 4000\nGROUP_PER_BATCH = 500000//SEQ_LEN\nwindow_sizes = [10, 50]\n\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:14.838509Z\",\"start_time\":\"2020-04-03T23:04:14.822835Z\"}}\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        if col != 'time':\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:20.620083Z\",\"start_time\":\"2020-04-03T23:04:17.837634Z\"}}\nss = pd.read_csv(PATH + \"liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\ntrain = pd.read_csv(PATH +'clean-kalman/train_clean_kalman.csv')\ntrain['filter'] = 0\ntest = pd.read_csv(PATH +'clean-kalman/test_clean_kalman.csv')\ntest['filter'] = 2\n\n\n# %% [code]\n\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:29.265784Z\",\"start_time\":\"2020-04-03T23:04:23.756861Z\"}}\n# %%time\n# for window in window_sizes:\n#     train[\"rolling_mean_\" + str(window)] = train['signal'].rolling(window=window).mean()\n#     train[\"rolling_std_\" + str(window)] = train['signal'].rolling(window=window).std()\n#     train[\"rolling_var_\" + str(window)] = train['signal'].rolling(window=window).var()\n#     train[\"rolling_min_\" + str(window)] = train['signal'].rolling(window=window).min()\n#     train[\"rolling_max_\" + str(window)] = train['signal'].rolling(window=window).max()\n    \n#     train[\"rolling_mean_\" + str(window)+\"_diff\"] = train[\"rolling_mean_\" + str(window)] - train['signal']\n#     train[\"rolling_std_\" + str(window)+\"_diff\"] = train[\"rolling_std_\" + str(window)] - train['signal']\n#     train[\"rolling_var_\" + str(window)+\"_diff\"] = train[\"rolling_var_\" + str(window)] - train['signal']\n#     train[\"rolling_min_\" + str(window)+\"_diff\"] = train[\"rolling_min_\" + str(window)] - train['signal']\n#     train[\"rolling_max_\" + str(window)+\"_diff\"] = train[\"rolling_max_\" + str(window)] - train['signal']  \n#     #train[\"rolling_min_max_ratio_\" + str(window)] = train[\"rolling_min_\" + str(window)] / train[\"rolling_max_\" + str(window)]\n#     #train[\"rolling_min_max_diff_\" + str(window)] = train[\"rolling_max_\" + str(window)] - train[\"rolling_min_\" + str(window)]\n    \n#     a = (train['signal'] - train['rolling_min_' + str(window)]) / (train['rolling_max_' + str(window)] - train['rolling_min_' + str(window)])\n#     train[\"norm_\" + str(window)] = a * (np.floor(train['rolling_max_' + str(window)]) - np.ceil(train['rolling_min_' + str(window)]))\n    \n# train = train.replace([np.inf, -np.inf], np.nan)    \n# train.fillna(0, inplace=True)\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:34.073285Z\",\"start_time\":\"2020-04-03T23:04:32.404428Z\"}}\n# %%time\n# for window in window_sizes:\n#     test[\"rolling_mean_\" + str(window)] = test['signal'].rolling(window=window).mean()\n#     test[\"rolling_std_\" + str(window)] = test['signal'].rolling(window=window).std()\n#     test[\"rolling_var_\" + str(window)] = test['signal'].rolling(window=window).var()\n#     test[\"rolling_min_\" + str(window)] = test['signal'].rolling(window=window).min()\n#     test[\"rolling_max_\" + str(window)] = test['signal'].rolling(window=window).max()\n    \n#     test[\"rolling_mean_\" + str(window)+\"_diff\"] = test[\"rolling_mean_\" + str(window)] - test['signal']\n#     test[\"rolling_std_\" + str(window)+\"_diff\"] = test[\"rolling_std_\" + str(window)] - test['signal']\n#     test[\"rolling_var_\" + str(window)+\"_diff\"] = test[\"rolling_var_\" + str(window)] - test['signal']\n#     test[\"rolling_min_\" + str(window)+\"_diff\"] = test[\"rolling_min_\" + str(window)] - test['signal']\n#     test[\"rolling_max_\" + str(window)+\"_diff\"] = test[\"rolling_max_\" + str(window)] - test['signal']  \n    \n#     #test[\"rolling_min_max_ratio_\" + str(window)] = test[\"rolling_min_\" + str(window)] / test[\"rolling_max_\" + str(window)]\n#     #test[\"rolling_min_max_diff_\" + str(window)] = test[\"rolling_max_\" + str(window)] - test[\"rolling_min_\" + str(window)]\n\n    \n#     a = (test['signal'] - test['rolling_min_' + str(window)]) / (test['rolling_max_' + str(window)] - test['rolling_min_' + str(window)])\n#     test[\"norm_\" + str(window)] = a * (np.floor(test['rolling_max_' + str(window)]) - np.ceil(test['rolling_min_' + str(window)]))\n\n# test = test.replace([np.inf, -np.inf], np.nan)    \n# test.fillna(0, inplace=True)\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:07:30.632968Z\",\"start_time\":\"2020-04-03T23:04:37.769007Z\"}}\n# %%time\n# def features(df):\n#     df = df.sort_values(by=['time']).reset_index(drop=True)\n#     df.index = ((df.time * 10_000) - 1).values\n#     df['batch'] = df.index // 25_000\n#     df['batch_index'] = df.index  - (df.batch * 25_000)\n#     df['batch_slices'] = df['batch_index']  // 2500\n#     df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), \n#                                                        str(r['batch_slices']).zfill(3)]), axis=1)\n    \n#     for c in ['batch','batch_slices2']:\n#         d = {}\n#         d['mean'+c] = df.groupby([c])['signal'].mean()\n# #         print(d['mean'+c])\n#         d['median'+c] = df.groupby([c])['signal'].median()\n#         d['max'+c] = df.groupby([c])['signal'].max()\n#         d['min'+c] = df.groupby([c])['signal'].min()\n#         d['std'+c] = df.groupby([c])['signal'].std()\n#         d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n#         d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n#         d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n#         d['range'+c] = d['max'+c] - d['min'+c]\n#         d['maxtomin'+c] = d['max'+c] / d['min'+c]\n#         d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) / 2\n# #         print('--------------------')\n#         for v in d:\n#             df[v] = df[c].map(d[v].to_dict())\n    \n#     # add shifts_1\n#     df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n#     df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n#     for i in df[df['batch_index']==0].index:\n#         df['signal_shift_+1'][i] = np.nan\n#     for i in df[df['batch_index']==49999].index:\n#         df['signal_shift_-1'][i] = np.nan\n    \n#     # add shifts_2 - my upgrade\n#     df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n#     df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n#     for i in df[df['batch_index']==0].index:\n#         df['signal_shift_+2'][i] = np.nan\n#     for i in df[df['batch_index']==1].index:\n#         df['signal_shift_+2'][i] = np.nan\n#     for i in df[df['batch_index']==49999].index:\n#         df['signal_shift_-2'][i] = np.nan\n#     for i in df[df['batch_index']==49998].index:\n#         df['signal_shift_-2'][i] = np.nan\n    \n#     df = df.drop(columns=['batch', 'batch_index', 'batch_slices', 'batch_slices2'])\n\n#     for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels']]:\n#         df[c+'_msignal'] = df[c] - df['signal']\n        \n#     df = df.replace([np.inf, -np.inf], np.nan)    \n#     df.fillna(0, inplace=True)\n#     gc.collect()\n#     return df\n\n# train = features(train)\n# test = features(test)\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:16:35.157641Z\",\"start_time\":\"2020-04-03T23:15:50.454252Z\"}}\n# train = reduce_mem_usage(train)\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:20:37.936034Z\",\"start_time\":\"2020-04-03T23:20:21.086950Z\"}}\n# test = reduce_mem_usage(test)\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T18:35:47.275695Z\",\"start_time\":\"2020-04-03T18:35:47.253657Z\"}}\n# pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3)\n\n# %% [code]\n\n\n# %% [code]\n# normalize the data (standard scaler). We can also try other scalers for a better score!\ndef normalize(train, test, div = 1.0):\n    train_input_mean = train.signal.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal - train_input_mean) / train_input_sigma / div\n    test['signal'] = (test.signal - train_input_mean) / train_input_sigma / div\n    return train, test\n\n# %% [code]\n\nprint(\"before normalize\")\norgnorm = 15.0\nprint(max(train['signal'])/orgnorm, min(train['signal'])/orgnorm)\nprint(max(test['signal'])/orgnorm, min(test['signal'])/orgnorm)\n\nprint(\"after normalize\")\ntrain, test = normalize(train, test, div=6.0) ## margic number\nprint('Reading and Normalizing Data Completed')\n\nprint(max(train['signal']), min(train['signal']))\nprint(max(test['signal']), min(test['signal']))\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-06T19:18:20.331633Z\",\"start_time\":\"2020-04-06T19:18:20.220051Z\"}}\n\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-06T19:20:48.033757Z\",\"start_time\":\"2020-04-06T19:20:48.028966Z\"}}\nfrom pykalman import KalmanFilter\n\ndef Kalman1D(observations,damping=1):\n    # To return the smoothed time series data\n    observation_covariance = damping\n    initial_value_guess = observations[0]\n    transition_matrix = 1\n    transition_covariance = 0.1\n    initial_value_guess\n    kf = KalmanFilter(\n            initial_state_mean=initial_value_guess,\n            initial_state_covariance=observation_covariance,\n            observation_covariance=observation_covariance,\n            transition_covariance=transition_covariance,\n            transition_matrices=transition_matrix\n        )\n    pred_state, state_cov = kf.smooth(observations)\n    return pred_state\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-06T19:20:48.042523Z\",\"start_time\":\"2020-04-06T19:20:48.036667Z\"}}\n# observation_covariance = .0015\n# train['signal']  = Kalman1D(train.signal.values,observation_covariance)\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-06T19:20:48.047780Z\",\"start_time\":\"2020-04-06T19:20:48.045452Z\"}}\n# plt.figure(figsize=(20, 10))\n# plt.plot(train_data[\"time\"], train_data[\"signal\"], color=\"r\")\n# plt.title(\"Signal data\", fontsize=20)\n# plt.xlabel(\"Time\", fontsize=18)\n# plt.ylabel(\"Signal\", fontsize=18)\n# plt.show()\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-06T19:20:48.057594Z\",\"start_time\":\"2020-04-06T19:20:48.051444Z\"}}\n# test['signal']  = Kalman1D(test.signal.values,observation_covariance)\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-06T19:27:22.895698Z\",\"start_time\":\"2020-04-06T19:21:30.192924Z\"}}\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:21:08.254985Z\",\"start_time\":\"2020-04-03T23:21:05.328127Z\"}}\nts1 = pd.concat([train, test], axis=0, sort=False).reset_index(drop=True)\nts1.head()\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:21:23.618036Z\",\"start_time\":\"2020-04-03T23:21:22.475969Z\"}}\n# 将每个batch一个time2 bin\nts1['time2'] = pd.cut(ts1['time'], bins=np.linspace(0.0000, 700., num=14 + 1), labels=list(range(14)), \n                      include_lowest=True).astype(int)\n#每个batch time2 做归一化\nts1['time2'] = ts1.groupby('time2')['time'].rank( )/500000. \n\n#将每个batch变成125个group\nnp.random.seed(321)\nts1['group'] = pd.cut(ts1['time'], bins=np.linspace(0.0000, 700., num=14*GROUP_PER_BATCH + 1), \n                      labels=list(range(14*GROUP_PER_BATCH)), \n                      include_lowest=True).astype(int)\nnp.random.seed(321)\nts1.head()\n# signal processing features\ndef calc_gradients(s, df, n_grads = 4):\n    '''\n    Calculate gradients for a pandas series. Returns the same number of samples\n    '''\n    \n    g = s.values\n    for i in range(n_grads):\n        g = np.gradient(g)\n        df['grad_' + str(i+1)] = g\n        \n    return df\n\ndef calc_low_pass(s, df, n_filts=10):\n    '''\n    Applies low pass filters to the signal. Left delayed and no delayed\n    '''\n    wns = np.logspace(-2, -0.3, n_filts)\n    print(\"low wns:\", wns)\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='low')\n        print(\"low wn:{},b:{},a:{}\".format(wn, b, a))\n        zi = signal.lfilter_zi(b, a)\n        df['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        df['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return df\n\ndef calc_high_pass(s, df, n_filts=10):\n    '''\n    Applies high pass filters to the signal. Left delayed and no delayed\n    '''\n    wns = np.logspace(-2, -0.1, n_filts)\n    print(\"high wns:\", wns)\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='high')\n        print(\"high wn:{},b:{},a:{}\".format(wn, b, a))\n        zi = signal.lfilter_zi(b, a)\n        df['highpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        df['highpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return df\n\ndef calc_ewm(s, df, windows=[10, 50, 100, 500, 1000]):\n    '''\n    Calculates exponential weighted functions\n    '''\n    for w in windows:\n        df['ewm_mean_' + str(w)] = s.ewm(span=w, min_periods=1).mean()\n        df['ewm_std_' + str(w)] = s.ewm(span=w, min_periods=1).std()\n        \n#     # add zeros when na values (std)\n#     df = df.fillna(value=0)\n        \n    return df\n\n\ndef add_features(df):\n    '''\n    All calculations together\n    '''\n    s = df['signal']\n    df = calc_gradients(s, df, n_grads = 2)\n    df = calc_low_pass(s, df, n_filts = 6)\n    df = calc_high_pass(s, df, n_filts = 6)\n#     df = calc_ewm(s, df, windows=[5,10])\n    \n    return df\n# %% [code]\n# get lead and lags features\ndef lag_with_pct_change(df, windows):\n    for window in windows:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n    return df\n\n# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\ndef run_feat_engineering(df):\n    # create leads and lags (1, 2, 3 making them 6 features)\n    df = add_features(df)\n#     df = lag_with_pct_change(df, [1, 2, 3])\n    # create signal ** 2 (this is the new feature)\n    df['signal_2'] = df['signal'] ** 2\n    return df\n\n# fillna with the mean and select features for training\ndef feature_selection_org(train, test):\n    features = use_cols\n    train = train.replace([np.inf, -np.inf], np.nan)\n    test = test.replace([np.inf, -np.inf], np.nan)\n    for feature in features:\n        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n        train[feature] = train[feature].fillna(feature_mean)\n        test[feature] = test[feature].fillna(feature_mean)\n    return train, test, features\n\ndef feature_selection(data, features):\n    data = data.replace([np.inf, -np.inf], np.nan)\n    for feature in features:\n        feature_mean = data[feature].mean()\n        data[feature] = data[feature].fillna(feature_mean)\n    return data, features\n\n# %% [code]\nprint('Creating Features')\nprint('Feature Engineering Started...')\nts1 = run_feat_engineering(ts1)\nuse_cols = [col for col in ts1.columns if col not in ['index','filter','group', 'open_channels', 'time', 'time2', \n                                                      'medianbatch', 'abs_avgbatch', 'abs_maxbatch']]\nts1, features = feature_selection(ts1, use_cols)\nprint(ts1.head(10))\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:21:26.967325Z\",\"start_time\":\"2020-04-03T23:21:26.773418Z\"}}\ny = ts1.loc[ts1['filter']==0, 'open_channels']\ngroup = ts1.loc[ts1['filter']==0, 'group']\nX = ts1.loc[ts1['filter']==0, 'signal']\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:22:22.903966Z\",\"start_time\":\"2020-04-03T23:22:22.478503Z\"}}\nnp.random.seed(321)\nskf = GroupKFold(n_splits=5)\nsplits = [x for x in skf.split(X, y, group)] ## split into train index and test index\nprint(features)\nprint(len(features))\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:22:25.960306Z\",\"start_time\":\"2020-04-03T23:22:25.955933Z\"}}\ndef analysis_splits(new_split):\n    bins = np.array(new_split)//GROUP_PER_BATCH\n    for b in bins:\n        print(np.histogram(b, bins=np.arange(11))[0])\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:22:38.774185Z\",\"start_time\":\"2020-04-03T23:22:27.923032Z\"}}\n# Create numpy array of inputs\nfor col in use_cols:\n    col_mean = ts1[col].mean()\n    ts1[col] = ts1[col].fillna(col_mean)\n \nval_preds_all = np.zeros((ts1[ts1['filter']==0].shape[0], 11))\ntest_preds_all = np.zeros((ts1[ts1['filter']==2].shape[0], 11))\n\ngroups = ts1.loc[ts1['filter']==0, 'group']\ntimes = ts1.loc[ts1['filter']==0, 'time']\n\nnew_splits = []\n# 5个fold，每个fold中train和test对应group无重复，将其利用group编号装入new_split.\nfor sp in splits:\n    new_split = []\n    new_split.append(np.unique(groups[sp[0]]))\n    new_split.append(np.unique(groups[sp[1]]))\n#     analysis_splits(new_split)\n    new_splits.append(new_split)\n    \n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T21:40:33.912812Z\",\"start_time\":\"2020-04-03T21:40:33.910507Z\"}}\n# print(ts1[ts1['filter']==0].groupby('group').size())\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:23:14.963427Z\",\"start_time\":\"2020-04-03T23:23:05.783728Z\"}}\ntrainval = np.array(list(ts1[ts1['filter']==0].groupby('group').apply(lambda x: x[use_cols].values)))\ntest = np.array(list(ts1[ts1['filter']==2].groupby('group').apply(lambda x: x[use_cols].values)))\ntrainval_y = np.array(list(ts1[ts1['filter']==0].groupby('group').apply(lambda x: x[['open_channels']].values)))\ngc.collect()\n# transpose to B x C x L\nprint(np.shape(trainval))\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:23:37.126091Z\",\"start_time\":\"2020-04-03T23:23:34.716237Z\"}}\ntrainval = trainval.transpose((0,2,1))\ntest = test.transpose((0,2,1))\n\ntrainval_y = trainval_y.reshape(trainval_y.shape[:2])\ntest_y = np.zeros((test.shape[0], trainval_y.shape[1]))\n\ntrainval = torch.Tensor(trainval)\ntest = torch.Tensor(test)\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:24:12.902101Z\",\"start_time\":\"2020-04-03T23:24:12.898415Z\"}}\n\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:24:41.652529Z\",\"start_time\":\"2020-04-03T23:24:41.645025Z\"}}\nclass EarlyStopping:\n    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n        self.counter, self.best_score = 0, None\n        self.is_maximize = is_maximize\n\n    def load_best_weights(self, model):\n        model.load_state_dict(torch.load(self.checkpoint_path))\n\n    def __call__(self, score, model):\n        if self.best_score is None or \\\n        (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n            torch.save(model.state_dict(), self.checkpoint_path)\n            self.best_score, self.counter = score, 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False\n\n    \n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:28:57.038045Z\",\"start_time\":\"2020-04-03T23:28:57.021837Z\"}}\nclass Seq2SeqRnn(nn.Module):\n    def __init__(self, input_size, seq_len, hidden_size, output_size, num_layers=1, bidirectional=False, dropout=.3,\n            hidden_layers = [100, 200]):\n        \n        super().__init__()\n        self.input_size = input_size\n        self.seq_len = seq_len\n        self.hidden_size = hidden_size\n        self.num_layers=num_layers\n        self.bidirectional=bidirectional\n        self.output_size=output_size\n        \n        self.rnn1 = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, \n                           bidirectional=bidirectional, batch_first=True,dropout=0.3)\n        \n        self.rnn2 = nn.GRU(input_size=hidden_size*2 if bidirectional else hidden_size, hidden_size=hidden_size, num_layers=num_layers, \n                           bidirectional=bidirectional, batch_first=True,dropout=0.3)        \n         # Input Layer\n        if hidden_layers and len(hidden_layers):\n            first_layer  = nn.Linear(hidden_size*2 if bidirectional else hidden_size, hidden_layers[0])\n\n            # Hidden Layers\n            self.hidden_layers = nn.ModuleList(\n                [first_layer]+[nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers) - 1)]\n            )\n            for layer in self.hidden_layers: nn.init.kaiming_normal_(layer.weight.data)   \n\n            self.intermediate_layer = nn.Linear(hidden_layers[-1], self.input_size)\n            # output layers\n            self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n            nn.init.kaiming_normal_(self.output_layer.weight.data) \n           \n        else:\n            self.hidden_layers = []\n            self.intermediate_layer = nn.Linear(hidden_size*2 if bidirectional else hidden_siz, self.input_size)\n            self.output_layer = nn.Linear(hidden_size*2 if bidirectional else hidden_size, output_size)\n            nn.init.kaiming_normal_(self.output_layer.weight.data) \n\n        self.activation_fn = torch.relu\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        batch_size = x.size(0)\n        x = x.permute(0,2,1)\n\n        outputs, hidden1 = self.rnn1(x) \n        x = self.dropout(self.activation_fn(outputs))\n        \n        outputs, hidden2 = self.rnn2(x)\n\n        x = self.dropout(self.activation_fn(outputs))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fn(hidden_layer(x))\n            x = self.dropout(x)\n            \n        x = self.output_layer(x)\n\n        return x\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:28:58.944588Z\",\"start_time\":\"2020-04-03T23:28:58.928437Z\"}}\nclass IonDataset(Dataset):\n    \"\"\"Car dataset.\"\"\"\n\n    def __init__(self, data, labels, training=True, transform=None, flip=0.5, noise_level=0, class_split=0.0):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n        self.training = training\n        self.flip = flip\n        self.noise_level = noise_level\n        self.class_split = class_split\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        data = self.data[idx]\n        labels = self.labels[idx]\n        if np.random.rand() < self.class_split:\n            data, labels = class_split(data, labels)\n        if  np.random.rand() < self.noise_level:\n            data = data * torch.FloatTensor(10000).uniform_(1-self.noise_level, 1+self.noise_level)\n        if np.random.rand() < self.flip:\n            data = torch.flip(data, dims=[1])\n            labels = np.flip(labels, axis=0).copy().astype(int)\n\n        return [data, labels.astype(int)]\n\n# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:30:48.085252Z\",\"start_time\":\"2020-04-03T23:29:14.731305Z\"}}\nif not os.path.exists(\"./models\"):\n            os.makedirs(\"./models\")\nidshift = 3\nfor index, (train_index, val_index ) in enumerate(new_splits[idshift:], start=0):\n    index = index + idshift\n    if index > 3:\n        break;\n    print(\"Fold : {}\".format(index))\n    \n    batchsize = 16\n    train_dataset = IonDataset(trainval[train_index],  trainval_y[train_index], flip=False, noise_level=0.0, \n                               class_split=0.0)\n    train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=8, pin_memory=True)\n\n    valid_dataset = IonDataset(trainval[val_index],  trainval_y[val_index], flip=False)\n    valid_dataloader = DataLoader(valid_dataset, batchsize, shuffle=False, num_workers=4, pin_memory=True)\n\n    test_dataset = IonDataset(test,  test_y, flip=False, noise_level=0.0, class_split=0.0)\n    test_dataloader = DataLoader(test_dataset, batchsize, shuffle=False, num_workers=8, pin_memory=True)\n    test_preds_iter = np.zeros((2000000, 11))\n    it = 0\n    for it in range(1):\n        device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        model=Seq2SeqRnn(input_size=trainval.shape[1], seq_len=SEQ_LEN, hidden_size=64, output_size=11, \n                         num_layers=2, hidden_layers=[64,64,64],\n                         bidirectional=True).to(device)\n    \n        no_of_epochs = 150\n        early_stopping = EarlyStopping(patience=20, is_maximize=True, \n                                       checkpoint_path=\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it))\n        criterion = L.FocalLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n        schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                                        max_lr=0.001, epochs=no_of_epochs,\n                                                        steps_per_epoch=len(train_dataloader))\n        avg_train_losses, avg_valid_losses = [], [] \n    \n    \n        for epoch in range(no_of_epochs):\n            start_time = time.time()\n    \n            print(\"Epoch : {}\".format(epoch))\n            print( \"learning_rate: {:0.9f}\".format(schedular.get_lr()[0]))\n            train_losses, valid_losses = [], []\n    \n            model.train() # prep model for training\n            train_preds, train_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n    \n            for x, y in train_dataloader:\n                x = x.to(device)\n                y = y.to(device)\n    \n                optimizer.zero_grad()\n                predictions = model(x[:, :trainval.shape[1], :])\n    \n                predictions_ = predictions.view(-1, predictions.shape[-1]) \n                y_ = y.view(-1)\n    \n                loss = criterion(predictions_, y_)\n                # backward pass: compute gradient of the loss with respect to model parameters\n                loss.backward()\n                # perform a single optimization step (parameter update)\n                optimizer.step()\n                schedular.step()\n                # record training lossa\n                train_losses.append(loss.item())\n    \n                train_true = torch.cat([train_true, y_], 0)\n                train_preds = torch.cat([train_preds, predictions_], 0)\n\n            model.eval() # prep model for evaluation\n            val_preds, val_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n            with torch.no_grad():\n                for x, y in valid_dataloader:\n                    x = x.to(device)\n                    y = y.to(device)\n    \n                    predictions = model(x[:,:trainval.shape[1],:])\n                    predictions_ = predictions.view(-1, predictions.shape[-1]) \n                    y_ = y.view(-1)\n    \n                    loss = criterion(predictions_, y_)\n                    valid_losses.append(loss.item())\n        \n                    val_true = torch.cat([val_true, y_], 0)\n                    val_preds = torch.cat([val_preds, predictions_], 0)\n\n            # calculate average loss over an epoch\n            train_loss = np.average(train_losses)\n            valid_loss = np.average(valid_losses)\n            avg_train_losses.append(train_loss)\n            avg_valid_losses.append(valid_loss)\n            \n            print( \"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n\n            train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1), \n                                   labels=list(range(11)), average='macro')\n    \n            val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1), \n                                 labels=list(range(11)), average='macro')\n            print( \"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n    \n            if early_stopping(val_score, model):\n                print(\"Early Stopping...\")\n                print(\"Best Val Score: {:0.6f}\".format(early_stopping.best_score))\n                break\n    \n            print(\"--- %s seconds ---\" % (time.time() - start_time))\n        \n        model.load_state_dict(torch.load(\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it)))\n        with torch.no_grad():\n            pred_list = []\n            for x, y in test_dataloader:\n                x = x.to(device)\n                y = y.to(device)\n\n                predictions = model(x[:,:trainval.shape[1],:])\n                predictions_ = predictions.view(-1, predictions.shape[-1]) \n\n                pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy())\n            test_preds = np.vstack(pred_list)\n       \n        test_preds_iter += test_preds\n        test_preds_all += test_preds\n        if not os.path.exists(\"./predictions/test\"):\n            os.makedirs(\"./predictions/test\")\n        np.save('./predictions/test/gru_clean_fold_{}_iter_{}_raw.npy'.format(index, it), arr=test_preds_iter)\n        np.save('./predictions/test/gru_clean_fold_{}_raw.npy'.format(index), arr=test_preds_all)\n\ntest_preds_all = test_preds_all/np.sum(test_preds_all, axis=1)[:, None]\ntest_pred_frame = pd.DataFrame({'time': ss['time'].astype(str),\n                                'open_channels': np.argmax(test_preds_all, axis=1)})\ntest_pred_frame.to_csv(\"./gru_preds.csv\", index=False)\n\n# %% [code]\n\n# %% [code]\n","execution_count":1,"outputs":[{"output_type":"stream","text":"before normalize\n0.5712801709920725 -0.3844742898121893\n0.5570543031187908 -0.3635959906107261\nafter normalize\nReading and Normalizing Data Completed\n0.5696501880988262 -0.39265437555998145\n0.555326825626684 -0.37163299043759834\nCreating Features\nFeature Engineering Started...\nlow wns: [0.01       0.02187762 0.04786301 0.10471285 0.22908677 0.50118723]\nlow wn:0.01,b:[0.01546629 0.01546629],a:[ 1.         -0.96906742]\nlow wn:0.02187761623949552,b:[0.03323619 0.03323619],a:[ 1.         -0.93352761]\nlow wn:0.047863009232263824,b:[0.0700486 0.0700486],a:[ 1.         -0.85990279]\nlow wn:0.10471285480508996,b:[0.14235392 0.14235392],a:[ 1.         -0.71529215]\nlow wn:0.22908676527677724,b:[0.27337731 0.27337731],a:[ 1.         -0.45324539]\nlow wn:0.5011872336272722,b:[0.50093245 0.50093245],a:[1.        0.0018649]\nhigh wns: [0.01       0.02398833 0.05754399 0.13803843 0.33113112 0.79432823]\nhigh wn:0.01,b:[ 0.98453371 -0.98453371],a:[ 1.         -0.96906742]\nhigh wn:0.023988329190194897,b:[ 0.96367093 -0.96367093],a:[ 1.         -0.92734187]\nhigh wn:0.057543993733715694,b:[ 0.91689546 -0.91689546],a:[ 1.         -0.83379092]\nhigh wn:0.13803842646028852,b:[ 0.81947498 -0.81947498],a:[ 1.         -0.63894995]\nhigh wn:0.3311311214825911,b:[ 0.63583012 -0.63583012],a:[ 1.         -0.27166023]\nhigh wn:0.7943282347242815,b:[ 0.25082314 -0.25082314],a:[1.         0.49835371]\n     time    signal  open_channels  filter     time2  group    grad_1  \\\n0  0.0001 -0.190850            0.0       0  0.000002      0 -0.005865   \n1  0.0002 -0.196715            0.0       0  0.000004      0  0.011287   \n2  0.0003 -0.168276            0.0       0  0.000006      0 -0.009465   \n3  0.0004 -0.215644            0.0       0  0.000008      0 -0.024182   \n4  0.0005 -0.216639            0.0       0  0.000010      0  0.016113   \n5  0.0006 -0.183418            0.0       0  0.000012      0  0.015028   \n6  0.0007 -0.186583            0.0       0  0.000014      0  0.001807   \n7  0.0008 -0.179805            0.0       0  0.000016      0  0.000958   \n8  0.0009 -0.184666            0.0       0  0.000018      0 -0.005577   \n9  0.0010 -0.190959            0.0       0  0.000020      0 -0.014530   \n\n     grad_2  lowpass_lf_0.0100  lowpass_ff_0.0100  lowpass_lf_0.0219  \\\n0  0.017152          -0.190850          -0.193030          -0.190850   \n1 -0.001800          -0.190941          -0.193009          -0.191045   \n2 -0.017735          -0.190680          -0.192992          -0.190477   \n3  0.012789          -0.190719          -0.192981          -0.190575   \n4  0.019605          -0.191506          -0.192959          -0.192275   \n5 -0.007153          -0.191769          -0.192922          -0.192790   \n6 -0.007035          -0.191560          -0.192886          -0.192272   \n7 -0.003692          -0.191301          -0.192858          -0.191669   \n8 -0.007744          -0.191021          -0.192840          -0.191042   \n9  0.004997          -0.190921          -0.192830          -0.190827   \n\n   lowpass_ff_0.0219  lowpass_lf_0.0479  lowpass_ff_0.0479  lowpass_lf_0.1047  \\\n0          -0.193123          -0.190850          -0.191922          -0.190850   \n1          -0.193173          -0.191261          -0.192118          -0.191685   \n2          -0.193246          -0.190033          -0.192405          -0.189069   \n3          -0.193348          -0.190303          -0.192811          -0.189892   \n4          -0.193400          -0.193923          -0.192960          -0.197365   \n5          -0.193381          -0.194778          -0.192763          -0.198124   \n6          -0.193367          -0.193408          -0.192573          -0.194387   \n7          -0.193397          -0.191977          -0.192575          -0.191200   \n8          -0.193478          -0.190612          -0.192803          -0.188648   \n9          -0.193598          -0.190220          -0.193208          -0.188410   \n\n   lowpass_ff_0.1047  lowpass_lf_0.2291  lowpass_ff_0.2291  lowpass_lf_0.5012  \\\n0          -0.190983          -0.190850          -0.190843          -0.190850   \n1          -0.191460          -0.192454          -0.190795          -0.193788   \n2          -0.192314          -0.187009          -0.192499          -0.182474   \n3          -0.193744          -0.189716          -0.197680          -0.191978   \n4          -0.194007          -0.204164          -0.198660          -0.216187   \n5          -0.192674          -0.201903          -0.193423          -0.199998   \n6          -0.191358          -0.192661          -0.188786          -0.184972   \n7          -0.190866          -0.187485          -0.187241          -0.183190   \n8          -0.191298          -0.184615          -0.188682          -0.182233   \n9          -0.192440          -0.186363          -0.192536          -0.187823   \n\n   lowpass_ff_0.5012  highpass_lf_0.0100  highpass_ff_0.0100  \\\n0          -0.190850        1.665335e-16            0.002180   \n1          -0.188136       -5.774449e-03           -0.003707   \n2          -0.187195        2.240410e-02            0.024717   \n3          -0.204075       -2.492512e-02           -0.022663   \n4          -0.208122       -2.513332e-02           -0.023680   \n5          -0.192501        8.351298e-03            0.009504   \n6          -0.184084        4.977237e-03            0.006303   \n7          -0.182708        1.149640e-02            0.013053   \n8          -0.185009        6.354932e-03            0.008174   \n9          -0.195087       -3.787616e-05            0.001871   \n\n   highpass_lf_0.0240  highpass_ff_0.0240  highpass_lf_0.0575  \\\n0       -5.551115e-17            0.002177        2.775558e-17   \n1       -5.652085e-03           -0.003627       -5.377740e-03   \n2        2.216518e-02            0.024903        2.159240e-02   \n3       -2.509326e-02           -0.022341       -2.542871e-02   \n4       -2.422847e-02           -0.023273       -2.211415e-02   \n5        9.546017e-03            0.009927        1.202159e-02   \n6        5.802711e-03            0.006746        7.121811e-03   \n7        1.191281e-02            0.013560        1.215277e-02   \n8        6.362810e-03            0.008796        5.675811e-03   \n9       -1.644325e-04            0.002648       -1.038108e-03   \n\n   highpass_ff_0.0575  highpass_lf_0.1380  highpass_ff_0.1380  \\\n0            0.000773       -2.775558e-17            0.000023   \n1           -0.004840       -4.806353e-03           -0.005334   \n2            0.023977        2.023467e-02            0.024275   \n3           -0.022849       -2.588862e-02           -0.020839   \n4           -0.023674       -1.735656e-02           -0.021464   \n5            0.009216        1.613379e-02            0.009492   \n6            0.005725        7.715307e-03            0.004143   \n7            0.012448        1.048405e-02            0.010089   \n8            0.007848        2.715291e-03            0.005861   \n9            0.002064       -3.422490e-03            0.001324   \n\n   highpass_lf_0.3311  highpass_ff_0.3311  highpass_lf_0.7943  \\\n0       -2.775558e-17       -5.998890e-07            0.000000   \n1       -3.729246e-03       -7.127896e-03           -0.001471   \n2        1.706978e-02        2.287797e-02            0.007866   \n3       -2.548134e-02       -1.525870e-02           -0.015801   \n4       -7.554649e-03       -1.417816e-02            0.007625   \n5        1.907060e-02        1.013525e-02            0.004533   \n6        3.168526e-03        9.157336e-05           -0.003053   \n7        5.170386e-03        5.023155e-03            0.003221   \n8       -1.686203e-03        2.442672e-03           -0.002825   \n9       -4.459717e-03        2.500186e-03           -0.000171   \n\n   highpass_ff_0.7943  signal_2  \n0           -0.000024  0.036424  \n1           -0.004798  0.038697  \n2            0.007713  0.028317  \n3           -0.004951  0.046502  \n4           -0.001164  0.046932  \n5            0.003548  0.033642  \n6           -0.003130  0.034813  \n7            0.003038  0.032330  \n8           -0.003010  0.034101  \n9            0.004683  0.036465  \n['signal', 'grad_1', 'grad_2', 'lowpass_lf_0.0100', 'lowpass_ff_0.0100', 'lowpass_lf_0.0219', 'lowpass_ff_0.0219', 'lowpass_lf_0.0479', 'lowpass_ff_0.0479', 'lowpass_lf_0.1047', 'lowpass_ff_0.1047', 'lowpass_lf_0.2291', 'lowpass_ff_0.2291', 'lowpass_lf_0.5012', 'lowpass_ff_0.5012', 'highpass_lf_0.0100', 'highpass_ff_0.0100', 'highpass_lf_0.0240', 'highpass_ff_0.0240', 'highpass_lf_0.0575', 'highpass_ff_0.0575', 'highpass_lf_0.1380', 'highpass_ff_0.1380', 'highpass_lf_0.3311', 'highpass_ff_0.3311', 'highpass_lf_0.7943', 'highpass_ff_0.7943', 'signal_2']\n28\n","name":"stdout"},{"output_type":"stream","text":"(1250, 4000, 28)\nFold : 3\nEpoch : 0\nlearning_rate: 0.000001000\n","name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"input.size(-1) must be equal to input_size. Expected 64, got 128","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b1e4ec519e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mtrainval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                 \u001b[0mpredictions_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-b1e4ec519e4e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# type: (Tensor, Tensor, Optional[Tensor]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    157\u001b[0m             raise RuntimeError(\n\u001b[1;32m    158\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 159\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 64, got 128"]}]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}