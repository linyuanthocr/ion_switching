{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WaveNet+norm+forward_CW-prediction.ipynb","provenance":[{"file_id":"1eA76K0yj_AycKAKmffphd5I6bcD7XHix","timestamp":1587337784143},{"file_id":"1BLWtOm94kz5RhqFD0i8pG5UCSmProVc0","timestamp":1587333168658},{"file_id":"1sTJQGiEHWpHwzEg4sLWJQTfbBqP-lWgt","timestamp":1587150830206},{"file_id":"1q2AzjPOSq2TJSwiyl9X0XvIlZKF6un7Z","timestamp":1587082620870}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"huixWaL42uZi","colab_type":"text"},"source":["The validation scheme is based on [seq2seq-rnn-with-gru](https://www.kaggle.com/brandenkmurray/seq2seq-rnn-with-gru/output), and cleaned data is from [data-without-drift](https://www.kaggle.com/cdeotte/data-without-drift) and Kalman filter is from [https://www.kaggle.com/teejmahal20/single-model-lgbm-kalman-filter](single-model-lgbm-kalman-filter) and the added feature is from [wavenet-with-1-more-feature](wavenet-with-1-more-feature). I also used ragnar's data in this version [clean-kalman](https://www.kaggle.com/ragnar123/clean-kalman). The Wavenet is based on [https://github.com/philipperemy/keras-tcn](https://github.com/philipperemy/keras-tcn), [https://github.com/peustr/wavenet](https://github.com/peustr/wavenet) and [https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet) and also [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm). If any refrence is not mentioned it was not intentional, please add them in comments.\n","\n","Previous versions were mainly based on [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm)  "]},{"cell_type":"code","metadata":{"id":"sCEKkXoRPR-k","colab_type":"code","outputId":"6c7653e0-a44a-47f4-8258-3955a3e21ce1","executionInfo":{"status":"ok","timestamp":1587435818339,"user_tz":420,"elapsed":3657,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Tue Apr 21 02:23:43 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.121605Z","start_time":"2020-04-11T02:14:22.792317Z"},"_kg_hide-input":true,"id":"LqmWjeYJ2uZn","trusted":true,"colab_type":"code","colab":{}},"source":["!pip install --no-warn-conflicts -q tensorflow-addons"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.152214Z","start_time":"2020-04-11T02:14:24.125295Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"y1qOuodBfSxN","trusted":true,"colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import (TimeDistributed, Dropout, BatchNormalization, Flatten, Convolution1D, Activation, Input, Dense, LSTM, Lambda, Bidirectional,\n","                                     Add, AveragePooling1D, Multiply, GRU, GRUCell, LSTMCell, SimpleRNNCell, SimpleRNN, TimeDistributed, RNN, SpatialDropout1D,\n","                                     RepeatVector, Conv1D, MaxPooling1D, Concatenate, GlobalAveragePooling1D, UpSampling1D, Layer)\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler, CSVLogger\n","from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy, mean_squared_error\n","# from tensorflow.keras.experimental import export_saved_model, load_from_saved_model\n","from tensorflow.keras.initializers import Ones, Zeros\n","from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n","from tensorflow.keras.utils import Sequence, to_categorical\n","from tensorflow.keras import losses, models, optimizers\n","from tensorflow.keras import backend as K\n","from tensorflow.python.ops import array_ops\n","import tensorflow as tf\n","from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n","from sklearn.metrics import f1_score, cohen_kappa_score, mean_squared_error\n","from logging import getLogger, Formatter, StreamHandler, FileHandler, INFO, DEBUG, NOTSET\n","from sklearn.model_selection import KFold, GroupKFold\n","from tqdm import tqdm_notebook as tqdm\n","from contextlib import contextmanager\n","from joblib import Parallel, delayed\n","from IPython.display import display\n","from sklearn import preprocessing\n","from sklearn.utils import class_weight\n","import tensorflow_addons as tfa\n","import scipy.stats as stats\n","import random as rn\n","import pandas as pd\n","import numpy as np\n","import scipy as sp\n","import itertools\n","import warnings\n","import time\n","import pywt\n","import os\n","import gc\n","\n","from tensorflow.keras.metrics import Precision, Recall\n","# from tensorflow_addons.metrics import F1Score\n","\n","warnings.simplefilter('ignore')\n","warnings.filterwarnings('ignore')\n","pd.set_option('display.max_columns', 1000)\n","pd.set_option('display.max_rows', 500)\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"zcAdwMGxAHn9","colab_type":"code","colab":{}},"source":["Kaggle = False\n","Colab = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LC8XqE14cSRm","outputId":"7bf59e08-6bd0-4f6c-91d7-3b346059eaae","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1587435824094,"user_tz":420,"elapsed":9075,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os, sys\n","from pathlib import Path\n","\n","if Colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    path = \"/content/drive/My Drive\"\n","\n","    os.chdir(path)\n","    os.listdir(path)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M-sYfdXdWyTI","trusted":true,"colab_type":"code","colab":{}},"source":["# sys.path.append('ion_switch/keras-one-cycle')\n","# # os.listdir(patholr)\n","# from clr import OneCycleLR"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.159766Z","start_time":"2020-04-11T02:14:24.155918Z"},"trusted":true,"id":"zdosugVWcOf0","colab_type":"code","colab":{}},"source":["if Kaggle:\n","    PATH = '/kaggle/input/'\n","    outdir = '.'\n","# PATH = '/Users/helen/Desktop/Data/'\n","else:\n","    PATH = 'ion_switch/'\n","    outdir = Path(PATH+'res')\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)\n","    outdir = Path(PATH+'res/ClassifierCW-DLR-PROBF11-v1')\n","    if not os.path.exists(outdir):\n","        os.mkdir(outdir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.168634Z","start_time":"2020-04-11T02:14:24.163612Z"},"id":"UafJMtyefSxU","trusted":true,"colab_type":"code","colab":{}},"source":["EPOCHS=150\n","NNBATCHSIZE=20\n","BATCHSIZE = 4000\n","SEED = 321\n","SELECT = True\n","SPLITS = 5\n","LR = 0.001\n","BETA = 0.99994\n","Gamma = 1.0\n","Prediction = True\n","\n","use_average = False\n","weight_exp = -0.01\n","add_weights = False and (not use_average)\n","\n","timestampStr = ''\n","fe_config = [\n","    (True, BATCHSIZE),\n","]\n","\n","COMPETITION = 'ION-Switching'\n","logger = getLogger(COMPETITION)\n","LOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\n","MODELNAME = 'WaveNet-ClassifierCW-DLR-FOCALLOSS'\n","if add_weights:\n","  VERSION = '{}_len{}_lr{}_exp{}'.format(MODELNAME, BATCHSIZE, LR, weight_exp)\n","else:\n","  VERSION = '{}_len{}_lr{}'.format(MODELNAME, BATCHSIZE, LR)\n","if not Prediction:\n","  outdir = os.path.join(outdir, VERSION)\n","  if not os.path.exists(outdir):\n","      os.mkdir(outdir)\n","\n","from datetime import datetime\n","\n","if Prediction:\n","    outdir = os.path.join(outdir, timestampStr)\n","else:\n","  dateTimeObj = datetime.now()\n","  timestampStr = dateTimeObj.strftime(\"%d-%b-%Y-%H_%M_%S\")\n","  outdir = os.path.join(outdir, timestampStr)\n","  if not os.path.exists(outdir):\n","      os.mkdir(outdir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fouKVpNaFNKQ","trusted":true,"colab_type":"code","colab":{}},"source":["\n","@contextmanager\n","def timer(name : Text):\n","    t0 = time.time()\n","    yield\n","    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.177661Z","start_time":"2020-04-11T02:14:24.171732Z"},"id":"EE4v8h1tfSxb","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def init_logger():\n","\n","    handler = StreamHandler()\n","    handler.setLevel(INFO)\n","    handler.setFormatter(Formatter(LOGFORMAT))\n","    fh_handler = FileHandler(os.path.join(outdir,'{}-len{}-lr{}-{}-{}.log'.format(MODELNAME,BATCHSIZE,LR,timestampStr,\n","                                                                                  (\"prediction\" if Prediction else \"train\"))))\n","    fh_handler.setFormatter(Formatter(LOGFORMAT))\n","    logger.setLevel(INFO)\n","    logger.addHandler(handler)\n","    logger.addHandler(fh_handler)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.358957Z","start_time":"2020-04-11T02:14:24.187096Z"},"id":"OC5DOcDifSxx","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def seed_everything(seed : int) -> NoReturn :\n","    \n","    rn.seed(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    tf.random.set_seed(seed)\n","    # os.environ['TF_CUDNN_DETERMINISTIC'] = str(seed) \n","\n","seed_everything(SEED)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5tInVDxBn-lQ","trusted":true,"colab_type":"code","colab":{}},"source":["class CyclicLR(tf.keras.callbacks.Callback):\n","\n","    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n","                 gamma=1., scale_fn=None, scale_mode='cycle'):\n","        super(CyclicLR, self).__init__()\n","\n","        self.base_lr = base_lr\n","        self.max_lr = max_lr\n","        self.step_size = step_size\n","        self.mode = mode\n","        self.gamma = gamma\n","        if scale_fn == None:\n","            if self.mode == 'triangular':\n","                self.scale_fn = lambda x: 1.\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'triangular2':\n","                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'exp_range':\n","                self.scale_fn = lambda x: gamma ** (x)\n","                self.scale_mode = 'iterations'\n","        else:\n","            self.scale_fn = scale_fn\n","            self.scale_mode = scale_mode\n","        self.clr_iterations = 0.\n","        self.trn_iterations = 0.\n","        self.history = {}\n","\n","        self._reset()\n","\n","    def _reset(self, new_base_lr=None, new_max_lr=None,\n","               new_step_size=None):\n","        \"\"\"Resets cycle iterations.\n","        Optional boundary/step size adjustment.\n","        \"\"\"\n","        if new_base_lr != None:\n","            self.base_lr = new_base_lr\n","        if new_max_lr != None:\n","            self.max_lr = new_max_lr\n","        if new_step_size != None:\n","            self.step_size = new_step_size\n","        self.clr_iterations = 0.\n","\n","    def clr(self):\n","        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n","        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n","        if self.scale_mode == 'cycle':\n","            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n","        else:\n","            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n","                self.clr_iterations)\n","\n","    def on_train_begin(self, logs={}):\n","        logs = logs or {}\n","\n","        if self.clr_iterations == 0:\n","            K.set_value(self.model.optimizer.lr, self.base_lr)\n","        else:\n","            K.set_value(self.model.optimizer.lr, self.clr())\n","\n","    def on_batch_end(self, epoch, logs=None):\n","\n","        logs = logs or {}\n","        self.trn_iterations += 1\n","        self.clr_iterations += 1\n","\n","        K.set_value(self.model.optimizer.lr, self.clr())\n","        # print(\"learning rate- self.model.optimizer.lr: \", self.model.optimizer.lr)\n","\n","    # def on_epoch_end(self, epoch, logs=None):\n","\n","    #     logs = logs or {}\n","    #     self.trn_iterations += 1\n","    #     self.clr_iterations += 1\n","\n","    #     K.set_value(self.model.optimizer.lr, self.clr())\n","    #     logger.info(f'epoch:{epoch:03d},'+str(K.eval(self.model.optimizer.lr)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmKgNg1Mmzrk","trusted":true,"colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","# 写一个LossHistory类，保存训练集的loss和acc\n","# 当然我也可以完全不这么做，可以直接使用model.fit()方法返回的 history对象去做\n","'''Callback有6个常用的方法，这里实现其中的四个\n","    def on_epoch_begin(self, epoch, logs=None):\n","    def on_epoch_end(self, epoch, logs=None):\n","    def on_batch_begin(self, batch, logs=None):\n","    def on_batch_end(self, batch, logs=None):\n","    def on_train_begin(self, logs=None):\n","    def on_train_end(self, logs=None):\n","'''\n","class LossHistory(Callback):  # 继承自Callback类\n"," \n","    '''\n","    在模型开始的时候定义四个属性，每一个属性都是字典类型，存储相对应的值和epoch\n","    '''\n","    def on_train_begin(self, logs={}):\n","        self.losses = {'batch':[], 'epoch':[]}\n","        self.accuracy = {'batch':[], 'epoch':[]}\n","        self.val_loss = {'batch':[], 'epoch':[]}\n","        self.val_acc = {'batch':[], 'epoch':[]}\n"," \n","    # 在每一个batch结束后记录相应的值\n","    def on_batch_end(self, batch, logs={}):\n","        self.losses['batch'].append(logs.get('loss'))\n","        self.accuracy['batch'].append(logs.get('accuracy'))\n","        self.val_loss['batch'].append(logs.get('val_loss'))\n","        self.val_acc['batch'].append(logs.get('val_accuracy'))\n","    \n","    # 在每一个epoch之后记录相应的值\n","    def on_epoch_end(self, epoch, logs={}):\n","        trloss, tracc, vloss, vacc = logs.get('loss'), logs.get('accuracy'), logs.get('val_loss'), logs.get('val_accuracy')\n","        self.losses['epoch'].append(trloss)\n","        self.accuracy['epoch'].append(tracc)\n","        self.val_loss['epoch'].append(vloss)\n","        self.val_acc['epoch'].append(vacc)\n","        logger.info(\"epoch:{:03d}, train_loss:{:1.5f}, train_acc:{:1.5f}, val_loss:{:1.5f}, val_acc:{:1.5f}\".format(epoch, \n","                                                                                                                trloss, tracc, vloss, vacc))\n"," \n","    def loss_plot(self, loss_type, pngname):\n","        '''\n","        loss_type：指的是 'epoch'或者是'batch'，分别表示是一个batch之后记录还是一个epoch之后记录\n","        '''\n","        iters = range(len(self.losses[loss_type]))\n","        plt.figure()\n","        # acc\n","        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n","        # loss\n","        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n","        if loss_type == 'epoch':\n","            # val_acc\n","            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n","            # val_loss\n","            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n","        plt.grid(True)\n","        plt.xlabel(loss_type)\n","        plt.ylabel('acc-loss')\n","        plt.legend(loc=\"upper right\")\n","        plt.savefig(pngname)\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.368249Z","start_time":"2020-04-11T02:14:24.362616Z"},"id":"adUHGQUTfSyA","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def read_data(base : os.path.abspath) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n","    \n","    train = pd.read_csv(PATH+'clean-kalman/train_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n","    test  = pd.read_csv(PATH+'clean-kalman/test_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32})\n","    sub  = pd.read_csv(PATH+'liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n","\n","    Y_train_proba = np.load(PATH+\"ion-shifted-rfc-proba/Y_train_proba.npy\")\n","    Y_test_proba = np.load(PATH+\"ion-shifted-rfc-proba/Y_test_proba.npy\")\n","    \n","    for i in range(11):\n","        train[f\"proba_{i}\"] = Y_train_proba[:, i]\n","        test[f\"proba_{i}\"] = Y_test_proba[:, i]\n","    \n","    return train, test, sub\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.376185Z","start_time":"2020-04-11T02:14:24.371687Z"},"id":"HpDaJQ5yfSyI","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def batching(df : pd.DataFrame,\n","             batch_size : int) -> pd.DataFrame :\n","    \n","    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n","    df['group'] = df['group'].astype(np.uint16)\n","        \n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.391087Z","start_time":"2020-04-11T02:14:24.378989Z"},"id":"iQxOYF3tfSyj","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def reduce_mem_usage(df: pd.DataFrame,\n","                     verbose: bool = True) -> pd.DataFrame:\n","    \n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2\n","\n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","\n","            if str(col_type)[:3] == 'int':\n","\n","                if (c_min > np.iinfo(np.int32).min\n","                      and c_max < np.iinfo(np.int32).max):\n","                    df[col] = df[col].astype(np.int32)\n","                elif (c_min > np.iinfo(np.int64).min\n","                      and c_max < np.iinfo(np.int64).max):\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                if (c_min > np.finfo(np.float16).min\n","                        and c_max < np.finfo(np.float16).max):\n","                    df[col] = df[col].astype(np.float16)\n","                elif (c_min > np.finfo(np.float32).min\n","                      and c_max < np.finfo(np.float32).max):\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    reduction = (start_mem - end_mem) / start_mem\n","\n","    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n","    if verbose:\n","        print(msg)\n","\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.402186Z","start_time":"2020-04-11T02:14:24.393787Z"},"id":"dytmNmL_fSzj","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def lag_with_pct_change(df : pd.DataFrame,\n","                        shift_sizes : Optional[List]=[1, 2],\n","                        add_pct_change : Optional[bool]=False,\n","                        add_pct_change_lag : Optional[bool]=False,\n","                        add_diff : Optional[bool]=False) -> pd.DataFrame:\n","    \n","    for shift_size in shift_sizes:    \n","        df['signal_shift_pos_'+str(shift_size)] = df.groupby('group')['signal'].shift(shift_size).fillna(0)\n","        df['signal_shift_neg_'+str(shift_size)] = df.groupby('group')['signal'].shift(-1*shift_size).fillna(0)\n","\n","    if add_pct_change:\n","        df['pct_change'] = df['signal'].pct_change()\n","        if add_pct_change_lag:\n","            for shift_size in shift_sizes:    \n","                df['pct_change_shift_pos_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(shift_size).fillna(0)\n","                df['pct_change_shift_neg_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(-1*shift_size).fillna(0)\n","    if add_diff:\n","        for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'group', 'index']]:\n","            df[c+'_msignal'] = df[c] - df['signal']\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.409381Z","start_time":"2020-04-11T02:14:24.404800Z"},"id":"m-ULWLF_fS0B","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def run_feat_enginnering(df : pd.DataFrame,\n","                         create_all_data_feats : bool,\n","                         batch_size : int) -> pd.DataFrame:\n","    \n","    df = batching(df, batch_size=batch_size)\n","    if create_all_data_feats:\n","        df = lag_with_pct_change(df, [1, 2, 3],  add_pct_change=False, add_pct_change_lag=False, add_diff=False)\n","    df['signal_2'] = df['signal'] ** 2\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.419991Z","start_time":"2020-04-11T02:14:24.412368Z"},"id":"6E87jcu2fS0O","trusted":true,"colab_type":"code","colab":{}},"source":["def feature_selection(df : pd.DataFrame,\n","                      df_test : pd.DataFrame) -> Tuple[pd.DataFrame , pd.DataFrame, List]:\n","    use_cols = [col for col in df.columns if col not in ['index','group', 'open_channels', 'time']]\n","    print(use_cols)\n","    df = df.replace([np.inf, -np.inf], np.nan)\n","    df_test = df_test.replace([np.inf, -np.inf], np.nan)\n","    for col in use_cols:\n","        col_mean = pd.concat([df[col], df_test[col]], axis=0).mean()\n","        df[col] = df[col].fillna(col_mean)\n","        df_test[col] = df_test[col].fillna(col_mean)\n","   \n","    gc.collect()\n","    return df, df_test, use_cols\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.427483Z","start_time":"2020-04-11T02:14:24.422535Z"},"trusted":true,"id":"Ft7_2jVZcOgQ","colab_type":"code","colab":{}},"source":["\n","def augment(X: np.array, y:np.array) -> Tuple[np.array, np.array]:\n","    \n","    X = np.vstack((X, np.flip(X, axis=1)))\n","    y = np.vstack((y, np.flip(y, axis=1)))\n","    \n","    return X, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.432964Z","start_time":"2020-04-11T02:14:24.430264Z"},"trusted":true,"id":"Y-nODxu7cOgS","colab_type":"code","colab":{}},"source":["# Add ops to save and restore all the variables.\n","# saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.438940Z","start_time":"2020-04-11T02:14:24.436211Z"},"trusted":true,"id":"VRzoz-eacOgU","colab_type":"code","colab":{}},"source":["# # %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:24:41.652529Z\",\"start_time\":\"2020-04-03T23:24:41.645025Z\"}}\n","# class EarlyStopping:\n","#     def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n","#         self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n","#         self.counter, self.best_score = 0, None\n","#         self.is_maximize = is_maximize\n","\n","#     def load_best_weights(self, sess):\n","#         saver.restore(sess, self.checkpoint_path)\n","\n","#     def __call__(self, score, sess):\n","#         if self.best_score is None or \\\n","#         (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n","#             saver.save(sess, self.checkpoint_path)\n","#             self.best_score, self.counter = score, 0\n","#         else:\n","#             self.counter += 1\n","#             if self.counter >= self.patience:\n","#                 return True\n","#         return False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6l0nc-llBdy","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.477074Z","start_time":"2020-04-11T02:14:24.441269Z"},"code_folding":[],"id":"e4QulGxHfS0n","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def run_cv_model_by_batch(train : pd.DataFrame,\n","                          test : pd.DataFrame,\n","                          splits : int,\n","                          batch_col : Text,\n","                          feats : List,\n","                          sample_submission: pd.DataFrame,\n","                          nn_epochs : int,\n","                          nn_batch_size : int) -> NoReturn:\n","    seed_everything(SEED)\n","    K.clear_session()\n","    if not os.path.exists(outdir):\n","      os.mkdir(outdir)\n","    print(outdir)\n","    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n","    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n","    tf.compat.v1.keras.backend.set_session(sess)\n","    oof_ = np.zeros((len(train), SPLITS, 11))\n","    preds_ = np.zeros((len(test), SPLITS, 11))\n","    target = ['open_channels']\n","    group = train['group']\n","    kf = GroupKFold(n_splits=5)\n","    splits = [x for x in kf.split(train, train[target], group)]\n","\n","    new_splits = []\n","    for sp in splits:\n","        new_split = []\n","        new_split.append(np.unique(group[sp[0]]))\n","        new_split.append(np.unique(group[sp[1]]))\n","        new_split.append(sp[1])    \n","        new_splits.append(new_split)\n","\n","    # Calculate the weights for each class so that we can balance the data\n","    weights_ = class_weight.compute_class_weight('balanced',\n","                                                np.unique(train.open_channels),\n","                                                train.open_channels)\n","    print(\"weights_:\", weights_)\n","\n","    nums_ = train.open_channels.value_counts(sort=False)\n","    print(\"nums_:\", nums_)\n","    beta = BETA\n","    effective_num = 1.0 - np.power(beta, nums_)\n","    weights = (1.0 - beta) / np.array(effective_num)\n","    cb_weights_ = weights / np.sum(weights) * 11\n","    print(\"cb_weights_:\", cb_weights_)   \n","        \n","    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n","\n","    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n","    # print(tr.head())\n","    target_cols = ['target_'+str(i) for i in range(11)]\n","    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n","#     print(np.shape(train_tr))\n","    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n","#     print(np.shape(train))\n","    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n","#     print(np.shape(test))\n","\n","    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n","        sub_dir = os.path.join(outdir,\"{}_fold\".format(n_fold))\n","        # if n_fold <4:\n","        #     continue\n","        if not os.path.exists(sub_dir):\n","            os.mkdir(sub_dir)\n","        \n","        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n","        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n","        \n","        # if n_fold < 2:\n","        train_x, train_y = augment(train_x, train_y)\n","\n","        gc.collect()\n","        shape_ = (None, train_x.shape[2])\n","        model = ClassifierCW(shape_, cb_weights_)\n","        print(\"model initilization done!\")\n","        # cb_lr_schedule = LearningRateScheduler(lr_schedule)\n","        cb_clr = CyclicLR(base_lr=1e-7, max_lr = LR, step_size= int(1.0*(train.shape[0])/(nn_batch_size*4)) , \n","                          mode='exp_range', gamma=Gamma, scale_fn=None, scale_mode='cycle')\n","        lr_scheduler = LRSchedulerPerStep(256, 4000) \n","        cb_prg = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,leave_overall_progress=False, \n","                                               show_epoch_progress=False,show_overall_progress=True)\n","        # cb_csv_logger= CSVLogger(os.path.join(sub_dir,'res.csv'))\n","        # cb_history = LossHistory()  # 这里是使用自定义的Callback回调函数，当然本身fit函数也会返回一个history可供使用\n","        save_checkpoint_path = os.path.join(sub_dir,'checkpoint-acc-modelonly-{}.h5'.format(n_fold))\n","        save_finalmodel_path = os.path.join(sub_dir,'fmodel-modelonly-{}.h5'.format(n_fold))\n","        save_bestf1macro_path = os.path.join(sub_dir,'checkpoint-modelonly-{}.h5'.format(n_fold)) \n","        if not Prediction:\n","            if Kaggle:\n","                cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_checkpoint_path,\n","                                                    monitor='val_get_f1',\n","                                                    mode = 'max',            \n","                                                    save_weights_only=True,\n","                                                    save_best_only=True,           \n","                                                    verbose=1)\n","                history = model.fit(train_x,train_y,\n","                                  epochs=nn_epochs,\n","                                  callbacks=[cb_prg, lr_scheduler, cp_callback,\n","                                              MacroF1ES(model, valid_x, valid_y, patience=30, delta=0, \n","                                                        checkpoint_path=save_bestf1macro_path)],\n","                                  batch_size=nn_batch_size,\n","                                  verbose=1,\n","                                  validation_data=(valid_x,valid_y))\n","                model.save_weights(save_finalmodel_path)\n","            else:\n","                history = model.fit(train_x,train_y,\n","                      epochs=nn_epochs,\n","                      callbacks=[cb_prg, lr_scheduler,\n","                                  MacroF1ES(model, valid_x, valid_y, patience=70, delta=0, \n","                                            checkpoint_path=save_bestf1macro_path)],\n","                      batch_size=nn_batch_size,\n","                      verbose=1,\n","                      validation_data=(valid_x,valid_y))\n","                model.save_weights(save_finalmodel_path)\n","\n","        # pd.DataFrame(history.history).to_csv(os.path.join(sub_dir,'{}-len{}-lr{}-{}-log-{}.csv'.format(MODELNAME,BATCHSIZE,LR,timestampStr, n_fold)), float_format='%.4f')\n","        # print('\\nhistory dict:', history.history)\n","        # cb_history.loss_plot('epoch',os.path.join(outdir,'{}_f{}.png'.format(VERSION, n_fold)))\n","        model.load_weights(save_bestf1macro_path)  \n","\n","        preds_f = model.predict(valid_x)\n","        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  \n","                             np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n","        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n","        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n","        oof_[val_orig_idx,n_fold,:] = preds_f\n","        te_preds = model.predict(test)\n","        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n","        preds_ [:,n_fold]= te_preds\n","    \n","    predv = ''\n","    if use_average:\n","        predv = 'av'\n","        p0 = np.argmax(np.mean(oof_, axis = 1), axis = 1)\n","        p1 = np.argmax(np.mean(preds_, axis = 1), axis = 1)\n","         \n","    elif add_weights:\n","        predv = 'wmm'\n","        proba0 = np.max(oof_, axis=1)\n","        p0 = np.argmax(proba0, axis=1)\n","        proba1 = np.max(preds_, axis=1)\n","        p1 = np.argmax(proba1, axis=1)\n","\n","        EXP = weight_exp\n","        s = pd.Series(p0)\n","        vc = s.value_counts().sort_index()\n","        df = pd.DataFrame({'a':np.arange(11),'b':np.ones(11)})\n","        df.b = df.a.map(vc)\n","        df.fillna(df.b.min(),inplace=True)\n","        mat1 = np.diag(df.b.astype('float32')**EXP)\n","        print('mat1', mat1)\n","\n","        s = pd.Series(p1)\n","        vc = s.value_counts().sort_index()\n","        df = pd.DataFrame({'a':np.arange(11),'b':np.ones(11)})\n","        df.b = df.a.map(vc)\n","        df.fillna(df.b.min(),inplace=True)\n","        mat2 = np.diag(df.b.astype('float32')**EXP)\n","\n","        p0 = np.argmax( proba0.dot(mat1), axis=1)\n","        p1 = np.argmax( proba1.dot(mat2), axis=1)\n","    else:\n","        predv = 'mm'\n","        proba0 = np.max(oof_, axis=1)\n","        p0 = np.argmax(proba0, axis=1)\n","        proba1 = np.max(preds_, axis=1)\n","        p1 = np.argmax(proba1, axis=1)\n","\n","    f1_score_ =f1_score(np.argmax(train_tr, axis=2).reshape(-1),  p0, average = 'macro')\n","    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n","    sample_submission['open_channels'] = p1.astype(int)\n","    sample_submission.to_csv(os.path.join(outdir,'{}_pred-{}-all{}.csv'.format(VERSION, (\"prediction\" if Prediction else \"train\"), predv)), index=False, float_format='%.4f')\n","    display(sample_submission.head())\n","\n","    return \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2drz7lYE0wj9","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.485692Z","start_time":"2020-04-11T02:14:24.479620Z"},"id":"dqt0VJAS2ucB","trusted":true,"colab_type":"code","colab":{}},"source":["def lr_schedule(epoch):\n","    if epoch < 40:\n","        lr = LR\n","    elif epoch < 50:\n","        lr = LR / 3\n","    elif epoch < 60:\n","        lr = LR / 6\n","    elif epoch < 75:\n","        lr = LR / 9\n","    elif epoch < 85:\n","        lr = LR / 12\n","    elif epoch < 100:\n","        lr = LR / 15\n","    else:\n","        lr = LR / 50\n","    return lr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aaClcd8zJIyC","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.499088Z","start_time":"2020-04-11T02:14:24.488223Z"},"id":"SjbrEQSa2ucN","trusted":true,"colab_type":"code","colab":{}},"source":["class Mish(tf.keras.layers.Layer):\n","\n","    def __init__(self, **kwargs):\n","        super(Mish, self).__init__(**kwargs)\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        return inputs * K.tanh(K.softplus(inputs))\n","\n","    def get_config(self):\n","        base_config = super(Mish, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","def mish(x):\n","\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n"," \n","from tensorflow.keras.utils import get_custom_objects\n","# from tensorflow.keras.layers import Activation\n","get_custom_objects().update({'mish': Activation(mish)})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.521416Z","start_time":"2020-04-11T02:14:24.502541Z"},"id":"dM8RiNfL2ucY","trusted":true,"colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras import initializers\n","from tensorflow.keras import regularizers\n","from tensorflow.keras import constraints\n","\n","class Attention(Layer):\n","    \"\"\"Multi-headed attention layer.\"\"\"\n","    \n","    def __init__(self, hidden_size, \n","                 num_heads = 8, \n","                 attention_dropout=.1,\n","                 trainable=True,\n","                 name='Attention'):\n","        \n","        if hidden_size % num_heads != 0:\n","            raise ValueError(\"Hidden size must be evenly divisible by the number of heads.\")\n","            \n","        self.hidden_size = hidden_size\n","        self.num_heads = num_heads\n","        self.trainable = trainable\n","        self.attention_dropout = attention_dropout\n","        self.dense = tf.keras.layers.Dense(self.hidden_size, use_bias=False)\n","        super(Attention, self).__init__(name=name)\n","\n","    def split_heads(self, x):\n","        \"\"\"Split x into different heads, and transpose the resulting value.\n","        The tensor is transposed to insure the inner dimensions hold the correct\n","        values during the matrix multiplication.\n","        Args:\n","          x: A tensor with shape [batch_size, length, hidden_size]\n","        Returns:\n","          A tensor with shape [batch_size, num_heads, length, hidden_size/num_heads]\n","        \"\"\"\n","        with tf.name_scope(\"split_heads\"):\n","            batch_size = tf.shape(x)[0]\n","            length = tf.shape(x)[1]\n","\n","            # Calculate depth of last dimension after it has been split.\n","            depth = (self.hidden_size // self.num_heads)\n","\n","            # Split the last dimension\n","            x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n","\n","            # Transpose the result\n","            return tf.transpose(x, [0, 2, 1, 3])\n","    \n","    def combine_heads(self, x):\n","        \"\"\"Combine tensor that has been split.\n","        Args:\n","          x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n","        Returns:\n","          A tensor with shape [batch_size, length, hidden_size]\n","        \"\"\"\n","        with tf.name_scope(\"combine_heads\"):\n","            batch_size = tf.shape(x)[0]\n","            length = tf.shape(x)[2]\n","            x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n","            return tf.reshape(x, [batch_size, length, self.hidden_size])        \n","\n","    def call(self, inputs):\n","        \"\"\"Apply attention mechanism to inputs.\n","        Args:\n","          inputs: a tensor with shape [batch_size, length_x, hidden_size]\n","        Returns:\n","          Attention layer output with shape [batch_size, length_x, hidden_size]\n","        \"\"\"\n","        # Google developper use tf.layer.Dense to linearly project the queries, keys, and values.\n","        q = self.dense(inputs)\n","        k = self.dense(inputs)\n","        v = self.dense(inputs)\n","\n","        q = self.split_heads(q)\n","        k = self.split_heads(k)\n","        v = self.split_heads(v)\n","        \n","        # Scale q to prevent the dot product between q and k from growing too large.\n","        depth = (self.hidden_size // self.num_heads)\n","        q *= depth ** -0.5\n","        \n","        logits = tf.matmul(q, k, transpose_b=True)\n","        # logits += self.bias\n","        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n","        \n","        if self.trainable:\n","            weights = tf.nn.dropout(weights, 1.0 - self.attention_dropout)\n","        \n","        attention_output = tf.matmul(weights, v)\n","        attention_output = self.combine_heads(attention_output)\n","        attention_output = self.dense(attention_output)\n","        return attention_output\n","        \n","    def compute_output_shape(self, input_shape):\n","        return tf.TensorShape(input_shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.531056Z","start_time":"2020-04-11T02:14:24.524047Z"},"id":"bG4cGcYYNVYw","trusted":true,"colab_type":"code","colab":{}},"source":["def categorical_focal_loss(gamma=2.0, alpha=0.25):\n","    \"\"\"\n","    Implementation of Focal Loss from the paper in multiclass classification\n","    Formula:\n","        loss = -alpha*((1-p)^gamma)*log(p)\n","    Parameters:\n","        alpha -- the same as wighting factor in balanced cross entropy\n","        gamma -- focusing parameter for modulating factor (1-p)\n","    Default value:\n","        gamma -- 2.0 as mentioned in the paper\n","        alpha -- 0.25 as mentioned in the paper\n","    \"\"\"\n","    epsilon = 1.e-7\n","    gamma = float(gamma)\n","    alpha = tf.constant(alpha, dtype=tf.float32)\n","    def loss(y_true, y_pred):\n","        # Define epsilon so that the backpropagation will not result in NaN\n","        # for 0 divisor case\n","        # epsilon = K.epsilon()\n","        # Add the epsilon to prediction value\n","        #y_pred = y_pred + epsilon\n","        # Clip the prediction value\n","        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0-epsilon)\n","        # Calculate cross entropy\n","        cross_entropy = -tf.multiply(y_true,tf.math.log(y_pred))\n","        # Calculate weight that consists of  modulating factor and weighting factor\n","        weight = tf.multiply(y_true , tf.pow((tf.subtract(1.,y_pred)), gamma))\n","        weight = tf.multiply(alpha, weight)\n","        # Calculate focal loss\n","        wloss = tf.multiply(weight , cross_entropy)\n","        # Sum the losses in mini_batch\n","        wloss = tf.reduce_mean(wloss)\n","        return wloss\n","\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYJFMgfk8VXh","trusted":true,"colab_type":"code","colab":{}},"source":["# focal loss with one-hot labels, multiclass\n","def focal_loss(classes_num, gamma=2., alpha=.25, e=3e-4):\n","    # classes_num contains sample number of each classes\n","    def focal_loss_fixed(target_tensor, prediction_tensor):\n","        '''\n","        prediction_tensor is the output tensor with shape [None, 100], where 100 is the number of classes\n","        target_tensor is the label tensor, same shape as predcition_tensor\n","        '''\n","\n","        #1# get focal loss with no balanced weight which presented in paper function (4)\n","        zeros = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n","        one_minus_p = array_ops.where(tf.greater(target_tensor,zeros), target_tensor - prediction_tensor, zeros)\n","        FT = -1 * (one_minus_p ** gamma) * tf.math.log(tf.clip_by_value(prediction_tensor, 1e-8, 1.0))\n","\n","        #2# get balanced weight alpha\n","        classes_weight = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n","\n","        total_num = float(sum(classes_num))\n","        classes_w_t1 = [ total_num / ff for ff in classes_num ]\n","        sum_ = sum(classes_w_t1)\n","        classes_w_t2 = [ ff/sum_ for ff in classes_w_t1 ]   #scale\n","        classes_w_tensor = tf.convert_to_tensor(classes_w_t2, dtype=prediction_tensor.dtype)\n","        classes_weight += classes_w_tensor\n","\n","        alpha = array_ops.where(tf.greater(target_tensor, zeros), classes_weight, zeros)\n","\n","        #3# get balanced focal loss\n","        balanced_fl = alpha * FT\n","        balanced_fl = tf.reduce_mean(balanced_fl)\n","\n","        #4# add other op to prevent overfit\n","        # reference : https://spaces.ac.cn/archives/4493\n","        nb_classes = len(classes_num)\n","        fianal_loss = (1-e) * balanced_fl + e * K.categorical_crossentropy(K.ones_like(prediction_tensor)/nb_classes, prediction_tensor)\n","\n","        return fianal_loss\n","    return focal_loss_fixed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vrzikcyx8ptJ","trusted":true,"colab_type":"code","colab":{}},"source":["# focal loss with one-hot labels, multiclass\n","def cb_focal_loss(classes_weights, gamma=2., alpha=.25, w=3e-4):\n","    # classes_weights contains weights of each classes\n","    def cb_focal_loss_fixed(target_tensor, prediction_tensor):\n","        '''\n","        prediction_tensor is the output tensor with shape [None, 100], where 100 is the number of classes\n","        target_tensor is the label tensor, same shape as predcition_tensor\n","        '''\n","\n","        #1# get focal loss with no balanced weight which presented in paper function (4)\n","        zeros = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n","        one_minus_p = array_ops.where(tf.greater(target_tensor,zeros), target_tensor - prediction_tensor, zeros)\n","        FT = -1 * (one_minus_p ** gamma) * tf.math.log(tf.clip_by_value(prediction_tensor, 1e-8, 1.0))\n","\n","        #2# get balanced weight alpha\n","        classes_weight = array_ops.zeros_like(prediction_tensor, dtype=prediction_tensor.dtype)\n","        classes_w_tensor = tf.convert_to_tensor(classes_weights, dtype=prediction_tensor.dtype)\n","        classes_weight += classes_w_tensor\n","\n","        alpha = array_ops.where(tf.greater(target_tensor, zeros), classes_weight, zeros)\n","\n","        #3# get balanced focal loss\n","        balanced_fl = alpha * FT\n","        balanced_fl = tf.reduce_mean(balanced_fl)\n","\n","        #4# add other op to prevent overfit\n","        # reference : https://spaces.ac.cn/archives/4493\n","        nb_classes = len(classes_weights)\n","        fianal_loss = (1-w) * balanced_fl + w * K.categorical_crossentropy(K.ones_like(prediction_tensor)/nb_classes, prediction_tensor)\n","\n","        return fianal_loss\n","    return cb_focal_loss_fixed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JM2iC9UwFd-c","trusted":true,"colab_type":"code","colab":{}},"source":["def multi_category_focal_loss2(gamma=2., alpha=.25):\n","    \"\"\"\n","    focal loss for multi category of multi label problem\n","    适用于多分类或多标签问题的focal loss\n","    alpha控制真值y_true为1/0时的权重\n","        1的权重为alpha, 0的权重为1-alpha\n","    当你的模型欠拟合，学习存在困难时，可以尝试适用本函数作为loss\n","    当模型过于激进(无论何时总是倾向于预测出1),尝试将alpha调小\n","    当模型过于惰性(无论何时总是倾向于预测出0,或是某一个固定的常数,说明没有学到有效特征)\n","        尝试将alpha调大,鼓励模型进行预测出1。\n","    Usage:\n","     model.compile(loss=[multi_category_focal_loss2(alpha=0.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n","    \"\"\"\n","    epsilon = 1.e-7\n","    gamma = float(gamma)\n","    alpha = tf.constant(alpha, dtype=tf.float32)\n","\n","    def multi_category_focal_loss2_fixed(y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.float32)\n","        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n","    \n","        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n","        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n","        ce = -tf.math.log(y_t)\n","        weight = tf.pow(tf.subtract(1., y_t), gamma)\n","        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n","        loss = tf.reduce_mean(fl)\n","        return loss\n","    return multi_category_focal_loss2_fixed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAM_FzFH3RPv","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m8XTRWemZEoT","trusted":true,"colab_type":"code","colab":{}},"source":["def f1v2(y_true, y_pred):\n","    def recall(y_true, y_pred):\n","        \"\"\"Recall metric.\n","\n","        Only computes a batch-wise average of recall.\n","\n","        Computes the recall, a metric for multi-label classification of\n","        how many relevant items are selected.\n","        \"\"\"\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + K.epsilon())\n","        return recall\n","\n","    def precision(y_true, y_pred):\n","        \"\"\"Precision metric.\n","\n","        Only computes a batch-wise average of precision.\n","\n","        Computes the precision, a metric for multi-label classification of\n","        how many selected items are relevant.\n","        \"\"\"\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        return precision\n","    precision = precision(y_true, y_pred)\n","    recall = recall(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Efvocyr0Okcy","trusted":true,"colab_type":"code","colab":{}},"source":["def f1macro(y_true, y_pred):\n","    b,l,c = np.shape(y_true)\n","    y_pred = K.round(y_pred)\n","    tp = K.sum(K.sum(K.cast(y_true*y_pred, 'float'), axis=2),axis=1)\n","    tn = K.sum(K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=2),axis=1)\n","    fp = K.sum(K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=2),axis=1)\n","    fn = K.sum(K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=2),axis=1)\n","    p = tp / (tp + fp + K.epsilon())\n","    r = tp / (tp + fn + K.epsilon())\n","\n","    f1 = 2*p*r / (p+r+K.epsilon())\n","    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n","    return K.mean(f1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6d4mH_0Omj2","trusted":true,"colab_type":"code","colab":{}},"source":["def get_f1(y_true, y_pred): #taken from old keras source code\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BuSYzMF2REK9","trusted":true,"colab_type":"code","colab":{}},"source":["def fbeta_score_macro(y_true, y_pred, beta=1, threshold=0.5):\n","\n","    y_true = K.cast(y_true, 'float')\n","    y_pred = K.cast(K.greater(K.cast(y_pred, 'float'), threshold), 'float')\n","\n","    tp = K.sum(y_true * y_pred, axis=0)\n","    fp = K.sum((1 - y_true) * y_pred, axis=0)\n","    fn = K.sum(y_true * (1 - y_pred), axis=0)\n","\n","    p = tp / (tp + fp + K.epsilon())\n","    r = tp / (tp + fn + K.epsilon())\n","\n","    f1 = (1 + beta ** 2) * p * r / ((beta ** 2) * p + r + K.epsilon())\n","    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n","\n","    return K.mean(f1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.549703Z","start_time":"2020-04-11T02:14:24.533479Z"},"id":"YSESlpaNwkkx","trusted":true,"colab_type":"code","colab":{}},"source":["def WaveNetResidualConv1D(num_filters, kernel_size, stacked_layer):\n","\n","    def build_residual_block(l_input):\n","        resid_input = l_input\n","        for dilation_rate in [2**i for i in range(stacked_layer)]:\n","            l_sigmoid_conv1d = Conv1D(\n","              num_filters, kernel_size, dilation_rate=dilation_rate,\n","              padding='same', activation='sigmoid')(l_input)\n","            l_tanh_conv1d = Conv1D(\n","             num_filters, kernel_size, dilation_rate=dilation_rate,\n","             padding='same', activation='mish')(l_input)\n","            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n","            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n","            resid_input = Add()([resid_input ,l_input])\n","        return resid_input\n","    return build_residual_block\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1PeJF2_Iad7g","trusted":true,"colab_type":"code","colab":{}},"source":["def WaveNetResidualConvBn1D(num_filters, kernel_size, stacked_layer):\n","\n","    def build_bn_residual_block(l_input):\n","        resid_input = l_input\n","        for dilation_rate in [2**i for i in range(stacked_layer)]:\n","            conv1d1 = Conv1D(\n","              num_filters, kernel_size, dilation_rate=dilation_rate,\n","              padding='same')(l_input)\n","            bn_conv1d1 = BatchNormalization()(conv1d1)\n","            l_sigmoid_conv1d = Activation('sigmoid')(bn_conv1d1)\n","            conv1d2 = Conv1D(\n","              num_filters, kernel_size, dilation_rate=dilation_rate,\n","              padding='same')(l_input)\n","            bn_conv1d2 = BatchNormalization()(conv1d2)\n","            l_tanh_conv1d = Activation('mish')(bn_conv1d2)\n","            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n","            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n","            resid_input = Add()([resid_input ,l_input])\n","        return resid_input\n","    return build_bn_residual_block"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GEn4iIRTaslB","trusted":true,"colab_type":"code","colab":{}},"source":["def Classifier(shape_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    l_output = Dense(11, activation='softmax')(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"erbbJrcxlkdS","trusted":true,"colab_type":"code","colab":{}},"source":["# def weighted_categorical_crossentropy(weights):\n","#     \"\"\"\n","#     A weighted version of keras.objectives.categorical_crossentropy\n","    \n","#     Variables:\n","#         weights: numpy array of shape (C,) where C is the number of classes\n","    \n","#     Usage:\n","#         weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n","#         loss = weighted_categorical_crossentropy(weights)\n","#         model.compile(loss=loss,optimizer='adam')\n","#     \"\"\"\n","    \n","#     weights = K.variable(weights)\n","        \n","#     def loss(y_true, y_pred):\n","#         # scale predictions so that the class probas of each sample sum to 1\n","#         y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n","#         # clip to prevent NaN's and Inf's\n","#         y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","#         # calc\n","#         loss = y_true * K.log(y_pred) * weights\n","#         loss = -K.sum(loss, -1)\n","#         return loss\n","    \n","#     return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ixp9c4OZrZL9","trusted":true,"colab_type":"code","colab":{}},"source":["# def w_categorical_crossentropy(weights):\n","#     \"\"\"\n","#     Keras-style categorical crossentropy loss function, with weighting for each class.\n","\n","#     Parameters\n","#     ----------\n","#     y_true : Tensor\n","#         Truth labels.\n","#     y_pred : Tensor\n","#         Predicted values.\n","#     weights: Tensor\n","#         Multiplicative factor for loss per class.\n","\n","#     Returns\n","#     -------\n","#     loss : Tensor\n","#         Weighted crossentropy loss between labels and predictions.\n","#     \"\"\"\n","#     weights = K.variable(weights)\n","#     def loss(y_true, y_pred):\n","#         y_true_max = K.argmax(y_true, axis=-1)\n","#         weighted_true = K.gather(weights, y_true_max)\n","#         loss = K.categorical_crossentropy(y_pred, y_true)*weighted_true\n","#         return loss\n","\n","#     return loss "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uX3YFhsglyLz","trusted":true,"colab_type":"code","colab":{}},"source":["# # # Custom loss function with costs\n","# def w_categorical_crossentropy(y_true, y_pred, weights):\n","#     nb_cl = len(weights)\n","#     final_mask = K.zeros_like(y_pred[:, 0])\n","#     y_pred_max = K.max(y_pred, axis=1)\n","#     y_pred_max = K.expand_dims(y_pred_max, 1)\n","#     y_pred_max_mat = K.equal(y_pred, y_pred_max)\n","#     for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n","#         final_mask += (K.cast(weights[c_t, c_p],K.floatx()) * K.cast(y_pred_max_mat[:, c_p] ,K.floatx())* \n","#                        K.cast(y_true[:, c_t],K.floatx()))\n","#     return K.categorical_crossentropy(y_pred, y_true) * final_mask\n","\n","# w_array = np.ones((3,3))\n","# w_array[2,1] = 1.2\n","# w_array[1,2] = 1.2\n","# ncce = partial(w_categorical_crossentropy, weights=w_array)\n","# ncce.__name__ ='w_categorical_crossentropy'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZymKtoVo8F06","trusted":true,"colab_type":"code","colab":{}},"source":["def ClassifierW(shape_, weights_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]*2\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    l_output = Dense(11, activation='softmax')(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","\n","    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n","    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n","\n","#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n","#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CO1ukCIvzgVZ","colab_type":"code","colab":{}},"source":["# # if tf.keras.__version__ > \"2.2.0\":\n","# #     from tensorflow.keras.backend import normalize_data_format\n","# # else:\n","# #     from tensorflow.keras.utils.conv_utils import normalize_data_format\n","\n","# class DropBlock1D(tf.keras.layers.Layer):\n","#     \"\"\"See: https://arxiv.org/pdf/1810.12890.pdf\"\"\"\n","\n","#     def __init__(self,\n","#                  block_size,\n","#                  keep_prob,\n","#                  sync_channels=False,\n","#                  data_format=None,\n","#                  **kwargs):\n","#         \"\"\"Initialize the layer.\n","#         :param block_size: Size for each mask block.\n","#         :param keep_prob: Probability of keeping the original feature.\n","#         :param sync_channels: Whether to use the same dropout for all channels.\n","#         :param data_format: 'channels_first' or 'channels_last' (default).\n","#         :param kwargs: Arguments for parent class.\n","#         \"\"\"\n","#         super(DropBlock1D, self).__init__(**kwargs)\n","#         self.block_size = block_size\n","#         self.keep_prob = keep_prob\n","#         self.sync_channels = sync_channels\n","#         self.data_format = 'channels_last' #K.image_data_format()\n","#         self.input_spec = tf.keras.layers.InputSpec(ndim=3)\n","#         self.supports_masking = True\n","\n","#     def get_config(self):\n","#         config = {'block_size': self.block_size,\n","#                   'keep_prob': self.keep_prob,\n","#                   'sync_channels': self.sync_channels,\n","#                   'data_format': self.data_format}\n","#         # print(\"config:\", config)\n","#         base_config = super(DropBlock1D, self).get_config()\n","#         return dict(list(base_config.items()) + list(config.items()))\n","\n","#     def compute_mask(self, inputs, mask=None):\n","#         return mask\n","\n","#     def compute_output_shape(self, input_shape):\n","#         return input_shape\n","\n","#     def _get_gamma(self, feature_dim):\n","#         \"\"\"Get the number of activation units to drop\"\"\"\n","#         feature_dim = K.cast(feature_dim, K.floatx())\n","#         # print(\"feature_dim:\", K.get_value(feature_dim))\n","#         block_size = K.constant(self.block_size, dtype=K.floatx())\n","#         gamma = ((1.0 - self.keep_prob) / block_size) * (feature_dim / (feature_dim - block_size + 1.0))\n","#         print(\"gamma:\", K.print_tensor(gamma))\n","#         return gamma\n","\n","#     # def _compute_valid_seed_region(self, seq_length):\n","#     #     positions = K.arange(start = 0, stop = seq_length)\n","#     #     half_block_size = self.block_size // 2\n","#     #     valid_seed_region = K.switch(\n","#     #         K.all(\n","#     #             K.stack(\n","#     #                 [\n","#     #                     positions >= half_block_size,\n","#     #                     positions < seq_length - half_block_size,\n","#     #                 ],\n","#     #                 axis=-1,\n","#     #             ),\n","#     #             axis=-1,\n","#     #         ),\n","#     #         K.ones((seq_length,)),\n","#     #         K.zeros((seq_length,)),\n","#     #     )\n","#     #     return K.expand_dims(K.expand_dims(valid_seed_region, axis=0), axis=-1)\n","\n","#     def _compute_drop_mask(self, shape):\n","#         print(\"shape:\", K.print_tensor(shape))\n","#         seq_length = shape[1]\n","#         print(\"seq_length:\", K.print_tensor(seq_length))\n","#         mask = K.random_binomial(shape, p=self._get_gamma(seq_length))\n","\n","#         positions = K.arange(start = 0, stop = seq_length)\n","#         half_block_size = self.block_size // 2\n","#         valid_seed_region = K.switch(\n","#             K.all(\n","#                 K.stack(\n","#                     [\n","#                         positions >= half_block_size,\n","#                         positions < seq_length - half_block_size,\n","#                     ],\n","#                     axis=-1,\n","#                 ),\n","#                 axis=-1,\n","#             ),\n","#             K.ones((seq_length,)),\n","#             K.zeros((seq_length,)),\n","#         )\n","#         valid_seed_region_ex = K.expand_dims(K.expand_dims(valid_seed_region, axis=0), axis=-1)\n","#         mask *= valid_seed_region_ex\n","#         # mask *= self._compute_valid_seed_region(seq_length)\n","#         mask = MaxPool1D(\n","#             pool_size=self.block_size,\n","#             padding='same',\n","#             strides=1,\n","#             data_format='channels_last',\n","#         )(mask)\n","#         return 1.0 - mask\n","\n","#     def call(self, inputs, training=None):\n","\n","#         def dropped_inputs():\n","#             outputs = inputs\n","#             if self.data_format == 'channels_first':\n","#                 outputs = K.permute_dimensions(outputs, [0, 2, 1])\n","#             shape = K.shape(outputs)\n","#             if self.sync_channels:\n","#                 mask = self._compute_drop_mask([shape[0], shape[1], 1])\n","#             else:\n","#                 mask = self._compute_drop_mask(shape)\n","#             outputs = outputs * mask *\\\n","#                 (K.cast(K.prod(shape), dtype=K.floatx()) / K.sum(mask))\n","#             if self.data_format == 'channels_first':\n","#                 outputs = K.permute_dimensions(outputs, [0, 2, 1])\n","#             return outputs\n","\n","#         return K.in_train_phase(dropped_inputs, inputs, training=training)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhxvv2NSnJ9p","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVBENxS0NBeR","trusted":true,"colab_type":"code","colab":{}},"source":["def ClassifierG(shape_, weights_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]*2\n","    drop_block_size = [64, 16, 4]\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    # x = DropBlock1D(block_size=drop_block_size[0], keep_prob=0.8)(x)\n","    x = BatchNormalization()(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    x = BatchNormalization()(x)\n","    # x = DropBlock1D(block_size=drop_block_size[1], keep_prob=0.8)(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    x = BatchNormalization()(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.2)(x)\n","    # x = DropBlock1D(block_size=drop_block_size[2], keep_prob=0.8)(x)\n","    x = TimeDistributed(Dense(64, activation='mish'))(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.3)(x)\n","    l_output = TimeDistributed(Dense(11, activation='softmax'))(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","\n","    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n","    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n","\n","#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n","#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"apXJ3oU7lknF","colab_type":"code","colab":{}},"source":["def ClassifierD(shape_, weights_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]*2\n","    drop_block_size = [64, 16, 4]\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    # x = SpatialDropout1D(0.3)(x)\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    # x = SpatialDropout1D(0.3)(x)\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    # x = SpatialDropout1D(0.2)(x)\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    x = SpatialDropout1D(0.2)(x)\n","    x = TimeDistributed(Dense(64, activation='mish'))(x)\n","    x = SpatialDropout1D(0.3)(x)\n","    x = TimeDistributed(Dense(64, activation='mish'))(x)\n","    x = SpatialDropout1D(0.3)(x)\n","    l_output = TimeDistributed(Dense(11, activation='softmax'))(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","\n","    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n","    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n","\n","#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n","#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F13LxPVQb5xH","colab_type":"code","colab":{}},"source":["class LayerNormalization(Layer):\n","\tdef __init__(self, eps=1e-6, **kwargs):\n","\t\tself.eps = eps\n","\t\tsuper(LayerNormalization, self).__init__(**kwargs)\n","\tdef build(self, input_shape):\n","\t\tself.gamma = self.add_weight(name='gamma', shape=input_shape[-1:], initializer=Ones(), trainable=True)\n","\t\tself.beta = self.add_weight(name='beta', shape=input_shape[-1:], initializer=Zeros(), trainable=True)\n","\t\tsuper(LayerNormalization, self).build(input_shape)\n","\tdef call(self, x):\n","\t\tmean = K.mean(x, axis=-1, keepdims=True)\n","\t\tstd = K.std(x, axis=-1, keepdims=True)\n","\t\treturn self.gamma * (x - mean) / (std + self.eps) + self.beta\n","\tdef compute_output_shape(self, input_shape):\n","\t\treturn input_shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_PJDobFqb6Zy","colab_type":"code","colab":{}},"source":["class PositionwiseFeedForward():\n","\tdef __init__(self, d_hid, d_inner_hid, dropout=0.1):\n","\t\tself.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n","\t\tself.w_2 = Conv1D(d_hid, 1)\n","\t\tself.layer_norm = LayerNormalization()\n","\t\tself.dropout = Dropout(dropout)\n","\tdef __call__(self, x):\n","\t\toutput = self.w_1(x) \n","\t\toutput = self.w_2(output)\n","\t\toutput = self.dropout(output)\n","\t\toutput = Add()([output, x])\n","\t\treturn self.layer_norm(output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZGOZbfmReDZl","colab_type":"code","colab":{}},"source":["class LRSchedulerPerStep(Callback):\n","    def __init__(self, d_model, warmup=4000):\n","        self.basic = d_model**-0.5\n","        self.warm = warmup**-1.5\n","        self.step_num = 0\n","    def on_batch_begin(self, batch, logs = None):\n","        self.step_num += 1\n","        lr = self.basic * min(self.step_num**-0.5, self.step_num*self.warm)\n","        K.set_value(self.model.optimizer.lr, lr)\n","        print(\"learning rate: \", lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iLv9YNrrbvuP","colab_type":"code","colab":{}},"source":["def ClassifierD2(shape_, weights_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]*2\n","    d_inner_hid = 64\n","    l_input = Input(shape=(shape_))\n","    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n","    x = LayerNormalization()(x)\n","    x = PositionwiseFeedForward(num_filters_, d_inner_hid)(x)\n","\n","    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n","    x = LayerNormalization()(x)\n","    x = PositionwiseFeedForward(num_filters_*2, d_inner_hid*2)(x)\n","\n","    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n","    x = LayerNormalization()(x)\n","    x = PositionwiseFeedForward(num_filters_*4, d_inner_hid*2)(x)\n","\n","    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n","    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n","    x = LayerNormalization()(x)\n","    x = PositionwiseFeedForward(num_filters_*8, d_inner_hid*4)(x)\n","    \n","    x = SpatialDropout1D(0.2)(x)\n","    x = TimeDistributed(Dense(64, activation='mish'))(x)\n","    x = SpatialDropout1D(0.3)(x)\n","    x = TimeDistributed(Dense(64, activation='mish'))(x)\n","    x = SpatialDropout1D(0.3)(x)\n","    l_output = TimeDistributed(Dense(11, activation='softmax'))(x)\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","\n","    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n","    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n","\n","#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n","#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.563702Z","start_time":"2020-04-11T02:14:24.556844Z"},"id":"9Cckpy4I2uco","trusted":true,"colab_type":"code","colab":{}},"source":["def Classifierx(shape_):        \n","    #dsize = [256, 512, 256, 128, 11]\n","    dsize = [64, 128, 64, 32, 11]  \n","    inp = Input(shape=(shape_))\n","    x = Bidirectional(GRU(dsize[0], return_sequences=True))(inp)\n","    x = Attention(dsize[1])(x)\n","    x = TimeDistributed(Dense(dsize[2], activation='mish'))(x)\n","    x = TimeDistributed(Dense(dsize[3], activation='mish'))(x)\n","    out = TimeDistributed(Dense(dsize[4], activation='softmax', name='out'))(x)\n","    \n","    model = models.Model(inputs=inp, outputs=out) \n","    \n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, \n","                  metrics=['accuracy'])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GwYGnyF5iWYX","colab_type":"code","colab":{}},"source":["def cbr(x, out_layer, kernel, stride, dilation):\n","    x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"mish\")(x)\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_TP0PyAriXeU","colab_type":"code","colab":{}},"source":["def WaveNetBlock(l_input, num_filters, kernel_size, dilation): #, d_inner_hid, dropout_ratio = 0.1\n","    x = Conv1D(num_filters, 1, padding='same')(l_input)\n","    x = WaveNetResidualConv1D(num_filters, kernel_size, dilation)(x)\n","    # x = LayerNormalization()(x)\n","    x = BatchNormalization()(x)\n","    # x = PositionwiseFeedForward(num_filters, d_inner_hid, dropout_ratio)(x)\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hE3TuWanhem_","colab_type":"code","colab":{}},"source":["def ClassifierCW(shape_, weights_):\n","    num_filters_ = 16\n","    kernel_size_ = 3\n","    stacked_layers_ = [12, 8, 4, 1]\n","    # d_inner_hid = 64\n","    l_input = Input(shape=(shape_))\n","\n","    x = cbr(l_input, 64, 7, 1, 1)\n","    # x = BatchNormalization()(x)\n","    x = WaveNetBlock(x, num_filters_, kernel_size_, stacked_layers_[0])\n","    x = WaveNetBlock(x, num_filters_*2, kernel_size_, stacked_layers_[1])\n","    x = WaveNetBlock(x, num_filters_*4, kernel_size_, stacked_layers_[2])\n","    x = WaveNetBlock(x, num_filters_*8, kernel_size_, stacked_layers_[3])\n","    \n","    x = cbr(x, 32, 7, 1, 1)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.2)(x)\n","    l_output = Dense(11, activation='softmax')(x)\n","\n","    model = models.Model(inputs=[l_input], outputs=[l_output])\n","    opt = Adam(lr=LR)\n","    opt = tfa.optimizers.SWA(opt)\n","\n","#      MacroF1(model, valid_x,valid_y)\n","    model.compile(loss = [multi_category_focal_loss2(alpha=0.25, gamma=2)], optimizer=opt, \n","                  metrics=['accuracy', Precision(), Recall(), get_f1])\n","\n","    # model.compile(loss = [categorical_focal_loss(alpha=0.25, gamma=2)], optimizer=opt, \n","    #               metrics=['accuracy', Precision(), Recall(), get_f1])    \n","\n","#     model.compile(loss = [cb_focal_loss(weights_, alpha=0.25, gamma=2)], optimizer=opt, \n","#                   metrics=['accuracy', Precision(), Recall(), get_f1])    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.579491Z","start_time":"2020-04-11T02:14:24.567764Z"},"trusted":true,"id":"L4cQ0L8ucOgn","colab_type":"code","colab":{}},"source":["\n","class MacroF1ES(Callback):\n","    def __init__(self, model, inputs, targets, \n","                 patience=5, delta=0, checkpoint_path='checkpoint.h5', is_maximize=True):\n","        \n","        self.model = model\n","        self.inputs = inputs\n","        self.targets = np.argmax(targets, axis=2).reshape(-1)\n","        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n","        self.counter, self.best_score = 0, None\n","        self.is_maximize = is_maximize\n","        self.stopped_epoch = 0\n","        \n","    def on_epoch_end(self, epoch, logs):\n","        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n","        score = f1_score(self.targets, pred, average=\"macro\")\n","        logger.info(f'\\n epoch:{epoch:03d}, F1Macro: {score:.5f}')   \n","        \n","        if self.best_score is None or \\\n","        (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n","            self.model.save(self.checkpoint_path)\n","#             torch.save(model.state_dict(), self.checkpoint_path)\n","            self.best_score, self.counter = score, 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience: ##stop training\n","                self.stopped_epoch = epoch\n","                self.model.stop_training = True\n","                \n","    def on_train_end(self, logs=None):\n","        if self.stopped_epoch > 0:\n","              logger.info('Epoch %05d: early stopping' % (self.stopped_epoch + 1))   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"415IZHTAHw5g","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.589759Z","start_time":"2020-04-11T02:14:24.582640Z"},"id":"1yfpldH_2ucw","trusted":true,"colab_type":"code","colab":{}},"source":["class MacroF1(Callback):\n","    def __init__(self, model, inputs, targets):\n","        self.model = model\n","        self.inputs = inputs\n","        self.targets = np.argmax(targets, axis=2).reshape(-1)\n","\n","    def on_epoch_end(self, epoch, logs):\n","        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n","        score = f1_score(self.targets, pred, average=\"macro\")\n","        print(f' F1Macro: {score:.5f}')    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.596693Z","start_time":"2020-04-11T02:14:24.591931Z"},"id":"skru4lPt2uc6","trusted":true,"colab_type":"code","colab":{}},"source":["def normalize(train, test):\n","    \n","    train_input_mean = train.signal.mean()\n","    train_input_sigma = train.signal.std()\n","    train['signal'] = (train.signal-train_input_mean)/train_input_sigma\n","    test['signal'] = (test.signal-train_input_mean)/train_input_sigma\n","\n","    return train, test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:14:24.609602Z","start_time":"2020-04-11T02:14:24.600166Z"},"id":"02TDJtejfS0_","trusted":true,"colab_type":"code","colab":{}},"source":["\n","def run_everything(fe_config : List) -> NoReturn:\n","    not_feats_cols = ['time']\n","    target_col = ['open_channels']\n","    init_logger()\n","    with timer(f'Reading Data'):\n","        logger.info('Reading Data Started ...')\n","        base = os.path.abspath(PATH+'liverpool-ion-switching/')\n","        train, test, sample_submission = read_data(base)\n","        train, test = normalize(train, test)    \n","        logger.info('Reading and Normalizing Data Completed ...')\n","    with timer(f'Creating Features'):\n","        logger.info('Feature Enginnering Started ...')\n","        for config in fe_config:\n","            train = run_feat_enginnering(train, create_all_data_feats=config[0], batch_size=config[1])\n","            train = reduce_mem_usage(train)\n","            test  = run_feat_enginnering(test,  create_all_data_feats=config[0], batch_size=config[1])\n","            test = reduce_mem_usage(test)\n","        train, test, feats = feature_selection(train, test)\n","        logger.info('Feature Enginnering Completed ...')\n","\n","    with timer(f'Running Wavenet model'):\n","        logger.info(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started ...')\n","        run_cv_model_by_batch(train, test, splits=SPLITS, batch_col='group', feats=feats, \n","                              sample_submission=sample_submission, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n","        logger.info(f'Training completed ...')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2020-04-11T02:15:53.901793Z","start_time":"2020-04-11T02:14:24.612467Z"},"id":"_w4_OJJtfS1K","outputId":"9507dadb-d819-4a8a-f43a-7443ac080b32","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1587435937002,"user_tz":420,"elapsed":110772,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":904}},"source":["run_everything(fe_config)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["2020-04-21 02:23:54,807 INFO Reading Data Started ...\n","2020-04-21 02:24:05,565 INFO Reading and Normalizing Data Completed ...\n","2020-04-21 02:24:05,567 INFO [Reading Data] done in 11 s\n","2020-04-21 02:24:05,571 INFO Feature Enginnering Started ...\n"],"name":"stderr"},{"output_type":"stream","text":["Mem. usage decreased to 219.35 MB (71.6 % reduction)\n","Mem. usage decreased to 80.11 MB (73.4 % reduction)\n","['signal', 'proba_0', 'proba_1', 'proba_2', 'proba_3', 'proba_4', 'proba_5', 'proba_6', 'proba_7', 'proba_8', 'proba_9', 'proba_10', 'signal_shift_pos_1', 'signal_shift_neg_1', 'signal_shift_pos_2', 'signal_shift_neg_2', 'signal_shift_pos_3', 'signal_shift_neg_3', 'signal_2']\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-21 02:24:21,379 INFO Feature Enginnering Completed ...\n","2020-04-21 02:24:21,381 INFO [Creating Features] done in 16 s\n","2020-04-21 02:24:21,384 INFO Training Wavenet model with 5 folds of GroupKFold Started ...\n"],"name":"stderr"},{"output_type":"stream","text":["ion_switch/res/ClassifierCW-DLR-PROBF11-v1/\n","weights_: [ 0.36652399  0.46106257  0.82059173  0.67983748  1.12675802  1.63577934\n","  2.41635544  1.71516878  1.85390282  3.33929955 12.72060713]\n","nums_: 0     1240152\n","1      985865\n","2      553924\n","3      668609\n","4      403410\n","5      277877\n","6      188112\n","7      265015\n","8      245183\n","9      136120\n","10      35733\n","Name: open_channels, dtype: int64\n","cb_weights_: [0.98805081 0.98805081 0.98805081 0.98805081 0.98805081 0.98805087\n"," 0.9880632  0.98805094 0.98805122 0.98833125 1.11919846]\n","model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-21 02:24:46,947 INFO Training fold 1 completed. macro f1 score : 0.94005\n"],"name":"stderr"},{"output_type":"stream","text":["model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-21 02:24:58,789 INFO Training fold 2 completed. macro f1 score : 0.94121\n"],"name":"stderr"},{"output_type":"stream","text":["model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-21 02:25:10,220 INFO Training fold 3 completed. macro f1 score : 0.93992\n"],"name":"stderr"},{"output_type":"stream","text":["model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-21 02:25:21,179 INFO Training fold 4 completed. macro f1 score : 0.94013\n"],"name":"stderr"},{"output_type":"stream","text":["model initilization done!\n"],"name":"stdout"},{"output_type":"stream","text":["2020-04-21 02:25:32,600 INFO Training fold 5 completed. macro f1 score : 0.94024\n","2020-04-21 02:25:39,180 INFO Training completed. oof macro f1 score : 0.94033\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>time</th>\n","      <th>open_channels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>500.000092</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>500.000214</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>500.000305</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>500.000397</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>500.000488</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         time  open_channels\n","0  500.000092              0\n","1  500.000214              0\n","2  500.000305              0\n","3  500.000397              0\n","4  500.000488              0"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2020-04-21 02:25:44,129 INFO Training completed ...\n","2020-04-21 02:25:44,141 INFO [Running Wavenet model] done in 83 s\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ztG5LowrD8-U","trusted":true,"colab_type":"code","colab":{}},"source":[" "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kgip_hNXigaV","colab_type":"code","outputId":"9c09f78f-35e1-4de9-ea1d-41e2329de8ee","executionInfo":{"status":"ok","timestamp":1587435937005,"user_tz":420,"elapsed":110455,"user":{"displayName":"Yuan Lin","photoUrl":"","userId":"06444578246622712027"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"test\")"],"execution_count":58,"outputs":[{"output_type":"stream","text":["test\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gh6dcdbkeELI","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5Ei_ggnqUaq","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRsp7eyDhXy4","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFdl9TskgNxL","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekHbPv0UlCSS","trusted":true,"colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}