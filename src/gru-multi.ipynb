{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:20:21.272657Z",
     "start_time": "2020-04-06T19:20:19.347236Z"
    }
   },
   "outputs": [],
   "source": [
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:09.840829Z\",\"start_time\":\"2020-04-03T23:04:07.112516Z\"}}\n",
    "#######################################################\n",
    "# Much of this comes from https://www.kaggle.com/pradeeppathak9/gamma-log-facies-type-prediction\n",
    "# https://www.crowdanalytix.com/contests/gamma-log-facies-type-prediction\n",
    "######################################################\n",
    "import os\n",
    "os.system('pip install pytorch_toolbelt')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 1000\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import groupby, accumulate\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, LeaveOneGroupOut\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pytorch_toolbelt import losses as L\n",
    "\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:20:26.556633Z",
     "start_time": "2020-04-06T19:20:21.275582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before normalize\n",
      "0.5712801709920725 -0.3844742898121893\n",
      "0.5570543031187908 -0.3635959906107261\n",
      "after normalize\n",
      "Reading and Normalizing Data Completed\n",
      "0.5696501880988262 -0.39265437555998145\n",
      "0.555326825626684 -0.37163299043759834\n"
     ]
    }
   ],
   "source": [
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:12.145471Z\",\"start_time\":\"2020-04-03T23:04:12.141914Z\"}}\n",
    "# PATH = '/kaggle/input/'\n",
    "PATH = '/Users/helen/Desktop/Data/'\n",
    "SEQ_LEN = 4000\n",
    "GROUP_PER_BATCH = 500000//SEQ_LEN\n",
    "window_sizes = [10, 50]\n",
    "\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:14.838509Z\",\"start_time\":\"2020-04-03T23:04:14.822835Z\"}}\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        if col != 'time':\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:20.620083Z\",\"start_time\":\"2020-04-03T23:04:17.837634Z\"}}\n",
    "ss = pd.read_csv(PATH + \"liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\n",
    "train = pd.read_csv(PATH +'clean-kalman/train_clean_kalman.csv')\n",
    "train['filter'] = 0\n",
    "test = pd.read_csv(PATH +'clean-kalman/test_clean_kalman.csv')\n",
    "test['filter'] = 2\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:29.265784Z\",\"start_time\":\"2020-04-03T23:04:23.756861Z\"}}\n",
    "# %%time\n",
    "# for window in window_sizes:\n",
    "#     train[\"rolling_mean_\" + str(window)] = train['signal'].rolling(window=window).mean()\n",
    "#     train[\"rolling_std_\" + str(window)] = train['signal'].rolling(window=window).std()\n",
    "#     train[\"rolling_var_\" + str(window)] = train['signal'].rolling(window=window).var()\n",
    "#     train[\"rolling_min_\" + str(window)] = train['signal'].rolling(window=window).min()\n",
    "#     train[\"rolling_max_\" + str(window)] = train['signal'].rolling(window=window).max()\n",
    "    \n",
    "#     train[\"rolling_mean_\" + str(window)+\"_diff\"] = train[\"rolling_mean_\" + str(window)] - train['signal']\n",
    "#     train[\"rolling_std_\" + str(window)+\"_diff\"] = train[\"rolling_std_\" + str(window)] - train['signal']\n",
    "#     train[\"rolling_var_\" + str(window)+\"_diff\"] = train[\"rolling_var_\" + str(window)] - train['signal']\n",
    "#     train[\"rolling_min_\" + str(window)+\"_diff\"] = train[\"rolling_min_\" + str(window)] - train['signal']\n",
    "#     train[\"rolling_max_\" + str(window)+\"_diff\"] = train[\"rolling_max_\" + str(window)] - train['signal']  \n",
    "#     #train[\"rolling_min_max_ratio_\" + str(window)] = train[\"rolling_min_\" + str(window)] / train[\"rolling_max_\" + str(window)]\n",
    "#     #train[\"rolling_min_max_diff_\" + str(window)] = train[\"rolling_max_\" + str(window)] - train[\"rolling_min_\" + str(window)]\n",
    "    \n",
    "#     a = (train['signal'] - train['rolling_min_' + str(window)]) / (train['rolling_max_' + str(window)] - train['rolling_min_' + str(window)])\n",
    "#     train[\"norm_\" + str(window)] = a * (np.floor(train['rolling_max_' + str(window)]) - np.ceil(train['rolling_min_' + str(window)]))\n",
    "    \n",
    "# train = train.replace([np.inf, -np.inf], np.nan)    \n",
    "# train.fillna(0, inplace=True)\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:04:34.073285Z\",\"start_time\":\"2020-04-03T23:04:32.404428Z\"}}\n",
    "# %%time\n",
    "# for window in window_sizes:\n",
    "#     test[\"rolling_mean_\" + str(window)] = test['signal'].rolling(window=window).mean()\n",
    "#     test[\"rolling_std_\" + str(window)] = test['signal'].rolling(window=window).std()\n",
    "#     test[\"rolling_var_\" + str(window)] = test['signal'].rolling(window=window).var()\n",
    "#     test[\"rolling_min_\" + str(window)] = test['signal'].rolling(window=window).min()\n",
    "#     test[\"rolling_max_\" + str(window)] = test['signal'].rolling(window=window).max()\n",
    "    \n",
    "#     test[\"rolling_mean_\" + str(window)+\"_diff\"] = test[\"rolling_mean_\" + str(window)] - test['signal']\n",
    "#     test[\"rolling_std_\" + str(window)+\"_diff\"] = test[\"rolling_std_\" + str(window)] - test['signal']\n",
    "#     test[\"rolling_var_\" + str(window)+\"_diff\"] = test[\"rolling_var_\" + str(window)] - test['signal']\n",
    "#     test[\"rolling_min_\" + str(window)+\"_diff\"] = test[\"rolling_min_\" + str(window)] - test['signal']\n",
    "#     test[\"rolling_max_\" + str(window)+\"_diff\"] = test[\"rolling_max_\" + str(window)] - test['signal']  \n",
    "    \n",
    "#     #test[\"rolling_min_max_ratio_\" + str(window)] = test[\"rolling_min_\" + str(window)] / test[\"rolling_max_\" + str(window)]\n",
    "#     #test[\"rolling_min_max_diff_\" + str(window)] = test[\"rolling_max_\" + str(window)] - test[\"rolling_min_\" + str(window)]\n",
    "\n",
    "    \n",
    "#     a = (test['signal'] - test['rolling_min_' + str(window)]) / (test['rolling_max_' + str(window)] - test['rolling_min_' + str(window)])\n",
    "#     test[\"norm_\" + str(window)] = a * (np.floor(test['rolling_max_' + str(window)]) - np.ceil(test['rolling_min_' + str(window)]))\n",
    "\n",
    "# test = test.replace([np.inf, -np.inf], np.nan)    \n",
    "# test.fillna(0, inplace=True)\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:07:30.632968Z\",\"start_time\":\"2020-04-03T23:04:37.769007Z\"}}\n",
    "# %%time\n",
    "# def features(df):\n",
    "#     df = df.sort_values(by=['time']).reset_index(drop=True)\n",
    "#     df.index = ((df.time * 10_000) - 1).values\n",
    "#     df['batch'] = df.index // 25_000\n",
    "#     df['batch_index'] = df.index  - (df.batch * 25_000)\n",
    "#     df['batch_slices'] = df['batch_index']  // 2500\n",
    "#     df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), \n",
    "#                                                        str(r['batch_slices']).zfill(3)]), axis=1)\n",
    "    \n",
    "#     for c in ['batch','batch_slices2']:\n",
    "#         d = {}\n",
    "#         d['mean'+c] = df.groupby([c])['signal'].mean()\n",
    "# #         print(d['mean'+c])\n",
    "#         d['median'+c] = df.groupby([c])['signal'].median()\n",
    "#         d['max'+c] = df.groupby([c])['signal'].max()\n",
    "#         d['min'+c] = df.groupby([c])['signal'].min()\n",
    "#         d['std'+c] = df.groupby([c])['signal'].std()\n",
    "#         d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n",
    "#         d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n",
    "#         d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n",
    "#         d['range'+c] = d['max'+c] - d['min'+c]\n",
    "#         d['maxtomin'+c] = d['max'+c] / d['min'+c]\n",
    "#         d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) / 2\n",
    "# #         print('--------------------')\n",
    "#         for v in d:\n",
    "#             df[v] = df[c].map(d[v].to_dict())\n",
    "    \n",
    "#     # add shifts_1\n",
    "#     df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n",
    "#     df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n",
    "#     for i in df[df['batch_index']==0].index:\n",
    "#         df['signal_shift_+1'][i] = np.nan\n",
    "#     for i in df[df['batch_index']==49999].index:\n",
    "#         df['signal_shift_-1'][i] = np.nan\n",
    "    \n",
    "#     # add shifts_2 - my upgrade\n",
    "#     df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n",
    "#     df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n",
    "#     for i in df[df['batch_index']==0].index:\n",
    "#         df['signal_shift_+2'][i] = np.nan\n",
    "#     for i in df[df['batch_index']==1].index:\n",
    "#         df['signal_shift_+2'][i] = np.nan\n",
    "#     for i in df[df['batch_index']==49999].index:\n",
    "#         df['signal_shift_-2'][i] = np.nan\n",
    "#     for i in df[df['batch_index']==49998].index:\n",
    "#         df['signal_shift_-2'][i] = np.nan\n",
    "    \n",
    "#     df = df.drop(columns=['batch', 'batch_index', 'batch_slices', 'batch_slices2'])\n",
    "\n",
    "#     for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels']]:\n",
    "#         df[c+'_msignal'] = df[c] - df['signal']\n",
    "        \n",
    "#     df = df.replace([np.inf, -np.inf], np.nan)    \n",
    "#     df.fillna(0, inplace=True)\n",
    "#     gc.collect()\n",
    "#     return df\n",
    "\n",
    "# train = features(train)\n",
    "# test = features(test)\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:16:35.157641Z\",\"start_time\":\"2020-04-03T23:15:50.454252Z\"}}\n",
    "# train = reduce_mem_usage(train)\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:20:37.936034Z\",\"start_time\":\"2020-04-03T23:20:21.086950Z\"}}\n",
    "# test = reduce_mem_usage(test)\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T18:35:47.275695Z\",\"start_time\":\"2020-04-03T18:35:47.253657Z\"}}\n",
    "# pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3)\n",
    "\n",
    "# %% [code]\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "# normalize the data (standard scaler). We can also try other scalers for a better score!\n",
    "def normalize(train, test, div = 1.0):\n",
    "    train_input_mean = train.signal.mean()\n",
    "    train_input_sigma = train.signal.std()\n",
    "    train['signal'] = (train.signal - train_input_mean) / train_input_sigma / div\n",
    "    test['signal'] = (test.signal - train_input_mean) / train_input_sigma / div\n",
    "    return train, test\n",
    "\n",
    "# %% [code]\n",
    "\n",
    "print(\"before normalize\")\n",
    "orgnorm = 15.0\n",
    "print(max(train['signal'])/orgnorm, min(train['signal'])/orgnorm)\n",
    "print(max(test['signal'])/orgnorm, min(test['signal'])/orgnorm)\n",
    "\n",
    "print(\"after normalize\")\n",
    "train, test = normalize(train, test, div=6.0) ## margic number\n",
    "print('Reading and Normalizing Data Completed')\n",
    "\n",
    "print(max(train['signal']), min(train['signal']))\n",
    "print(max(test['signal']), min(test['signal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:18:20.331633Z",
     "start_time": "2020-04-06T19:18:20.220051Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:20:48.033757Z",
     "start_time": "2020-04-06T19:20:48.028966Z"
    }
   },
   "outputs": [],
   "source": [
    "from pykalman import KalmanFilter\n",
    "\n",
    "def Kalman1D(observations,damping=1):\n",
    "    # To return the smoothed time series data\n",
    "    observation_covariance = damping\n",
    "    initial_value_guess = observations[0]\n",
    "    transition_matrix = 1\n",
    "    transition_covariance = 0.1\n",
    "    initial_value_guess\n",
    "    kf = KalmanFilter(\n",
    "            initial_state_mean=initial_value_guess,\n",
    "            initial_state_covariance=observation_covariance,\n",
    "            observation_covariance=observation_covariance,\n",
    "            transition_covariance=transition_covariance,\n",
    "            transition_matrices=transition_matrix\n",
    "        )\n",
    "    pred_state, state_cov = kf.smooth(observations)\n",
    "    return pred_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:20:48.042523Z",
     "start_time": "2020-04-06T19:20:48.036667Z"
    }
   },
   "outputs": [],
   "source": [
    "# observation_covariance = .0015\n",
    "# train['signal']  = Kalman1D(train.signal.values,observation_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:20:48.047780Z",
     "start_time": "2020-04-06T19:20:48.045452Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# plt.plot(train_data[\"time\"], train_data[\"signal\"], color=\"r\")\n",
    "# plt.title(\"Signal data\", fontsize=20)\n",
    "# plt.xlabel(\"Time\", fontsize=18)\n",
    "# plt.ylabel(\"Signal\", fontsize=18)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:20:48.057594Z",
     "start_time": "2020-04-06T19:20:48.051444Z"
    }
   },
   "outputs": [],
   "source": [
    "# test['signal']  = Kalman1D(test.signal.values,observation_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T19:27:22.895698Z",
     "start_time": "2020-04-06T19:21:30.192924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Features\n",
      "Feature Engineering Started...\n",
      "low wns: [0.01       0.07079458 0.50118723]\n",
      "low wn:0.01,b:[0.01546629 0.01546629],a:[ 1.         -0.96906742]\n",
      "low wn:0.0707945784384138,b:[0.10044805 0.10044805],a:[ 1.        -0.7991039]\n",
      "low wn:0.5011872336272722,b:[0.50093245 0.50093245],a:[1.        0.0018649]\n",
      "high wns: [0.01       0.08912509 0.79432823]\n",
      "high wn:0.01,b:[ 0.98453371 -0.98453371],a:[ 1.         -0.96906742]\n",
      "high wn:0.08912509381337455,b:[ 0.87648625 -0.87648625],a:[ 1.        -0.7529725]\n",
      "high wn:0.7943282347242815,b:[ 0.25082314 -0.25082314],a:[1.         0.49835371]\n",
      "     time    signal  open_channels  filter     time2  group    grad_1  \\\n",
      "0  0.0001 -0.190850            0.0       0  0.000002      0 -0.005865   \n",
      "1  0.0002 -0.196715            0.0       0  0.000004      0  0.011287   \n",
      "2  0.0003 -0.168276            0.0       0  0.000006      0 -0.009465   \n",
      "3  0.0004 -0.215644            0.0       0  0.000008      0 -0.024182   \n",
      "4  0.0005 -0.216639            0.0       0  0.000010      0  0.016113   \n",
      "5  0.0006 -0.183418            0.0       0  0.000012      0  0.015028   \n",
      "6  0.0007 -0.186583            0.0       0  0.000014      0  0.001807   \n",
      "7  0.0008 -0.179805            0.0       0  0.000016      0  0.000958   \n",
      "8  0.0009 -0.184666            0.0       0  0.000018      0 -0.005577   \n",
      "9  0.0010 -0.190959            0.0       0  0.000020      0 -0.014530   \n",
      "\n",
      "     grad_2    grad_3  lowpass_lf_0.0100  lowpass_ff_0.0100  \\\n",
      "0  0.017152 -0.018952          -0.190850          -0.193030   \n",
      "1 -0.001800 -0.017443          -0.190941          -0.193009   \n",
      "2 -0.017735  0.007294          -0.190680          -0.192992   \n",
      "3  0.012789  0.018670          -0.190719          -0.192981   \n",
      "4  0.019605 -0.009971          -0.191506          -0.192959   \n",
      "5 -0.007153 -0.013320          -0.191769          -0.192922   \n",
      "6 -0.007035  0.001731          -0.191560          -0.192886   \n",
      "7 -0.003692 -0.000355          -0.191301          -0.192858   \n",
      "8 -0.007744  0.004344          -0.191021          -0.192840   \n",
      "9  0.004997  0.010600          -0.190921          -0.192830   \n",
      "\n",
      "   lowpass_lf_0.0708  lowpass_ff_0.0708  lowpass_lf_0.5012  lowpass_ff_0.5012  \\\n",
      "0          -0.190850          -0.191336          -0.190850          -0.190850   \n",
      "1          -0.191439          -0.191665          -0.193788          -0.188136   \n",
      "2          -0.189643          -0.192172          -0.182474          -0.187195   \n",
      "3          -0.190108          -0.192928          -0.191978          -0.204075   \n",
      "4          -0.195338          -0.193123          -0.216187          -0.208122   \n",
      "5          -0.196280          -0.192562          -0.199998          -0.192501   \n",
      "6          -0.194014          -0.192003          -0.184972          -0.184084   \n",
      "7          -0.191840          -0.191844          -0.183190          -0.182708   \n",
      "8          -0.189911          -0.192146          -0.182233          -0.185009   \n",
      "9          -0.189489          -0.192807          -0.187823          -0.195087   \n",
      "\n",
      "   highpass_lf_0.0100  highpass_ff_0.0100  highpass_lf_0.0891  \\\n",
      "0        1.665335e-16            0.002180            0.000000   \n",
      "1       -5.774449e-03           -0.003707           -0.005141   \n",
      "2        2.240410e-02            0.024717            0.021056   \n",
      "3       -2.492512e-02           -0.022663           -0.025663   \n",
      "4       -2.513332e-02           -0.023680           -0.020196   \n",
      "5        8.351298e-03            0.009504            0.013911   \n",
      "6        4.977237e-03            0.006303            0.007701   \n",
      "7        1.149640e-02            0.013053            0.011739   \n",
      "8        6.354932e-03            0.008174            0.004579   \n",
      "9       -3.787616e-05            0.001871           -0.002069   \n",
      "\n",
      "   highpass_ff_0.0891  highpass_lf_0.7943  highpass_ff_0.7943  ewm_mean_10  \\\n",
      "0            0.000247            0.000000           -0.000024    -0.190850   \n",
      "1           -0.005197           -0.001471           -0.004798    -0.194076   \n",
      "2            0.023937            0.007866            0.007713    -0.183704   \n",
      "3           -0.022333           -0.015801           -0.004951    -0.194227   \n",
      "4           -0.023099            0.007625           -0.001164    -0.200661   \n",
      "5            0.009171            0.004533            0.003548    -0.196182   \n",
      "6            0.005062           -0.003053           -0.003130    -0.193869   \n",
      "7            0.011507            0.003221            0.003038    -0.190669   \n",
      "8            0.007010           -0.002825           -0.003010    -0.189363   \n",
      "9            0.001620           -0.000171            0.004683    -0.189699   \n",
      "\n",
      "   ewm_std_10  ewm_mean_50  ewm_std_50  signal_shift_pos_1  \\\n",
      "0    0.040694    -0.190850    0.048222            0.000000   \n",
      "1    0.004147    -0.193841    0.004147           -0.190850   \n",
      "2    0.015843    -0.184976    0.015186           -0.196715   \n",
      "3    0.021346    -0.193109    0.019879           -0.168276   \n",
      "4    0.020895    -0.198199    0.020194           -0.215644   \n",
      "5    0.019532    -0.195483    0.018946           -0.216639   \n",
      "6    0.017395    -0.194054    0.017477           -0.183418   \n",
      "7    0.016449    -0.192014    0.016879           -0.186583   \n",
      "8    0.014702    -0.191061    0.015842           -0.179805   \n",
      "9    0.013024    -0.191048    0.014779           -0.184666   \n",
      "\n",
      "   signal_shift_neg_1  signal_shift_pos_2  signal_shift_neg_2  \\\n",
      "0           -0.196715            0.000000           -0.168276   \n",
      "1           -0.168276            0.000000           -0.215644   \n",
      "2           -0.215644           -0.190850           -0.216639   \n",
      "3           -0.216639           -0.196715           -0.183418   \n",
      "4           -0.183418           -0.168276           -0.186583   \n",
      "5           -0.186583           -0.215644           -0.179805   \n",
      "6           -0.179805           -0.216639           -0.184666   \n",
      "7           -0.184666           -0.183418           -0.190959   \n",
      "8           -0.190959           -0.186583           -0.213726   \n",
      "9           -0.213726           -0.179805           -0.182126   \n",
      "\n",
      "   signal_shift_pos_3  signal_shift_neg_3  signal_2  \n",
      "0            0.000000           -0.215644  0.036424  \n",
      "1            0.000000           -0.216639  0.038697  \n",
      "2            0.000000           -0.183418  0.028317  \n",
      "3           -0.190850           -0.186583  0.046502  \n",
      "4           -0.196715           -0.179805  0.046932  \n",
      "5           -0.168276           -0.184666  0.033642  \n",
      "6           -0.215644           -0.190959  0.034813  \n",
      "7           -0.216639           -0.213726  0.032330  \n",
      "8           -0.183418           -0.182126  0.034101  \n",
      "9           -0.186583           -0.188964  0.036465  \n",
      "['signal', 'grad_1', 'grad_2', 'grad_3', 'lowpass_lf_0.0100', 'lowpass_ff_0.0100', 'lowpass_lf_0.0708', 'lowpass_ff_0.0708', 'lowpass_lf_0.5012', 'lowpass_ff_0.5012', 'highpass_lf_0.0100', 'highpass_ff_0.0100', 'highpass_lf_0.0891', 'highpass_ff_0.0891', 'highpass_lf_0.7943', 'highpass_ff_0.7943', 'ewm_mean_10', 'ewm_std_10', 'ewm_mean_50', 'ewm_std_50', 'signal_shift_pos_1', 'signal_shift_neg_1', 'signal_shift_pos_2', 'signal_shift_neg_2', 'signal_shift_pos_3', 'signal_shift_neg_3', 'signal_2']\n",
      "27\n",
      "(1250, 4000, 27)\n",
      "Fold : 3\n",
      "Epoch : 0\n",
      "learning_rate: 0.000001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-14cd70b6d0a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:21:08.254985Z\",\"start_time\":\"2020-04-03T23:21:05.328127Z\"}}\n",
    "ts1 = pd.concat([train, test], axis=0, sort=False).reset_index(drop=True)\n",
    "ts1.head()\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:21:23.618036Z\",\"start_time\":\"2020-04-03T23:21:22.475969Z\"}}\n",
    "# 将每个batch一个time2 bin\n",
    "ts1['time2'] = pd.cut(ts1['time'], bins=np.linspace(0.0000, 700., num=14 + 1), labels=list(range(14)), \n",
    "                      include_lowest=True).astype(int)\n",
    "#每个batch time2 做归一化\n",
    "ts1['time2'] = ts1.groupby('time2')['time'].rank( )/500000. \n",
    "\n",
    "#将每个batch变成125个group\n",
    "np.random.seed(321)\n",
    "ts1['group'] = pd.cut(ts1['time'], bins=np.linspace(0.0000, 700., num=14*GROUP_PER_BATCH + 1), \n",
    "                      labels=list(range(14*GROUP_PER_BATCH)), \n",
    "                      include_lowest=True).astype(int)\n",
    "np.random.seed(321)\n",
    "ts1.head()\n",
    "# signal processing features\n",
    "def calc_gradients(s, df, n_grads = 4):\n",
    "    '''\n",
    "    Calculate gradients for a pandas series. Returns the same number of samples\n",
    "    '''\n",
    "    \n",
    "    g = s.values\n",
    "    for i in range(n_grads):\n",
    "        g = np.gradient(g)\n",
    "        df['grad_' + str(i+1)] = g\n",
    "        \n",
    "    return df\n",
    "\n",
    "def calc_low_pass(s, df, n_filts=10):\n",
    "    '''\n",
    "    Applies low pass filters to the signal. Left delayed and no delayed\n",
    "    '''\n",
    "    wns = np.logspace(-2, -0.3, n_filts)\n",
    "    print(\"low wns:\", wns)\n",
    "    x = s.values\n",
    "    for wn in wns:\n",
    "        b, a = signal.butter(1, Wn=wn, btype='low')\n",
    "        print(\"low wn:{},b:{},a:{}\".format(wn, b, a))\n",
    "        zi = signal.lfilter_zi(b, a)\n",
    "        df['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n",
    "        df['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def calc_high_pass(s, df, n_filts=10):\n",
    "    '''\n",
    "    Applies high pass filters to the signal. Left delayed and no delayed\n",
    "    '''\n",
    "    wns = np.logspace(-2, -0.1, n_filts)\n",
    "    print(\"high wns:\", wns)\n",
    "    x = s.values\n",
    "    for wn in wns:\n",
    "        b, a = signal.butter(1, Wn=wn, btype='high')\n",
    "        print(\"high wn:{},b:{},a:{}\".format(wn, b, a))\n",
    "        zi = signal.lfilter_zi(b, a)\n",
    "        df['highpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n",
    "        df['highpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def calc_ewm(s, df, windows=[10, 50, 100, 500, 1000]):\n",
    "    '''\n",
    "    Calculates exponential weighted functions\n",
    "    '''\n",
    "    for w in windows:\n",
    "        df['ewm_mean_' + str(w)] = s.ewm(span=w, min_periods=1).mean()\n",
    "        df['ewm_std_' + str(w)] = s.ewm(span=w, min_periods=1).std()\n",
    "        \n",
    "#     # add zeros when na values (std)\n",
    "#     df = df.fillna(value=0)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    '''\n",
    "    All calculations together\n",
    "    '''\n",
    "    s = df['signal']\n",
    "    df = calc_gradients(s, df, n_grads = 4)\n",
    "    df = calc_low_pass(s, df, n_filts = 6)\n",
    "    df = calc_high_pass(s, df, n_filts = 6)\n",
    "    df = calc_ewm(s, df, windows=[5,10])\n",
    "    \n",
    "    return df\n",
    "# %% [code]\n",
    "# get lead and lags features\n",
    "def lag_with_pct_change(df, windows):\n",
    "    for window in windows:    \n",
    "        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n",
    "        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n",
    "    return df\n",
    "\n",
    "# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\n",
    "def run_feat_engineering(df):\n",
    "    # create leads and lags (1, 2, 3 making them 6 features)\n",
    "    df = add_features(df)\n",
    "    df = lag_with_pct_change(df, [1, 2, 3])\n",
    "    # create signal ** 2 (this is the new feature)\n",
    "    df['signal_2'] = df['signal'] ** 2\n",
    "    return df\n",
    "\n",
    "# fillna with the mean and select features for training\n",
    "def feature_selection_org(train, test):\n",
    "    features = use_cols\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "    for feature in features:\n",
    "        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n",
    "        train[feature] = train[feature].fillna(feature_mean)\n",
    "        test[feature] = test[feature].fillna(feature_mean)\n",
    "    return train, test, features\n",
    "\n",
    "def feature_selection(data, features):\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    for feature in features:\n",
    "        feature_mean = data[feature].mean()\n",
    "        data[feature] = data[feature].fillna(feature_mean)\n",
    "    return data, features\n",
    "\n",
    "# %% [code]\n",
    "print('Creating Features')\n",
    "print('Feature Engineering Started...')\n",
    "ts1 = run_feat_engineering(ts1)\n",
    "use_cols = [col for col in ts1.columns if col not in ['index','filter','group', 'open_channels', 'time', 'time2', \n",
    "                                                      'medianbatch', 'abs_avgbatch', 'abs_maxbatch']]\n",
    "ts1, features = feature_selection(ts1, use_cols)\n",
    "print(ts1.head(10))\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:21:26.967325Z\",\"start_time\":\"2020-04-03T23:21:26.773418Z\"}}\n",
    "y = ts1.loc[ts1['filter']==0, 'open_channels']\n",
    "group = ts1.loc[ts1['filter']==0, 'group']\n",
    "X = ts1.loc[ts1['filter']==0, 'signal']\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:22:22.903966Z\",\"start_time\":\"2020-04-03T23:22:22.478503Z\"}}\n",
    "np.random.seed(321)\n",
    "skf = GroupKFold(n_splits=5)\n",
    "splits = [x for x in skf.split(X, y, group)] ## split into train index and test index\n",
    "print(features)\n",
    "print(len(features))\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:22:25.960306Z\",\"start_time\":\"2020-04-03T23:22:25.955933Z\"}}\n",
    "def analysis_splits(new_split):\n",
    "    bins = np.array(new_split)//GROUP_PER_BATCH\n",
    "    for b in bins:\n",
    "        print(np.histogram(b, bins=np.arange(11))[0])\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:22:38.774185Z\",\"start_time\":\"2020-04-03T23:22:27.923032Z\"}}\n",
    "# Create numpy array of inputs\n",
    "for col in use_cols:\n",
    "    col_mean = ts1[col].mean()\n",
    "    ts1[col] = ts1[col].fillna(col_mean)\n",
    " \n",
    "val_preds_all = np.zeros((ts1[ts1['filter']==0].shape[0], 11))\n",
    "test_preds_all = np.zeros((ts1[ts1['filter']==2].shape[0], 11))\n",
    "\n",
    "groups = ts1.loc[ts1['filter']==0, 'group']\n",
    "times = ts1.loc[ts1['filter']==0, 'time']\n",
    "\n",
    "new_splits = []\n",
    "# 5个fold，每个fold中train和test对应group无重复，将其利用group编号装入new_split.\n",
    "for sp in splits:\n",
    "    new_split = []\n",
    "    new_split.append(np.unique(groups[sp[0]]))\n",
    "    new_split.append(np.unique(groups[sp[1]]))\n",
    "#     analysis_splits(new_split)\n",
    "    new_splits.append(new_split)\n",
    "    \n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T21:40:33.912812Z\",\"start_time\":\"2020-04-03T21:40:33.910507Z\"}}\n",
    "# print(ts1[ts1['filter']==0].groupby('group').size())\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:23:14.963427Z\",\"start_time\":\"2020-04-03T23:23:05.783728Z\"}}\n",
    "trainval = np.array(list(ts1[ts1['filter']==0].groupby('group').apply(lambda x: x[use_cols].values)))\n",
    "test = np.array(list(ts1[ts1['filter']==2].groupby('group').apply(lambda x: x[use_cols].values)))\n",
    "trainval_y = np.array(list(ts1[ts1['filter']==0].groupby('group').apply(lambda x: x[['open_channels']].values)))\n",
    "gc.collect()\n",
    "# transpose to B x C x L\n",
    "print(np.shape(trainval))\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:23:37.126091Z\",\"start_time\":\"2020-04-03T23:23:34.716237Z\"}}\n",
    "trainval = trainval.transpose((0,2,1))\n",
    "test = test.transpose((0,2,1))\n",
    "\n",
    "trainval_y = trainval_y.reshape(trainval_y.shape[:2])\n",
    "test_y = np.zeros((test.shape[0], trainval_y.shape[1]))\n",
    "\n",
    "trainval = torch.Tensor(trainval)\n",
    "test = torch.Tensor(test)\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:24:12.902101Z\",\"start_time\":\"2020-04-03T23:24:12.898415Z\"}}\n",
    "\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:24:41.652529Z\",\"start_time\":\"2020-04-03T23:24:41.645025Z\"}}\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n",
    "        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n",
    "        self.counter, self.best_score = 0, None\n",
    "        self.is_maximize = is_maximize\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        model.load_state_dict(torch.load(self.checkpoint_path))\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None or \\\n",
    "        (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n",
    "            torch.save(model.state_dict(), self.checkpoint_path)\n",
    "            self.best_score, self.counter = score, 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:28:57.038045Z\",\"start_time\":\"2020-04-03T23:28:57.021837Z\"}}\n",
    "class Seq2SeqRnn(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, hidden_size, output_size, num_layers=1, bidirectional=False, dropout=.3,\n",
    "            hidden_layers = [100, 200]):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.output_size=output_size\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, batch_first=True,dropout=0.3)\n",
    "         # Input Layer\n",
    "        if hidden_layers and len(hidden_layers):\n",
    "            first_layer  = nn.Linear(hidden_size*2 if bidirectional else hidden_size, hidden_layers[0])\n",
    "\n",
    "            # Hidden Layers\n",
    "            self.hidden_layers = nn.ModuleList(\n",
    "                [first_layer]+[nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers) - 1)]\n",
    "            )\n",
    "            for layer in self.hidden_layers: nn.init.kaiming_normal_(layer.weight.data)   \n",
    "\n",
    "            self.intermediate_layer = nn.Linear(hidden_layers[-1], self.input_size)\n",
    "            # output layers\n",
    "            self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n",
    "            nn.init.kaiming_normal_(self.output_layer.weight.data) \n",
    "           \n",
    "        else:\n",
    "            self.hidden_layers = []\n",
    "            self.intermediate_layer = nn.Linear(hidden_size*2 if bidirectional else hidden_siz, self.input_size)\n",
    "            self.output_layer = nn.Linear(hidden_size*2 if bidirectional else hidden_size, output_size)\n",
    "            nn.init.kaiming_normal_(self.output_layer.weight.data) \n",
    "\n",
    "        self.activation_fn = torch.relu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        outputs, hidden = self.rnn(x)        \n",
    "\n",
    "        x = self.dropout(self.activation_fn(outputs))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fn(hidden_layer(x))\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:28:58.944588Z\",\"start_time\":\"2020-04-03T23:28:58.928437Z\"}}\n",
    "class IonDataset(Dataset):\n",
    "    \"\"\"Car dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, labels, training=True, transform=None, flip=0.5, noise_level=0, class_split=0.0):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.training = training\n",
    "        self.flip = flip\n",
    "        self.noise_level = noise_level\n",
    "        self.class_split = class_split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data = self.data[idx]\n",
    "        labels = self.labels[idx]\n",
    "        if np.random.rand() < self.class_split:\n",
    "            data, labels = class_split(data, labels)\n",
    "        if  np.random.rand() < self.noise_level:\n",
    "            data = data * torch.FloatTensor(10000).uniform_(1-self.noise_level, 1+self.noise_level)\n",
    "        if np.random.rand() < self.flip:\n",
    "            data = torch.flip(data, dims=[1])\n",
    "            labels = np.flip(labels, axis=0).copy().astype(int)\n",
    "\n",
    "        return [data, labels.astype(int)]\n",
    "\n",
    "# %% [code] {\"ExecuteTime\":{\"end_time\":\"2020-04-03T23:30:48.085252Z\",\"start_time\":\"2020-04-03T23:29:14.731305Z\"}}\n",
    "if not os.path.exists(\"./models\"):\n",
    "            os.makedirs(\"./models\")\n",
    "idshift = 3\n",
    "for index, (train_index, val_index ) in enumerate(new_splits[idshift:], start=0):\n",
    "    index = index + idshift\n",
    "    if index > 3:\n",
    "        break;\n",
    "    print(\"Fold : {}\".format(index))\n",
    "    \n",
    "    batchsize = 16\n",
    "    train_dataset = IonDataset(trainval[train_index],  trainval_y[train_index], flip=False, noise_level=0.0, \n",
    "                               class_split=0.0)\n",
    "    train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    valid_dataset = IonDataset(trainval[val_index],  trainval_y[val_index], flip=False)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batchsize, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    test_dataset = IonDataset(test,  test_y, flip=False, noise_level=0.0, class_split=0.0)\n",
    "    test_dataloader = DataLoader(test_dataset, batchsize, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_preds_iter = np.zeros((2000000, 11))\n",
    "    it = 0\n",
    "    for it in range(1):\n",
    "        device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        model=Seq2SeqRnn(input_size=trainval.shape[1], seq_len=SEQ_LEN, hidden_size=64, output_size=11, \n",
    "                         num_layers=2, hidden_layers=[64,64,64],\n",
    "                         bidirectional=True).to(device)\n",
    "    \n",
    "        no_of_epochs = 150\n",
    "        early_stopping = EarlyStopping(patience=20, is_maximize=True, \n",
    "                                       checkpoint_path=\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it))\n",
    "        criterion = L.FocalLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "                                                        max_lr=0.001, epochs=no_of_epochs,\n",
    "                                                        steps_per_epoch=len(train_dataloader))\n",
    "        avg_train_losses, avg_valid_losses = [], [] \n",
    "    \n",
    "    \n",
    "        for epoch in range(no_of_epochs):\n",
    "            start_time = time.time()\n",
    "    \n",
    "            print(\"Epoch : {}\".format(epoch))\n",
    "            print( \"learning_rate: {:0.9f}\".format(schedular.get_lr()[0]))\n",
    "            train_losses, valid_losses = [], []\n",
    "    \n",
    "            model.train() # prep model for training\n",
    "            train_preds, train_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n",
    "    \n",
    "            for x, y in train_dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(x[:, :trainval.shape[1], :])\n",
    "    \n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "                y_ = y.view(-1)\n",
    "    \n",
    "                loss = criterion(predictions_, y_)\n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                optimizer.step()\n",
    "                schedular.step()\n",
    "                # record training lossa\n",
    "                train_losses.append(loss.item())\n",
    "    \n",
    "                train_true = torch.cat([train_true, y_], 0)\n",
    "                train_preds = torch.cat([train_preds, predictions_], 0)\n",
    "\n",
    "            model.eval() # prep model for evaluation\n",
    "            val_preds, val_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n",
    "            with torch.no_grad():\n",
    "                for x, y in valid_dataloader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "    \n",
    "                    predictions = model(x[:,:trainval.shape[1],:])\n",
    "                    predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "                    y_ = y.view(-1)\n",
    "    \n",
    "                    loss = criterion(predictions_, y_)\n",
    "                    valid_losses.append(loss.item())\n",
    "        \n",
    "                    val_true = torch.cat([val_true, y_], 0)\n",
    "                    val_preds = torch.cat([val_preds, predictions_], 0)\n",
    "\n",
    "            # calculate average loss over an epoch\n",
    "            train_loss = np.average(train_losses)\n",
    "            valid_loss = np.average(valid_losses)\n",
    "            avg_train_losses.append(train_loss)\n",
    "            avg_valid_losses.append(valid_loss)\n",
    "            \n",
    "            print( \"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n",
    "\n",
    "            train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1), \n",
    "                                   labels=list(range(11)), average='macro')\n",
    "    \n",
    "            val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1), \n",
    "                                 labels=list(range(11)), average='macro')\n",
    "            print( \"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n",
    "    \n",
    "            if early_stopping(val_score, model):\n",
    "                print(\"Early Stopping...\")\n",
    "                print(\"Best Val Score: {:0.6f}\".format(early_stopping.best_score))\n",
    "                break\n",
    "    \n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        model.load_state_dict(torch.load(\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it)))\n",
    "        with torch.no_grad():\n",
    "            pred_list = []\n",
    "            for x, y in test_dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                predictions = model(x[:,:trainval.shape[1],:])\n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "\n",
    "                pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy())\n",
    "            test_preds = np.vstack(pred_list)\n",
    "       \n",
    "        test_preds_iter += test_preds\n",
    "        test_preds_all += test_preds\n",
    "        if not os.path.exists(\"./predictions/test\"):\n",
    "            os.makedirs(\"./predictions/test\")\n",
    "        np.save('./predictions/test/gru_clean_fold_{}_iter_{}_raw.npy'.format(index, it), arr=test_preds_iter)\n",
    "        np.save('./predictions/test/gru_clean_fold_{}_raw.npy'.format(index), arr=test_preds_all)\n",
    "\n",
    "test_preds_all = test_preds_all/np.sum(test_preds_all, axis=1)[:, None]\n",
    "test_pred_frame = pd.DataFrame({'time': ss['time'].astype(str),\n",
    "                                'open_channels': np.argmax(test_preds_all, axis=1)})\n",
    "test_pred_frame.to_csv(\"./gru_preds.csv\", index=False)\n",
    "\n",
    "# %% [code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
